{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Read in the preprocessed text file with explicit encoding\n",
    "with open('C:/langchain2/wiki/wiki_2/trext_data/processed_text.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into smaller chunks\n",
    "chunks = [text[i:i+100] for i in range(0, len(text), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 2064\n"
     ]
    }
   ],
   "source": [
    "num_chunks = len(chunks)\n",
    "print(\"Number of chunks created:\", num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features for each chunk\n",
    "features = []\n",
    "for chunk in chunks:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        chunk,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    features.append(encoding)\n",
    "\n",
    "# stack the features\n",
    "input_ids = torch.cat([f['input_ids'] for f in features], dim=0)\n",
    "attention_mask = torch.cat([f['attention_mask'] for f in features], dim=0)\n",
    "token_type_ids = torch.cat([f['token_type_ids'] for f in features], dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1658, 49314,  ...,     1,     1,     1],\n",
       "        [    0,  2780,  6626,  ...,     1,     1,     1],\n",
       "        [    0,    75,   289,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,    91,     5,  ...,     1,     1,     1],\n",
       "        [    0,    96,     5,  ...,     1,     1,     1],\n",
       "        [    0,  4820,   162,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self,i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x2a6f0037c40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2a6f0037be0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForMaskedLM(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): XLMRobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/129 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m input_ids \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 9\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, labels\u001b[39m=\u001b[39mlabels)\n\u001b[0;32m     11\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "\u001b[1;31mKeyError\u001b[0m: 'labels'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 1\n",
    "epoch = 4\n",
    "loop = tqdm(dataloader, leave=True)\n",
    "for i, batch in enumerate(loop):\n",
    "    if i % batch_size == 0:\n",
    "        optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    if (i + 1) % batch_size == 0:\n",
    "        optim.step()\n",
    "        loop.set_description(f'Epoch:{epoch}')\n",
    "        loop.set_description(f'loss:{loss.item()}')\n",
    "\n",
    "    # Free up GPU memory after every batch\n",
    "    del input_ids, attention_mask, labels, outputs\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('C:/langchain2/wiki/wiki_2/trext_data/roberta_model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaTokenizer\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# # Read in the preprocessed text file with explicit encoding\n",
    "# with open('C:/langchain2/wiki/wiki_2/trext_data/processed_text.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # Tokenize the text using the tokenizer\n",
    "# encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   611,   967,  1545,   879,  2636, 27802, 37771,  1855,   967,\n",
       "          1545,   879,  2636,  6793,    36,  1855,   967,   705,  4839,   479,\n",
       "         28667, 48079, 11696,  2660,  2400,   479,   842, 33488,   636,  5948,\n",
       "            80, 11901,   523,   705,   183,  4553,   366,   710,   479, 28667,\n",
       "           189, 48079,   471,  1488,  2156, 11721,  3998,  2400,  2156,  2660,\n",
       "         27435,  2156, 21563,   479, 28667,  4505, 32092,   624,   186, 25606,\n",
       "           141,  3623,  2156,  5852,  2660,  2400,   189,    94,   353,    76,\n",
       "           479,   810,   744,   198,   112,     6,   151,     4,   664,  2156,\n",
       "           793,  2156,   474,   936,   810, 20242, 40177,   281,   479,  6793,\n",
       "          2504,  3723, 25934,    80,  1907, 22443,  4832,    10,   196,  1076,\n",
       "           428,  1517, 11726,   687,    10,   196,    10,   242, 44724,   118,\n",
       "           479,  1049, 10970,   183,   479,  6793,   189, 27884,   922,   624,\n",
       "           346, 23727, 48079,  5103, 39680,   479, 25378, 15879,   364,   853,\n",
       "          1296,  1925,  6793,   128,    29,   910,  2133, 42814, 15457,  6793,\n",
       "           479, 28667, 22037,   385, 38381, 11696,   992,  4151, 11696,   479,\n",
       "         43731,   705,  3723, 25934, 44949,  1075, 13998,  7884,   462, 27802,\n",
       "           479,   275,  1266,  2097,    81,   337, 22443,   797,  1877, 10970,\n",
       "           443, 40177,   281,  1537,   479,   189,   233, 43842,   705, 33186,\n",
       "           281, 22443,   899,   514,   304, 21442,  2851,   523, 22443,  1161,\n",
       "           479, 37980,   179, 12002,  1594,  1416,   336,     4,  5940, 48079,\n",
       "          1079,  2156, 12293,  2156, 26467,   244, 11696,  2660,  2400,   479,\n",
       "         40177,   281, 33488,   636,  5948,  9724, 14962,    25,   493,  2156,\n",
       "          8507,   266,  2287,   642, 38187,   102,   579,  3976,  3788,    29,\n",
       "           479,   777,   153,  1985,   403,  5948,   479,   777,  5948,  2342,\n",
       "           368,  4347,  9183,  1933,   194,   336, 15503,   338,   400, 19071,\n",
       "           853,   403,   479, 40177,   281,    78, 26702, 16665, 26891, 15149,\n",
       "           329,  8295,   479,  1385,   449,   757,   677,  2832, 24891,  1073,\n",
       "          1266, 45518, 44949,  1075,  8541,  2723, 12801,   479,   198,  5663,\n",
       "          7606,  3723, 25934, 27802,  1855,   967,  1545,   879,  2636,  6793,\n",
       "         34433,   118, 28667,  2156, 33488,   636,  1642,  7207,   239, 11696,\n",
       "            36, 12747, 33397,   506,  4839,   479, 11696,  1010,  1407, 20242,\n",
       "         11721,  3998,  2660,  2400,   479,  2400,  4505,  3327, 40158,  2660,\n",
       "          3124,  2985,  2156, 42662, 17985,   939,     4,   242,   479,    65,\n",
       "         13789,  3327,  2156,   157,   479,  3723, 25934,  1855,   967,  1545,\n",
       "           879,  2636,    67,  7690, 34433,   118,   471,  1488,  2156,   124,\n",
       "          2400,  2156, 27214,  2156, 36239,   257,   479,   198,   457,  3327,\n",
       "          2179, 21563,  2156,   769, 39893, 45420,   757,   650,  8792, 14262,\n",
       "          2156,  2767,  6922,  2156, 28762,  2156,   652,   479,  2156, 21563,\n",
       "          1091, 10759,  9946,   650,   233, 28072,   118, 25606,    50,  2156,\n",
       "         21563,  8935,  1290,  2156,  1719,  1814,  7606,  3024,   479,  3723,\n",
       "         25934, 34433,   118, 39678,  2544,   990,   179, 23265,  2156, 47585,\n",
       "           179,  2400, 39791,   479,    50, 34433,   118,  2295,   936,  2156,\n",
       "           766, 28195,  1109,  2156, 21044, 27688,  2156,  2400,   639,  2295,\n",
       "           479,    78,   278, 28667,   486, 45518,  4285,  1182,  4359, 12801,\n",
       "          1855,   967,  1545,   879,  2636,    94,   198,   186,  2156, 28667,\n",
       "          5032,  1168,   705, 10209,   479,   313,   118,  3723, 25934, 27351,\n",
       "         28667, 45518,  4285,  1182,  4359, 12801,  5032,  1168,   705,  2156,\n",
       "          1385, 45518,   618,    12,  1043,  1182,  4359, 12801, 28667,    94,\n",
       "           130,   186,   130,   353,  2156, 45518,  7642,  1289, 12801, 28667,\n",
       "            94,  1181,   130,   353,   479,   403,  2156,    94, 28667,  3805,\n",
       "          2660,  2400,  4832,  4709,   212,  3961,   118,  2156,  2724,   366,\n",
       "          3892,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoded_input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
