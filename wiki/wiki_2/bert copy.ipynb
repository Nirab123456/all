{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Chikungunya.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\dengue.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Fatigue.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Fever.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Headache.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Malaria.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Nausea.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Rash.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Tuberculosis.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Typhoid_fever.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\Vomiting.txt',\n",
       " 'C:\\\\langchain2\\\\wiki\\\\wiki_2\\\\trext_data\\\\who_dengue.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "paths = [str(x) for x in Path('C:/langchain2/wiki/wiki_2/trext_data').glob('**/*.txt')]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty string to hold all the text\n",
    "text = \"\"\n",
    "\n",
    "# loop over the file paths and read each file into the text string\n",
    "for path in paths:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text += f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples=text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 310883 characters\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text: {} characters\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split the text into smaller chunks\n",
    "# chunks = [text[i:i+100] for i in range(0, len(text), 100)]\n",
    "# num_chunks = len(chunks)\n",
    "# print(\"Number of chunks created:\", num_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples, padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenized_eli5 \u001b[39m=\u001b[39m examples\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m      2\u001b[0m     preprocess_function,\n\u001b[0;32m      3\u001b[0m     batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     num_proc\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "tokenized_eli5 = examples.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15).map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_mlm_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
