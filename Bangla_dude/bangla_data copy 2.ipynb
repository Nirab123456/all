{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bertaTokenizer' from 'transformers' (c:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m bertaTokenizer,AutoModelForMaskedLM\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'bertaTokenizer' from 'transformers' (c:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import bertaTokenizer,AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('C:/Bangla_dude/new_ban_t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমি', 'বাংলায', 'গান', 'গাই', '।']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"আমি বাংলায় গান গাই।\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the tensors\n",
    "file_path = 'bangla_tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensors\n",
    "input_ids = torch.load(os.path.join(file_path, \"input_ids.pt\"))\n",
    "mask = torch.load(os.path.join(file_path, \"mask.pt\"))\n",
    "labels = torch.load(os.path.join(file_path, \"labels.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1114481, 512]),\n",
       " torch.Size([1114481, 512]),\n",
       " torch.Size([1114481, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_ids.size(), mask.size(), labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1114481, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1114481, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1114481, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = {\n",
    "    \n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': mask,\n",
    "    'labels': labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 30600,   309,  ...,     1,     1,     1],\n",
       "         [    0,   311,   264,  ...,   265,  3214,     2],\n",
       "         [    0,     4,   267,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,     4,   267,  ...,     1,     1,     1],\n",
       "         [    0,     4,   267,  ...,   399,   279,     2],\n",
       "         [    0,     4,   268,  ...,   283,   276,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([[    0, 30600,   309,  ...,     1,     1,     1],\n",
       "         [    0,   311,   264,  ...,   265,  3214,     2],\n",
       "         [    0,  1116,   267,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,   899,   267,  ...,     1,     1,     1],\n",
       "         [    0,   899,   267,  ...,   399,   279,     2],\n",
       "         [    0,   293,   268,  ...,   283,   276,     2]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return{key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x179a1e4fdc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278621"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x179a1e4f610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= AutoModelForMaskedLM.from_pretrained('C:/Bangla_dude/checkpoint-2000', return_dict=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=52000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "# log_dir = \"logs/\"\n",
    "# writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# # Create an EventAccumulator object to load the log file\n",
    "# event_acc = EventAccumulator(log_dir)\n",
    "# event_acc.Reload()\n",
    "\n",
    "# # Get the scalar summaries from the log file\n",
    "# tags = event_acc.Tags()['scalars']\n",
    "# for tag in tags:\n",
    "#     events = event_acc.Scalars(tag)\n",
    "#     for event in events:\n",
    "#         writer.add_scalar(tag, event.value, event.step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set the second log directory\n",
    "# log_dir_2 = \"logs/2\"\n",
    "# writer_2 = SummaryWriter(log_dir=log_dir_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d22fec7358c488badabd2343ad3b9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Training loss: 10.90858830833435\n",
      "Step 2000: Training loss: 10.908246829032898\n",
      "Step 3000: Training loss: 10.908221113204956\n",
      "Step 4000: Training loss: 10.908805342674256\n",
      "Step 5000: Training loss: 10.90810200023651\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     41\u001b[0m \u001b[39m# Clip the gradients to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), max_norm\u001b[39m=\u001b[39;49mmax_grad_norm)\n\u001b[0;32m     44\u001b[0m \u001b[39m# Update the parameters\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[1;32m---> 76\u001b[0m         torch\u001b[39m.\u001b[39;49m_foreach_mul_(grads, clip_coef_clamped\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[0;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the hyperparameters\n",
    "per_gpu_train_batch_size = 24\n",
    "per_gpu_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 2 # Number of updates steps to accumulate before performing a backward/update pass\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = .01\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 3\n",
    "max_steps = -1\n",
    "warmup_steps = 0\n",
    "logging_steps = 1000\n",
    "save_steps = 5000\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "# Define the training loop\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_train_epochs}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Move the inputs and labels to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss /= gradient_accumulation_steps # Normalize the loss because it is averaged\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # Update the parameters\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log training loss\n",
    "            epoch_loss += loss.item()\n",
    "            if global_step % logging_steps == 0:\n",
    "                print(f'Step {global_step}: Training loss: {epoch_loss/logging_steps}')\n",
    "                writer.add_scalar('Training loss', epoch_loss/logging_steps, global_step)\n",
    "                epoch_loss = 0\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if global_step % save_steps == 0:\n",
    "                output_dir = f'./checkpoints/checkpoint-{global_step}'\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "                print(f'Saved model checkpoint to {output_dir}')\n",
    "                \n",
    "                    \n",
    "                model.train()\n",
    "                \n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "        \n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9fcd8778c2416eb379cb02a493fc10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Training loss: 4.728098392486572\n",
      "Step 2: Training loss: 2.9990234375\n",
      "Step 3: Training loss: 3.944324493408203\n",
      "Step 4: Training loss: 2.5557453632354736\n",
      "Step 5: Training loss: 3.993995428085327\n",
      "Step 6: Training loss: 2.436546564102173\n",
      "Step 7: Training loss: 2.5162978172302246\n",
      "Step 8: Training loss: 4.281270503997803\n",
      "Step 9: Training loss: 3.0095937252044678\n",
      "Step 10: Training loss: 3.233635663986206\n",
      "Step 11: Training loss: 2.568081855773926\n",
      "Step 12: Training loss: 2.4903323650360107\n",
      "Step 13: Training loss: 2.6215434074401855\n",
      "Step 14: Training loss: 3.314723014831543\n",
      "Step 15: Training loss: 4.425722122192383\n",
      "Step 16: Training loss: 4.745405197143555\n",
      "Step 17: Training loss: 3.9612879753112793\n",
      "Step 18: Training loss: 4.351767539978027\n",
      "Step 19: Training loss: 3.066232681274414\n",
      "Step 20: Training loss: 3.0739309787750244\n",
      "Step 21: Training loss: 3.4359536170959473\n",
      "Step 22: Training loss: 2.8286311626434326\n",
      "Step 23: Training loss: 2.5409109592437744\n",
      "Step 24: Training loss: 3.8375720977783203\n",
      "Step 25: Training loss: 4.546136856079102\n",
      "Step 26: Training loss: 2.729944944381714\n",
      "Step 27: Training loss: 2.4914934635162354\n",
      "Step 28: Training loss: 3.81847882270813\n",
      "Step 29: Training loss: 3.6075265407562256\n",
      "Step 30: Training loss: 2.51784610748291\n",
      "Step 31: Training loss: 3.580321788787842\n",
      "Step 32: Training loss: 5.641279220581055\n",
      "Step 33: Training loss: 4.05225944519043\n",
      "Step 34: Training loss: 5.126191139221191\n",
      "Step 35: Training loss: 3.8489253520965576\n",
      "Step 36: Training loss: 3.821357011795044\n",
      "Step 37: Training loss: 2.693329095840454\n",
      "Step 38: Training loss: 4.214120388031006\n",
      "Step 39: Training loss: 4.058706283569336\n",
      "Step 40: Training loss: 3.9380414485931396\n",
      "Step 41: Training loss: 4.787463665008545\n",
      "Step 42: Training loss: 3.598402976989746\n",
      "Step 43: Training loss: 3.826866626739502\n",
      "Step 44: Training loss: 4.32150411605835\n",
      "Step 45: Training loss: 4.467276096343994\n",
      "Step 46: Training loss: 2.463318347930908\n",
      "Step 47: Training loss: 5.280038833618164\n",
      "Step 48: Training loss: 4.666024208068848\n",
      "Step 49: Training loss: 2.565520763397217\n",
      "Step 50: Training loss: 4.962170124053955\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-50\n",
      "Step 51: Training loss: 3.3811612129211426\n",
      "Step 52: Training loss: 5.28570556640625\n",
      "Step 53: Training loss: 5.2420244216918945\n",
      "Step 54: Training loss: 2.632535219192505\n",
      "Step 55: Training loss: 4.220305919647217\n",
      "Step 56: Training loss: 2.764474630355835\n",
      "Step 57: Training loss: 2.5848491191864014\n",
      "Step 58: Training loss: 3.7232139110565186\n",
      "Step 59: Training loss: 2.7400641441345215\n",
      "Step 60: Training loss: 2.487377166748047\n",
      "Step 61: Training loss: 3.3406312465667725\n",
      "Step 62: Training loss: 2.6332530975341797\n",
      "Step 63: Training loss: 3.9064948558807373\n",
      "Step 64: Training loss: 2.6563351154327393\n",
      "Step 65: Training loss: 2.6827635765075684\n",
      "Step 66: Training loss: 4.76471471786499\n",
      "Step 67: Training loss: 4.2145586013793945\n",
      "Step 68: Training loss: 4.275505542755127\n",
      "Step 69: Training loss: 3.690969467163086\n",
      "Step 70: Training loss: 2.554755210876465\n",
      "Step 71: Training loss: 5.241278171539307\n",
      "Step 72: Training loss: 3.1525979042053223\n",
      "Step 73: Training loss: 4.974734306335449\n",
      "Step 74: Training loss: 2.631500005722046\n",
      "Step 75: Training loss: 4.1265788078308105\n",
      "Step 76: Training loss: 3.27059268951416\n",
      "Step 77: Training loss: 3.607203960418701\n",
      "Step 78: Training loss: 5.133242607116699\n",
      "Step 79: Training loss: 2.891266345977783\n",
      "Step 80: Training loss: 3.930969715118408\n",
      "Step 81: Training loss: 2.9074883460998535\n",
      "Step 82: Training loss: 2.3425445556640625\n",
      "Step 83: Training loss: 2.424373149871826\n",
      "Step 84: Training loss: 2.5105113983154297\n",
      "Step 85: Training loss: 3.904714345932007\n",
      "Step 86: Training loss: 2.448422431945801\n",
      "Step 87: Training loss: 3.7536051273345947\n",
      "Step 88: Training loss: 2.5478415489196777\n",
      "Step 89: Training loss: 3.8018276691436768\n",
      "Step 90: Training loss: 3.4872090816497803\n",
      "Step 91: Training loss: 3.764008045196533\n",
      "Step 92: Training loss: 3.851731538772583\n",
      "Step 93: Training loss: 4.7815656661987305\n",
      "Step 94: Training loss: 2.7910518646240234\n",
      "Step 95: Training loss: 3.9229676723480225\n",
      "Step 96: Training loss: 3.38454270362854\n",
      "Step 97: Training loss: 2.693915605545044\n",
      "Step 98: Training loss: 2.56679368019104\n",
      "Step 99: Training loss: 4.144118785858154\n",
      "Step 100: Training loss: 3.765260934829712\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-100\n",
      "Step 101: Training loss: 3.521470308303833\n",
      "Step 102: Training loss: 4.568663597106934\n",
      "Step 103: Training loss: 4.03944730758667\n",
      "Step 104: Training loss: 6.013891696929932\n",
      "Step 105: Training loss: 4.021867752075195\n",
      "Step 106: Training loss: 3.18224835395813\n",
      "Step 107: Training loss: 2.6710822582244873\n",
      "Step 108: Training loss: 3.8485054969787598\n",
      "Step 109: Training loss: 3.260368585586548\n",
      "Step 110: Training loss: 2.426685094833374\n",
      "Step 111: Training loss: 2.7489712238311768\n",
      "Step 112: Training loss: 3.7866129875183105\n",
      "Step 113: Training loss: 3.4455580711364746\n",
      "Step 114: Training loss: 4.1089768409729\n",
      "Step 115: Training loss: 2.6091229915618896\n",
      "Step 116: Training loss: 3.672438621520996\n",
      "Step 117: Training loss: 2.893188238143921\n",
      "Step 118: Training loss: 4.455652236938477\n",
      "Step 119: Training loss: 4.204966068267822\n",
      "Step 120: Training loss: 4.187316417694092\n",
      "Step 121: Training loss: 4.06634521484375\n",
      "Step 122: Training loss: 4.376612663269043\n",
      "Step 123: Training loss: 3.6426334381103516\n",
      "Step 124: Training loss: 3.157904624938965\n",
      "Step 125: Training loss: 2.832246780395508\n",
      "Step 126: Training loss: 6.143277168273926\n",
      "Step 127: Training loss: 2.5634024143218994\n",
      "Step 128: Training loss: 3.8152220249176025\n",
      "Step 129: Training loss: 3.810441732406616\n",
      "Step 130: Training loss: 3.0573301315307617\n",
      "Step 131: Training loss: 3.726749897003174\n",
      "Step 132: Training loss: 4.344442844390869\n",
      "Step 133: Training loss: 2.78678297996521\n",
      "Step 134: Training loss: 3.830873727798462\n",
      "Step 135: Training loss: 3.343320369720459\n",
      "Step 136: Training loss: 2.8757755756378174\n",
      "Step 137: Training loss: 4.749166011810303\n",
      "Step 138: Training loss: 6.023928165435791\n",
      "Step 139: Training loss: 2.846526861190796\n",
      "Step 140: Training loss: 3.842848062515259\n",
      "Step 141: Training loss: 3.1480538845062256\n",
      "Step 142: Training loss: 2.827148199081421\n",
      "Step 143: Training loss: 2.511194944381714\n",
      "Step 144: Training loss: 2.9422762393951416\n",
      "Step 145: Training loss: 2.881984233856201\n",
      "Step 146: Training loss: 2.5935118198394775\n",
      "Step 147: Training loss: 2.5237927436828613\n",
      "Step 148: Training loss: 3.229160785675049\n",
      "Step 149: Training loss: 3.294451951980591\n",
      "Step 150: Training loss: 2.610912322998047\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-150\n",
      "Step 151: Training loss: 3.4175586700439453\n",
      "Step 152: Training loss: 3.9117960929870605\n",
      "Step 153: Training loss: 3.7254080772399902\n",
      "Step 154: Training loss: 3.4530842304229736\n",
      "Step 155: Training loss: 2.59443998336792\n",
      "Step 156: Training loss: 2.6566450595855713\n",
      "Step 157: Training loss: 3.389040231704712\n",
      "Step 158: Training loss: 2.7518467903137207\n",
      "Step 159: Training loss: 2.6145718097686768\n",
      "Step 160: Training loss: 3.3763201236724854\n",
      "Step 161: Training loss: 2.8326332569122314\n",
      "Step 162: Training loss: 4.552288055419922\n",
      "Step 163: Training loss: 3.498802661895752\n",
      "Step 164: Training loss: 2.603945255279541\n",
      "Step 165: Training loss: 2.5378639698028564\n",
      "Step 166: Training loss: 2.4760196208953857\n",
      "Step 167: Training loss: 2.653125524520874\n",
      "Step 168: Training loss: 2.743288040161133\n",
      "Step 169: Training loss: 3.520292282104492\n",
      "Step 170: Training loss: 5.39732551574707\n",
      "Step 171: Training loss: 3.4186289310455322\n",
      "Step 172: Training loss: 2.8445749282836914\n",
      "Step 173: Training loss: 2.653805732727051\n",
      "Step 174: Training loss: 3.4830520153045654\n",
      "Step 175: Training loss: 5.235860347747803\n",
      "Step 176: Training loss: 2.580221652984619\n",
      "Step 177: Training loss: 4.121649742126465\n",
      "Step 178: Training loss: 5.076104640960693\n",
      "Step 179: Training loss: 3.296262502670288\n",
      "Step 180: Training loss: 6.08684778213501\n",
      "Step 181: Training loss: 5.034898281097412\n",
      "Step 182: Training loss: 4.528494358062744\n",
      "Step 183: Training loss: 3.2134904861450195\n",
      "Step 184: Training loss: 3.6763126850128174\n",
      "Step 185: Training loss: 2.4669034481048584\n",
      "Step 186: Training loss: 4.986410140991211\n",
      "Step 187: Training loss: 2.511763095855713\n",
      "Step 188: Training loss: 4.654743194580078\n",
      "Step 189: Training loss: 5.2477803230285645\n",
      "Step 190: Training loss: 3.6797828674316406\n",
      "Step 191: Training loss: 3.590315103530884\n",
      "Step 192: Training loss: 2.434380292892456\n",
      "Step 193: Training loss: 2.844119071960449\n",
      "Step 194: Training loss: 6.000481605529785\n",
      "Step 195: Training loss: 5.600741863250732\n",
      "Step 196: Training loss: 2.58699107170105\n",
      "Step 197: Training loss: 2.7355763912200928\n",
      "Step 198: Training loss: 3.1451265811920166\n",
      "Step 199: Training loss: 2.974684238433838\n",
      "Step 200: Training loss: 3.9699461460113525\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-200\n",
      "Step 201: Training loss: 3.9646663665771484\n",
      "Step 202: Training loss: 3.111462354660034\n",
      "Step 203: Training loss: 2.5558431148529053\n",
      "Step 204: Training loss: 4.3930344581604\n",
      "Step 205: Training loss: 4.310277462005615\n",
      "Step 206: Training loss: 3.8639779090881348\n",
      "Step 207: Training loss: 3.2633426189422607\n",
      "Step 208: Training loss: 3.9619739055633545\n",
      "Step 209: Training loss: 3.7145376205444336\n",
      "Step 210: Training loss: 5.463233947753906\n",
      "Step 211: Training loss: 3.728212356567383\n",
      "Step 212: Training loss: 3.35843825340271\n",
      "Step 213: Training loss: 4.300319671630859\n",
      "Step 214: Training loss: 4.418875217437744\n",
      "Step 215: Training loss: 3.513406753540039\n",
      "Step 216: Training loss: 3.073659896850586\n",
      "Step 217: Training loss: 3.076261520385742\n",
      "Step 218: Training loss: 3.1590723991394043\n",
      "Step 219: Training loss: 5.25730562210083\n",
      "Step 220: Training loss: 2.7048158645629883\n",
      "Step 221: Training loss: 4.684842586517334\n",
      "Step 222: Training loss: 4.343078136444092\n",
      "Step 223: Training loss: 3.710286855697632\n",
      "Step 224: Training loss: 5.169207572937012\n",
      "Step 225: Training loss: 5.97942590713501\n",
      "Step 226: Training loss: 2.7164804935455322\n",
      "Step 227: Training loss: 3.1430859565734863\n",
      "Step 228: Training loss: 2.60961651802063\n",
      "Step 229: Training loss: 2.4153406620025635\n",
      "Step 230: Training loss: 3.438795328140259\n",
      "Step 231: Training loss: 3.9056942462921143\n",
      "Step 232: Training loss: 3.909226655960083\n",
      "Step 233: Training loss: 6.308176040649414\n",
      "Step 234: Training loss: 3.7600655555725098\n",
      "Step 235: Training loss: 2.6796035766601562\n",
      "Step 236: Training loss: 3.809436798095703\n",
      "Step 237: Training loss: 3.0619490146636963\n",
      "Step 238: Training loss: 2.6737172603607178\n",
      "Step 239: Training loss: 3.923844337463379\n",
      "Step 240: Training loss: 5.952051639556885\n",
      "Step 241: Training loss: 4.288846969604492\n",
      "Step 242: Training loss: 3.677805185317993\n",
      "Step 243: Training loss: 2.61220645904541\n",
      "Step 244: Training loss: 2.597348690032959\n",
      "Step 245: Training loss: 4.771455764770508\n",
      "Step 246: Training loss: 4.9509196281433105\n",
      "Step 247: Training loss: 3.132549285888672\n",
      "Step 248: Training loss: 2.478415012359619\n",
      "Step 249: Training loss: 5.2926740646362305\n",
      "Step 250: Training loss: 2.4339020252227783\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-250\n",
      "Step 251: Training loss: 4.286094665527344\n",
      "Step 252: Training loss: 3.4185616970062256\n",
      "Step 253: Training loss: 4.033117294311523\n",
      "Step 254: Training loss: 2.6316864490509033\n",
      "Step 255: Training loss: 3.8540937900543213\n",
      "Step 256: Training loss: 3.8738186359405518\n",
      "Step 257: Training loss: 2.489928722381592\n",
      "Step 258: Training loss: 2.658935785293579\n",
      "Step 259: Training loss: 2.5793566703796387\n",
      "Step 260: Training loss: 2.6903648376464844\n",
      "Step 261: Training loss: 2.9268839359283447\n",
      "Step 262: Training loss: 5.789217472076416\n",
      "Step 263: Training loss: 3.770817279815674\n",
      "Step 264: Training loss: 4.849967002868652\n",
      "Step 265: Training loss: 2.550799608230591\n",
      "Step 266: Training loss: 3.1800038814544678\n",
      "Step 267: Training loss: 4.7217793464660645\n",
      "Step 268: Training loss: 4.904458522796631\n",
      "Step 269: Training loss: 4.393453598022461\n",
      "Step 270: Training loss: 4.712090969085693\n",
      "Step 271: Training loss: 3.8453831672668457\n",
      "Step 272: Training loss: 4.529865264892578\n",
      "Step 273: Training loss: 3.697338104248047\n",
      "Step 274: Training loss: 4.821763038635254\n",
      "Step 275: Training loss: 4.694876670837402\n",
      "Step 276: Training loss: 2.4212465286254883\n",
      "Step 277: Training loss: 3.890493869781494\n",
      "Step 278: Training loss: 3.7302870750427246\n",
      "Step 279: Training loss: 2.665241241455078\n",
      "Step 280: Training loss: 3.6723787784576416\n",
      "Step 281: Training loss: 4.439889907836914\n",
      "Step 282: Training loss: 3.3931708335876465\n",
      "Step 283: Training loss: 3.4062132835388184\n",
      "Step 284: Training loss: 3.572448492050171\n",
      "Step 285: Training loss: 3.2668895721435547\n",
      "Step 286: Training loss: 3.6741411685943604\n",
      "Step 287: Training loss: 3.5381569862365723\n",
      "Step 288: Training loss: 4.229747295379639\n",
      "Step 289: Training loss: 4.590686321258545\n",
      "Step 290: Training loss: 3.723158359527588\n",
      "Step 291: Training loss: 3.7680678367614746\n",
      "Step 292: Training loss: 2.5042197704315186\n",
      "Step 293: Training loss: 2.481569290161133\n",
      "Step 294: Training loss: 2.4947805404663086\n",
      "Step 295: Training loss: 3.3840503692626953\n",
      "Step 296: Training loss: 3.9115421772003174\n",
      "Step 297: Training loss: 2.6763510704040527\n",
      "Step 298: Training loss: 3.3969295024871826\n",
      "Step 299: Training loss: 3.7167294025421143\n",
      "Step 300: Training loss: 3.982577323913574\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-300\n",
      "Step 301: Training loss: 2.3321919441223145\n",
      "Step 302: Training loss: 3.078688383102417\n",
      "Step 303: Training loss: 4.534861087799072\n",
      "Step 304: Training loss: 3.151989221572876\n",
      "Step 305: Training loss: 2.4946582317352295\n",
      "Step 306: Training loss: 4.946199417114258\n",
      "Step 307: Training loss: 6.7579731941223145\n",
      "Step 308: Training loss: 4.221103191375732\n",
      "Step 309: Training loss: 3.018275499343872\n",
      "Step 310: Training loss: 2.7701125144958496\n",
      "Step 311: Training loss: 3.584472179412842\n",
      "Step 312: Training loss: 2.4681928157806396\n",
      "Step 313: Training loss: 4.817287445068359\n",
      "Step 314: Training loss: 3.627751111984253\n",
      "Step 315: Training loss: 3.459123134613037\n",
      "Step 316: Training loss: 5.052933216094971\n",
      "Step 317: Training loss: 4.412552356719971\n",
      "Step 318: Training loss: 3.1709492206573486\n",
      "Step 319: Training loss: 3.3570120334625244\n",
      "Step 320: Training loss: 3.3696465492248535\n",
      "Step 321: Training loss: 2.4929544925689697\n",
      "Step 322: Training loss: 3.5271244049072266\n",
      "Step 323: Training loss: 2.857679605484009\n",
      "Step 324: Training loss: 3.7332000732421875\n",
      "Step 325: Training loss: 3.7133312225341797\n",
      "Step 326: Training loss: 2.5117132663726807\n",
      "Step 327: Training loss: 2.4279050827026367\n",
      "Step 328: Training loss: 4.772965431213379\n",
      "Step 329: Training loss: 3.3350369930267334\n",
      "Step 330: Training loss: 4.6616530418396\n",
      "Step 331: Training loss: 2.7110137939453125\n",
      "Step 332: Training loss: 2.888376235961914\n",
      "Step 333: Training loss: 4.324648380279541\n",
      "Step 334: Training loss: 4.461160182952881\n",
      "Step 335: Training loss: 4.8465704917907715\n",
      "Step 336: Training loss: 4.048112392425537\n",
      "Step 337: Training loss: 4.128194808959961\n",
      "Step 338: Training loss: 3.5795176029205322\n",
      "Step 339: Training loss: 3.5261096954345703\n",
      "Step 340: Training loss: 4.897587299346924\n",
      "Step 341: Training loss: 3.074143409729004\n",
      "Step 342: Training loss: 3.5259432792663574\n",
      "Step 343: Training loss: 2.750535011291504\n",
      "Step 344: Training loss: 2.6110799312591553\n",
      "Step 345: Training loss: 2.694533586502075\n",
      "Step 346: Training loss: 3.3692398071289062\n",
      "Step 347: Training loss: 3.945500373840332\n",
      "Step 348: Training loss: 2.4199371337890625\n",
      "Step 349: Training loss: 3.6598117351531982\n",
      "Step 350: Training loss: 4.4606614112854\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-350\n",
      "Step 351: Training loss: 2.757582664489746\n",
      "Step 352: Training loss: 2.7165110111236572\n",
      "Step 353: Training loss: 2.4343056678771973\n",
      "Step 354: Training loss: 2.552086114883423\n",
      "Step 355: Training loss: 3.146580696105957\n",
      "Step 356: Training loss: 4.013049602508545\n",
      "Step 357: Training loss: 2.6260650157928467\n",
      "Step 358: Training loss: 2.4821712970733643\n",
      "Step 359: Training loss: 5.0872087478637695\n",
      "Step 360: Training loss: 4.013859748840332\n",
      "Step 361: Training loss: 5.238961219787598\n",
      "Step 362: Training loss: 2.705929756164551\n",
      "Step 363: Training loss: 2.3657753467559814\n",
      "Step 364: Training loss: 3.7934396266937256\n",
      "Step 365: Training loss: 4.619778156280518\n",
      "Step 366: Training loss: 3.4888014793395996\n",
      "Step 367: Training loss: 3.6840827465057373\n",
      "Step 368: Training loss: 4.96138858795166\n",
      "Step 369: Training loss: 3.270806074142456\n",
      "Step 370: Training loss: 3.566183567047119\n",
      "Step 371: Training loss: 4.268444061279297\n",
      "Step 372: Training loss: 2.9564967155456543\n",
      "Step 373: Training loss: 2.286799907684326\n",
      "Step 374: Training loss: 3.9942080974578857\n",
      "Step 375: Training loss: 3.317047357559204\n",
      "Step 376: Training loss: 3.6802141666412354\n",
      "Step 377: Training loss: 5.051685333251953\n",
      "Step 378: Training loss: 4.308915138244629\n",
      "Step 379: Training loss: 3.021308422088623\n",
      "Step 380: Training loss: 4.6577019691467285\n",
      "Step 381: Training loss: 3.227806329727173\n",
      "Step 382: Training loss: 2.4282777309417725\n",
      "Step 383: Training loss: 2.3983969688415527\n",
      "Step 384: Training loss: 5.395556449890137\n",
      "Step 385: Training loss: 3.979051113128662\n",
      "Step 386: Training loss: 4.5345540046691895\n",
      "Step 387: Training loss: 3.419621229171753\n",
      "Step 388: Training loss: 5.5940165519714355\n",
      "Step 389: Training loss: 4.06998348236084\n",
      "Step 390: Training loss: 2.3977320194244385\n",
      "Step 391: Training loss: 3.5960493087768555\n",
      "Step 392: Training loss: 2.7167487144470215\n",
      "Step 393: Training loss: 2.602102041244507\n",
      "Step 394: Training loss: 5.356669902801514\n",
      "Step 395: Training loss: 2.9486403465270996\n",
      "Step 396: Training loss: 2.899902105331421\n",
      "Step 397: Training loss: 3.860659599304199\n",
      "Step 398: Training loss: 4.313647747039795\n",
      "Step 399: Training loss: 2.458534002304077\n",
      "Step 400: Training loss: 3.4993889331817627\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-400\n",
      "Step 401: Training loss: 2.8855621814727783\n",
      "Step 402: Training loss: 2.5107314586639404\n",
      "Step 403: Training loss: 3.5614395141601562\n",
      "Step 404: Training loss: 3.703176498413086\n",
      "Step 405: Training loss: 5.171176910400391\n",
      "Step 406: Training loss: 2.506345272064209\n",
      "Step 407: Training loss: 3.641737937927246\n",
      "Step 408: Training loss: 4.1328582763671875\n",
      "Step 409: Training loss: 4.7074174880981445\n",
      "Step 410: Training loss: 2.638814687728882\n",
      "Step 411: Training loss: 4.141139984130859\n",
      "Step 412: Training loss: 2.9584438800811768\n",
      "Step 413: Training loss: 3.544146776199341\n",
      "Step 414: Training loss: 2.89397931098938\n",
      "Step 415: Training loss: 2.9051549434661865\n",
      "Step 416: Training loss: 4.470108509063721\n",
      "Step 417: Training loss: 3.9489645957946777\n",
      "Step 418: Training loss: 3.6917662620544434\n",
      "Step 419: Training loss: 3.7997653484344482\n",
      "Step 420: Training loss: 2.3776304721832275\n",
      "Step 421: Training loss: 4.644196510314941\n",
      "Step 422: Training loss: 2.6812057495117188\n",
      "Step 423: Training loss: 2.4141664505004883\n",
      "Step 424: Training loss: 3.6886825561523438\n",
      "Step 425: Training loss: 2.55863618850708\n",
      "Step 426: Training loss: 4.697646617889404\n",
      "Step 427: Training loss: 2.3907101154327393\n",
      "Step 428: Training loss: 3.6636221408843994\n",
      "Step 429: Training loss: 6.007439613342285\n",
      "Step 430: Training loss: 2.312387466430664\n",
      "Step 431: Training loss: 3.661500930786133\n",
      "Step 432: Training loss: 5.490472793579102\n",
      "Step 433: Training loss: 3.9722793102264404\n",
      "Step 434: Training loss: 4.7348504066467285\n",
      "Step 435: Training loss: 3.7132339477539062\n",
      "Step 436: Training loss: 5.093135833740234\n",
      "Step 437: Training loss: 3.7358715534210205\n",
      "Step 438: Training loss: 4.418683052062988\n",
      "Step 439: Training loss: 2.456664562225342\n",
      "Step 440: Training loss: 4.950304985046387\n",
      "Step 441: Training loss: 3.4666953086853027\n",
      "Step 442: Training loss: 3.3254668712615967\n",
      "Step 443: Training loss: 2.984286308288574\n",
      "Step 444: Training loss: 4.941775798797607\n",
      "Step 445: Training loss: 3.1465091705322266\n",
      "Step 446: Training loss: 5.133975982666016\n",
      "Step 447: Training loss: 4.398212432861328\n",
      "Step 448: Training loss: 2.348863124847412\n",
      "Step 449: Training loss: 2.4973678588867188\n",
      "Step 450: Training loss: 2.9067306518554688\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-450\n",
      "Step 451: Training loss: 4.112544536590576\n",
      "Step 452: Training loss: 3.2559077739715576\n",
      "Step 453: Training loss: 4.576483249664307\n",
      "Step 454: Training loss: 3.828108072280884\n",
      "Step 455: Training loss: 3.6923668384552\n",
      "Step 456: Training loss: 3.9250566959381104\n",
      "Step 457: Training loss: 3.1361334323883057\n",
      "Step 458: Training loss: 3.3578598499298096\n",
      "Step 459: Training loss: 2.671938419342041\n",
      "Step 460: Training loss: 2.8347115516662598\n",
      "Step 461: Training loss: 4.0017218589782715\n",
      "Step 462: Training loss: 4.573730945587158\n",
      "Step 463: Training loss: 4.32366943359375\n",
      "Step 464: Training loss: 4.738170623779297\n",
      "Step 465: Training loss: 4.737765789031982\n",
      "Step 466: Training loss: 4.985713958740234\n",
      "Step 467: Training loss: 2.857001781463623\n",
      "Step 468: Training loss: 2.4560368061065674\n",
      "Step 469: Training loss: 3.720546245574951\n",
      "Step 470: Training loss: 2.4810707569122314\n",
      "Step 471: Training loss: 2.48432993888855\n",
      "Step 472: Training loss: 2.578209161758423\n",
      "Step 473: Training loss: 2.4830987453460693\n",
      "Step 474: Training loss: 3.4103903770446777\n",
      "Step 475: Training loss: 2.6942360401153564\n",
      "Step 476: Training loss: 3.8819711208343506\n",
      "Step 477: Training loss: 4.30044412612915\n",
      "Step 478: Training loss: 2.7416813373565674\n",
      "Step 479: Training loss: 4.720975875854492\n",
      "Step 480: Training loss: 4.588216304779053\n",
      "Step 481: Training loss: 3.47369647026062\n",
      "Step 482: Training loss: 2.7625372409820557\n",
      "Step 483: Training loss: 4.653656959533691\n",
      "Step 484: Training loss: 3.7942042350769043\n",
      "Step 485: Training loss: 3.5752689838409424\n",
      "Step 486: Training loss: 4.48962926864624\n",
      "Step 487: Training loss: 5.0406036376953125\n",
      "Step 488: Training loss: 2.5103394985198975\n",
      "Step 489: Training loss: 5.157118797302246\n",
      "Step 490: Training loss: 3.7497987747192383\n",
      "Step 491: Training loss: 2.717637062072754\n",
      "Step 492: Training loss: 3.3500967025756836\n",
      "Step 493: Training loss: 2.713569164276123\n",
      "Step 494: Training loss: 3.497709274291992\n",
      "Step 495: Training loss: 2.341480255126953\n",
      "Step 496: Training loss: 2.370302677154541\n",
      "Step 497: Training loss: 3.7600760459899902\n",
      "Step 498: Training loss: 5.500374794006348\n",
      "Step 499: Training loss: 2.6132166385650635\n",
      "Step 500: Training loss: 3.9403629302978516\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-500\n",
      "Step 501: Training loss: 3.2555902004241943\n",
      "Step 502: Training loss: 2.4318175315856934\n",
      "Step 503: Training loss: 4.577314376831055\n",
      "Step 504: Training loss: 3.966057062149048\n",
      "Step 505: Training loss: 5.4221320152282715\n",
      "Step 506: Training loss: 2.789182662963867\n",
      "Step 507: Training loss: 3.5385842323303223\n",
      "Step 508: Training loss: 3.6038105487823486\n",
      "Step 509: Training loss: 3.135021924972534\n",
      "Step 510: Training loss: 5.005702495574951\n",
      "Step 511: Training loss: 2.606024980545044\n",
      "Step 512: Training loss: 2.4572203159332275\n",
      "Step 513: Training loss: 3.690828323364258\n",
      "Step 514: Training loss: 4.1787800788879395\n",
      "Step 515: Training loss: 3.3652234077453613\n",
      "Step 516: Training loss: 3.9212539196014404\n",
      "Step 517: Training loss: 2.3579635620117188\n",
      "Step 518: Training loss: 2.374103307723999\n",
      "Step 519: Training loss: 3.262532949447632\n",
      "Step 520: Training loss: 4.825642108917236\n",
      "Step 521: Training loss: 2.480099678039551\n",
      "Step 522: Training loss: 2.838548421859741\n",
      "Step 523: Training loss: 3.3659794330596924\n",
      "Step 524: Training loss: 4.065201282501221\n",
      "Step 525: Training loss: 3.180461883544922\n",
      "Step 526: Training loss: 3.391822099685669\n",
      "Step 527: Training loss: 3.8269476890563965\n",
      "Step 528: Training loss: 4.051763534545898\n",
      "Step 529: Training loss: 3.0142366886138916\n",
      "Step 530: Training loss: 3.0743062496185303\n",
      "Step 531: Training loss: 2.5043251514434814\n",
      "Step 532: Training loss: 4.63988733291626\n",
      "Step 533: Training loss: 2.98808217048645\n",
      "Step 534: Training loss: 3.393709421157837\n",
      "Step 535: Training loss: 2.5417511463165283\n",
      "Step 536: Training loss: 3.6415750980377197\n",
      "Step 537: Training loss: 2.2550055980682373\n",
      "Step 538: Training loss: 2.951077699661255\n",
      "Step 539: Training loss: 2.71883225440979\n",
      "Step 540: Training loss: 3.815666437149048\n",
      "Step 541: Training loss: 2.57236647605896\n",
      "Step 542: Training loss: 3.4910888671875\n",
      "Step 543: Training loss: 4.553657531738281\n",
      "Step 544: Training loss: 3.7177233695983887\n",
      "Step 545: Training loss: 2.6542465686798096\n",
      "Step 546: Training loss: 2.439495325088501\n",
      "Step 547: Training loss: 4.070314407348633\n",
      "Step 548: Training loss: 2.346128225326538\n",
      "Step 549: Training loss: 4.384676456451416\n",
      "Step 550: Training loss: 3.5873682498931885\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-550\n",
      "Step 551: Training loss: 2.5052006244659424\n",
      "Step 552: Training loss: 2.5534980297088623\n",
      "Step 553: Training loss: 2.6266183853149414\n",
      "Step 554: Training loss: 3.327181339263916\n",
      "Step 555: Training loss: 3.546140432357788\n",
      "Step 556: Training loss: 3.5416481494903564\n",
      "Step 557: Training loss: 2.4442455768585205\n",
      "Step 558: Training loss: 3.8065946102142334\n",
      "Step 559: Training loss: 3.15895676612854\n",
      "Step 560: Training loss: 4.545616149902344\n",
      "Step 561: Training loss: 4.892042636871338\n",
      "Step 562: Training loss: 4.52752161026001\n",
      "Step 563: Training loss: 2.566298723220825\n",
      "Step 564: Training loss: 5.135422706604004\n",
      "Step 565: Training loss: 2.543409824371338\n",
      "Step 566: Training loss: 2.4672763347625732\n",
      "Step 567: Training loss: 2.634040594100952\n",
      "Step 568: Training loss: 4.436173915863037\n",
      "Step 569: Training loss: 3.516512632369995\n",
      "Step 570: Training loss: 2.599477767944336\n",
      "Step 571: Training loss: 3.850254535675049\n",
      "Step 572: Training loss: 3.0141570568084717\n",
      "Step 573: Training loss: 3.93746280670166\n",
      "Step 574: Training loss: 2.3601014614105225\n",
      "Step 575: Training loss: 2.8344838619232178\n",
      "Step 576: Training loss: 2.355705976486206\n",
      "Step 577: Training loss: 3.510928153991699\n",
      "Step 578: Training loss: 3.935678243637085\n",
      "Step 579: Training loss: 2.6026296615600586\n",
      "Step 580: Training loss: 2.5837466716766357\n",
      "Step 581: Training loss: 3.4177982807159424\n",
      "Step 582: Training loss: 3.739861249923706\n",
      "Step 583: Training loss: 2.438180685043335\n",
      "Step 584: Training loss: 3.6438405513763428\n",
      "Step 585: Training loss: 2.4545693397521973\n",
      "Step 586: Training loss: 2.41874098777771\n",
      "Step 587: Training loss: 4.0373921394348145\n",
      "Step 588: Training loss: 3.243929862976074\n",
      "Step 589: Training loss: 4.254244804382324\n",
      "Step 590: Training loss: 6.3025431632995605\n",
      "Step 591: Training loss: 4.901648044586182\n",
      "Step 592: Training loss: 4.01870584487915\n",
      "Step 593: Training loss: 2.5009050369262695\n",
      "Step 594: Training loss: 4.396413326263428\n",
      "Step 595: Training loss: 2.2469871044158936\n",
      "Step 596: Training loss: 2.411245584487915\n",
      "Step 597: Training loss: 4.127226829528809\n",
      "Step 598: Training loss: 2.3424694538116455\n",
      "Step 599: Training loss: 2.4180493354797363\n",
      "Step 600: Training loss: 3.694444417953491\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-600\n",
      "Step 601: Training loss: 2.5164706707000732\n",
      "Step 602: Training loss: 3.9262301921844482\n",
      "Step 603: Training loss: 4.736379623413086\n",
      "Step 604: Training loss: 2.4925174713134766\n",
      "Step 605: Training loss: 2.71832537651062\n",
      "Step 606: Training loss: 4.875292778015137\n",
      "Step 607: Training loss: 2.5790092945098877\n",
      "Step 608: Training loss: 5.521395683288574\n",
      "Step 609: Training loss: 3.6167290210723877\n",
      "Step 610: Training loss: 4.020533561706543\n",
      "Step 611: Training loss: 2.4895572662353516\n",
      "Step 612: Training loss: 3.990462303161621\n",
      "Step 613: Training loss: 3.083733081817627\n",
      "Step 614: Training loss: 2.71718168258667\n",
      "Step 615: Training loss: 3.12355375289917\n",
      "Step 616: Training loss: 4.49656867980957\n",
      "Step 617: Training loss: 2.643315076828003\n",
      "Step 618: Training loss: 3.026289701461792\n",
      "Step 619: Training loss: 4.491591930389404\n",
      "Step 620: Training loss: 2.828446626663208\n",
      "Step 621: Training loss: 3.2286009788513184\n",
      "Step 622: Training loss: 2.405172824859619\n",
      "Step 623: Training loss: 3.853217363357544\n",
      "Step 624: Training loss: 2.43243670463562\n",
      "Step 625: Training loss: 3.8400466442108154\n",
      "Step 626: Training loss: 5.211068630218506\n",
      "Step 627: Training loss: 4.459021091461182\n",
      "Step 628: Training loss: 2.3779990673065186\n",
      "Step 629: Training loss: 2.5425777435302734\n",
      "Step 630: Training loss: 3.760101795196533\n",
      "Step 631: Training loss: 3.0589394569396973\n",
      "Step 632: Training loss: 2.707942247390747\n",
      "Step 633: Training loss: 2.4861738681793213\n",
      "Step 634: Training loss: 3.5170791149139404\n",
      "Step 635: Training loss: 3.283665180206299\n",
      "Step 636: Training loss: 5.529541492462158\n",
      "Step 637: Training loss: 2.450686454772949\n",
      "Step 638: Training loss: 2.64617919921875\n",
      "Step 639: Training loss: 4.193204879760742\n",
      "Step 640: Training loss: 2.2557168006896973\n",
      "Step 641: Training loss: 3.7732560634613037\n",
      "Step 642: Training loss: 3.8549978733062744\n",
      "Step 643: Training loss: 3.5492639541625977\n",
      "Step 644: Training loss: 4.424907207489014\n",
      "Step 645: Training loss: 4.208432197570801\n",
      "Step 646: Training loss: 2.3538098335266113\n",
      "Step 647: Training loss: 4.5868635177612305\n",
      "Step 648: Training loss: 2.423788547515869\n",
      "Step 649: Training loss: 4.809474468231201\n",
      "Step 650: Training loss: 3.574814558029175\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-650\n",
      "Step 651: Training loss: 2.264683723449707\n",
      "Step 652: Training loss: 2.3454737663269043\n",
      "Step 653: Training loss: 2.663727045059204\n",
      "Step 654: Training loss: 4.821145057678223\n",
      "Step 655: Training loss: 4.04722785949707\n",
      "Step 656: Training loss: 3.701913833618164\n",
      "Step 657: Training loss: 3.829435110092163\n",
      "Step 658: Training loss: 4.368016719818115\n",
      "Step 659: Training loss: 3.109034538269043\n",
      "Step 660: Training loss: 4.987621784210205\n",
      "Step 661: Training loss: 6.883548259735107\n",
      "Step 662: Training loss: 3.018592596054077\n",
      "Step 663: Training loss: 2.378345489501953\n",
      "Step 664: Training loss: 3.521972179412842\n",
      "Step 665: Training loss: 3.568225860595703\n",
      "Step 666: Training loss: 5.0353617668151855\n",
      "Step 667: Training loss: 3.0091986656188965\n",
      "Step 668: Training loss: 2.358538866043091\n",
      "Step 669: Training loss: 2.4166412353515625\n",
      "Step 670: Training loss: 2.40749192237854\n",
      "Step 671: Training loss: 4.826958179473877\n",
      "Step 672: Training loss: 3.8824100494384766\n",
      "Step 673: Training loss: 3.7492971420288086\n",
      "Step 674: Training loss: 3.9456191062927246\n",
      "Step 675: Training loss: 3.4708399772644043\n",
      "Step 676: Training loss: 3.7001006603240967\n",
      "Step 677: Training loss: 3.0507354736328125\n",
      "Step 678: Training loss: 2.355970859527588\n",
      "Step 679: Training loss: 3.710083246231079\n",
      "Step 680: Training loss: 3.0032222270965576\n",
      "Step 681: Training loss: 5.2314581871032715\n",
      "Step 682: Training loss: 4.5557403564453125\n",
      "Step 683: Training loss: 4.741152286529541\n",
      "Step 684: Training loss: 3.4234771728515625\n",
      "Step 685: Training loss: 2.6368207931518555\n",
      "Step 686: Training loss: 3.6234655380249023\n",
      "Step 687: Training loss: 3.142047166824341\n",
      "Step 688: Training loss: 3.096790075302124\n",
      "Step 689: Training loss: 4.427380561828613\n",
      "Step 690: Training loss: 2.246246099472046\n",
      "Step 691: Training loss: 2.3593573570251465\n",
      "Step 692: Training loss: 3.7374587059020996\n",
      "Step 693: Training loss: 2.2666454315185547\n",
      "Step 694: Training loss: 2.4600234031677246\n",
      "Step 695: Training loss: 2.8949224948883057\n",
      "Step 696: Training loss: 3.2304089069366455\n",
      "Step 697: Training loss: 4.61966609954834\n",
      "Step 698: Training loss: 2.7112228870391846\n",
      "Step 699: Training loss: 3.585606575012207\n",
      "Step 700: Training loss: 3.3878605365753174\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-700\n",
      "Step 701: Training loss: 2.3354973793029785\n",
      "Step 702: Training loss: 4.527954578399658\n",
      "Step 703: Training loss: 2.507335901260376\n",
      "Step 704: Training loss: 3.6432993412017822\n",
      "Step 705: Training loss: 4.141640663146973\n",
      "Step 706: Training loss: 3.299959659576416\n",
      "Step 707: Training loss: 2.2896370887756348\n",
      "Step 708: Training loss: 2.4905025959014893\n",
      "Step 709: Training loss: 2.283797025680542\n",
      "Step 710: Training loss: 3.577421188354492\n",
      "Step 711: Training loss: 4.4626030921936035\n",
      "Step 712: Training loss: 2.506065845489502\n",
      "Step 713: Training loss: 2.3793580532073975\n",
      "Step 714: Training loss: 4.181248664855957\n",
      "Step 715: Training loss: 2.443561315536499\n",
      "Step 716: Training loss: 3.1993086338043213\n",
      "Step 717: Training loss: 3.805685520172119\n",
      "Step 718: Training loss: 2.5285305976867676\n",
      "Step 719: Training loss: 4.0169572830200195\n",
      "Step 720: Training loss: 4.500024795532227\n",
      "Step 721: Training loss: 3.672913074493408\n",
      "Step 722: Training loss: 2.414763927459717\n",
      "Step 723: Training loss: 2.362868070602417\n",
      "Step 724: Training loss: 4.393174648284912\n",
      "Step 725: Training loss: 2.5035996437072754\n",
      "Step 726: Training loss: 3.7863504886627197\n",
      "Step 727: Training loss: 2.495617628097534\n",
      "Step 728: Training loss: 3.127565622329712\n",
      "Step 729: Training loss: 2.3764865398406982\n",
      "Step 730: Training loss: 2.433116912841797\n",
      "Step 731: Training loss: 2.587002992630005\n",
      "Step 732: Training loss: 3.365887403488159\n",
      "Step 733: Training loss: 3.4365572929382324\n",
      "Step 734: Training loss: 4.098464488983154\n",
      "Step 735: Training loss: 2.608624219894409\n",
      "Step 736: Training loss: 3.365861415863037\n",
      "Step 737: Training loss: 2.382366418838501\n",
      "Step 738: Training loss: 4.28652286529541\n",
      "Step 739: Training loss: 3.1322684288024902\n",
      "Step 740: Training loss: 2.4509081840515137\n",
      "Step 741: Training loss: 2.3060879707336426\n",
      "Step 742: Training loss: 4.2294111251831055\n",
      "Step 743: Training loss: 3.4923081398010254\n",
      "Step 744: Training loss: 3.671969175338745\n",
      "Step 745: Training loss: 4.599277496337891\n",
      "Step 746: Training loss: 3.618668556213379\n",
      "Step 747: Training loss: 2.471393346786499\n",
      "Step 748: Training loss: 2.9517509937286377\n",
      "Step 749: Training loss: 2.509589195251465\n",
      "Step 750: Training loss: 2.3034822940826416\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-750\n",
      "Step 751: Training loss: 3.3222053050994873\n",
      "Step 752: Training loss: 2.3971505165100098\n",
      "Step 753: Training loss: 3.4624059200286865\n",
      "Step 754: Training loss: 3.7023940086364746\n",
      "Step 755: Training loss: 3.711735963821411\n",
      "Step 756: Training loss: 4.633620738983154\n",
      "Step 757: Training loss: 3.2493605613708496\n",
      "Step 758: Training loss: 4.568159103393555\n",
      "Step 759: Training loss: 4.687445163726807\n",
      "Step 760: Training loss: 3.4429948329925537\n",
      "Step 761: Training loss: 2.423490524291992\n",
      "Step 762: Training loss: 2.2003986835479736\n",
      "Step 763: Training loss: 2.3759536743164062\n",
      "Step 764: Training loss: 3.84206485748291\n",
      "Step 765: Training loss: 3.2011077404022217\n",
      "Step 766: Training loss: 3.5207340717315674\n",
      "Step 767: Training loss: 2.4193146228790283\n",
      "Step 768: Training loss: 4.882021903991699\n",
      "Step 769: Training loss: 2.4006314277648926\n",
      "Step 770: Training loss: 2.4398977756500244\n",
      "Step 771: Training loss: 3.838672637939453\n",
      "Step 772: Training loss: 4.926609992980957\n",
      "Step 773: Training loss: 2.7506561279296875\n",
      "Step 774: Training loss: 4.669239044189453\n",
      "Step 775: Training loss: 3.2062137126922607\n",
      "Step 776: Training loss: 2.391435384750366\n",
      "Step 777: Training loss: 2.535975456237793\n",
      "Step 778: Training loss: 2.424159049987793\n",
      "Step 779: Training loss: 3.172830820083618\n",
      "Step 780: Training loss: 3.425764799118042\n",
      "Step 781: Training loss: 2.6417393684387207\n",
      "Step 782: Training loss: 3.558798313140869\n",
      "Step 783: Training loss: 2.1877849102020264\n",
      "Step 784: Training loss: 3.095339775085449\n",
      "Step 785: Training loss: 2.592444658279419\n",
      "Step 786: Training loss: 3.219473123550415\n",
      "Step 787: Training loss: 2.346013307571411\n",
      "Step 788: Training loss: 2.3773422241210938\n",
      "Step 789: Training loss: 2.563077211380005\n",
      "Step 790: Training loss: 3.233252763748169\n",
      "Step 791: Training loss: 2.709852695465088\n",
      "Step 792: Training loss: 3.7463908195495605\n",
      "Step 793: Training loss: 2.233124017715454\n",
      "Step 794: Training loss: 3.0360805988311768\n",
      "Step 795: Training loss: 2.534323215484619\n",
      "Step 796: Training loss: 3.5903165340423584\n",
      "Step 797: Training loss: 4.088503360748291\n",
      "Step 798: Training loss: 4.117323875427246\n",
      "Step 799: Training loss: 3.3883612155914307\n",
      "Step 800: Training loss: 2.223419427871704\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-800\n",
      "Step 801: Training loss: 3.515869140625\n",
      "Step 802: Training loss: 2.673696517944336\n",
      "Step 803: Training loss: 3.3068690299987793\n",
      "Step 804: Training loss: 2.3100457191467285\n",
      "Step 805: Training loss: 4.573422431945801\n",
      "Step 806: Training loss: 2.510423421859741\n",
      "Step 807: Training loss: 3.1031394004821777\n",
      "Step 808: Training loss: 4.535036563873291\n",
      "Step 809: Training loss: 2.265446662902832\n",
      "Step 810: Training loss: 2.410045623779297\n",
      "Step 811: Training loss: 2.5780246257781982\n",
      "Step 812: Training loss: 2.362107038497925\n",
      "Step 813: Training loss: 3.5552141666412354\n",
      "Step 814: Training loss: 2.3252391815185547\n",
      "Step 815: Training loss: 5.260311126708984\n",
      "Step 816: Training loss: 2.3161683082580566\n",
      "Step 817: Training loss: 4.340176105499268\n",
      "Step 818: Training loss: 2.4748616218566895\n",
      "Step 819: Training loss: 2.3897833824157715\n",
      "Step 820: Training loss: 2.1773715019226074\n",
      "Step 821: Training loss: 2.5029220581054688\n",
      "Step 822: Training loss: 3.109576463699341\n",
      "Step 823: Training loss: 2.46366286277771\n",
      "Step 824: Training loss: 2.8372387886047363\n",
      "Step 825: Training loss: 3.8429644107818604\n",
      "Step 826: Training loss: 2.8328914642333984\n",
      "Step 827: Training loss: 3.557764768600464\n",
      "Step 828: Training loss: 2.1801984310150146\n",
      "Step 829: Training loss: 4.004114151000977\n",
      "Step 830: Training loss: 3.5579917430877686\n",
      "Step 831: Training loss: 2.2027993202209473\n",
      "Step 832: Training loss: 3.311680793762207\n",
      "Step 833: Training loss: 5.343959808349609\n",
      "Step 834: Training loss: 4.595848560333252\n",
      "Step 835: Training loss: 2.4149169921875\n",
      "Step 836: Training loss: 4.030702590942383\n",
      "Step 837: Training loss: 2.837092638015747\n",
      "Step 838: Training loss: 2.751070022583008\n",
      "Step 839: Training loss: 2.359703540802002\n",
      "Step 840: Training loss: 3.441375255584717\n",
      "Step 841: Training loss: 2.2737791538238525\n",
      "Step 842: Training loss: 3.149029493331909\n",
      "Step 843: Training loss: 2.9339380264282227\n",
      "Step 844: Training loss: 3.4393019676208496\n",
      "Step 845: Training loss: 3.9542171955108643\n",
      "Step 846: Training loss: 2.431043863296509\n",
      "Step 847: Training loss: 5.648369312286377\n",
      "Step 848: Training loss: 2.518401861190796\n",
      "Step 849: Training loss: 3.466897487640381\n",
      "Step 850: Training loss: 4.798838138580322\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-850\n",
      "Step 851: Training loss: 2.414304733276367\n",
      "Step 852: Training loss: 2.27091121673584\n",
      "Step 853: Training loss: 3.038378953933716\n",
      "Step 854: Training loss: 2.2984113693237305\n",
      "Step 855: Training loss: 2.2196409702301025\n",
      "Step 856: Training loss: 2.5325422286987305\n",
      "Step 857: Training loss: 3.421086072921753\n",
      "Step 858: Training loss: 3.452011823654175\n",
      "Step 859: Training loss: 2.3469626903533936\n",
      "Step 860: Training loss: 3.3694119453430176\n",
      "Step 861: Training loss: 2.1969828605651855\n",
      "Step 862: Training loss: 2.749481439590454\n",
      "Step 863: Training loss: 4.100679874420166\n",
      "Step 864: Training loss: 5.964709758758545\n",
      "Step 865: Training loss: 3.2430858612060547\n",
      "Step 866: Training loss: 2.8418493270874023\n",
      "Step 867: Training loss: 3.3309144973754883\n",
      "Step 868: Training loss: 3.884352207183838\n",
      "Step 869: Training loss: 5.837743282318115\n",
      "Step 870: Training loss: 2.414832592010498\n",
      "Step 871: Training loss: 4.8095197677612305\n",
      "Step 872: Training loss: 4.679253578186035\n",
      "Step 873: Training loss: 5.564664840698242\n",
      "Step 874: Training loss: 3.084885835647583\n",
      "Step 875: Training loss: 3.4077577590942383\n",
      "Step 876: Training loss: 3.3780622482299805\n",
      "Step 877: Training loss: 3.663220167160034\n",
      "Step 878: Training loss: 2.507568836212158\n",
      "Step 879: Training loss: 2.497340440750122\n",
      "Step 880: Training loss: 2.436006784439087\n",
      "Step 881: Training loss: 4.290170192718506\n",
      "Step 882: Training loss: 3.478807210922241\n",
      "Step 883: Training loss: 2.3124592304229736\n",
      "Step 884: Training loss: 2.191220283508301\n",
      "Step 885: Training loss: 5.9184675216674805\n",
      "Step 886: Training loss: 2.3998773097991943\n",
      "Step 887: Training loss: 2.6211256980895996\n",
      "Step 888: Training loss: 2.7366104125976562\n",
      "Step 889: Training loss: 3.495793581008911\n",
      "Step 890: Training loss: 2.2766146659851074\n",
      "Step 891: Training loss: 2.8386173248291016\n",
      "Step 892: Training loss: 2.366255283355713\n",
      "Step 893: Training loss: 2.2086243629455566\n",
      "Step 894: Training loss: 4.245529651641846\n",
      "Step 895: Training loss: 3.097722053527832\n",
      "Step 896: Training loss: 3.090484857559204\n",
      "Step 897: Training loss: 2.3867440223693848\n",
      "Step 898: Training loss: 2.6886117458343506\n",
      "Step 899: Training loss: 2.1714227199554443\n",
      "Step 900: Training loss: 2.6624691486358643\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-900\n",
      "Step 901: Training loss: 2.593691825866699\n",
      "Step 902: Training loss: 2.2657980918884277\n",
      "Step 903: Training loss: 5.01743221282959\n",
      "Step 904: Training loss: 4.221137046813965\n",
      "Step 905: Training loss: 3.2036359310150146\n",
      "Step 906: Training loss: 3.4676761627197266\n",
      "Step 907: Training loss: 2.2843799591064453\n",
      "Step 908: Training loss: 2.1820921897888184\n",
      "Step 909: Training loss: 3.1927173137664795\n",
      "Step 910: Training loss: 6.061958312988281\n",
      "Step 911: Training loss: 6.572764873504639\n",
      "Step 912: Training loss: 2.361112117767334\n",
      "Step 913: Training loss: 2.959829807281494\n",
      "Step 914: Training loss: 2.3431849479675293\n",
      "Step 915: Training loss: 2.6196422576904297\n",
      "Step 916: Training loss: 4.308975696563721\n",
      "Step 917: Training loss: 2.3172712326049805\n",
      "Step 918: Training loss: 2.2768495082855225\n",
      "Step 919: Training loss: 2.237318277359009\n",
      "Step 920: Training loss: 3.005624771118164\n",
      "Step 921: Training loss: 3.0712246894836426\n",
      "Step 922: Training loss: 3.529162645339966\n",
      "Step 923: Training loss: 2.770526170730591\n",
      "Step 924: Training loss: 3.516763925552368\n",
      "Step 925: Training loss: 6.576781749725342\n",
      "Step 926: Training loss: 3.099538564682007\n",
      "Step 927: Training loss: 2.7668957710266113\n",
      "Step 928: Training loss: 2.2458407878875732\n",
      "Step 929: Training loss: 2.170297861099243\n",
      "Step 930: Training loss: 3.1921188831329346\n",
      "Step 931: Training loss: 2.1887245178222656\n",
      "Step 932: Training loss: 4.158249855041504\n",
      "Step 933: Training loss: 5.394779205322266\n",
      "Step 934: Training loss: 2.4445993900299072\n",
      "Step 935: Training loss: 2.764961004257202\n",
      "Step 936: Training loss: 2.2641592025756836\n",
      "Step 937: Training loss: 3.434983253479004\n",
      "Step 938: Training loss: 3.4920084476470947\n",
      "Step 939: Training loss: 3.2979705333709717\n",
      "Step 940: Training loss: 3.2420642375946045\n",
      "Step 941: Training loss: 2.657352924346924\n",
      "Step 942: Training loss: 3.6589701175689697\n",
      "Step 943: Training loss: 4.8445587158203125\n",
      "Step 944: Training loss: 3.019482135772705\n",
      "Step 945: Training loss: 4.48392915725708\n",
      "Step 946: Training loss: 2.2696099281311035\n",
      "Step 947: Training loss: 3.394965171813965\n",
      "Step 948: Training loss: 3.8870601654052734\n",
      "Step 949: Training loss: 2.2178902626037598\n",
      "Step 950: Training loss: 3.135972499847412\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-950\n",
      "Step 951: Training loss: 4.532985687255859\n",
      "Step 952: Training loss: 3.6119778156280518\n",
      "Step 953: Training loss: 3.530627727508545\n",
      "Step 954: Training loss: 2.6937127113342285\n",
      "Step 955: Training loss: 2.59220027923584\n",
      "Step 956: Training loss: 4.631964683532715\n",
      "Step 957: Training loss: 3.463566780090332\n",
      "Step 958: Training loss: 3.012671709060669\n",
      "Step 959: Training loss: 2.15693998336792\n",
      "Step 960: Training loss: 3.1378889083862305\n",
      "Step 961: Training loss: 3.065620183944702\n",
      "Step 962: Training loss: 2.462172746658325\n",
      "Step 963: Training loss: 2.4368016719818115\n",
      "Step 964: Training loss: 3.189535617828369\n",
      "Step 965: Training loss: 3.252399444580078\n",
      "Step 966: Training loss: 2.446040153503418\n",
      "Step 967: Training loss: 3.3691279888153076\n",
      "Step 968: Training loss: 2.505326986312866\n",
      "Step 969: Training loss: 2.9188570976257324\n",
      "Step 970: Training loss: 4.794608116149902\n",
      "Step 971: Training loss: 4.088238716125488\n",
      "Step 972: Training loss: 4.226073741912842\n",
      "Step 973: Training loss: 3.321024179458618\n",
      "Step 974: Training loss: 2.4664995670318604\n",
      "Step 975: Training loss: 3.661438465118408\n",
      "Step 976: Training loss: 3.7999298572540283\n",
      "Step 977: Training loss: 2.5915045738220215\n",
      "Step 978: Training loss: 2.0284008979797363\n",
      "Step 979: Training loss: 3.1932132244110107\n",
      "Step 980: Training loss: 2.2431960105895996\n",
      "Step 981: Training loss: 3.4329864978790283\n",
      "Step 982: Training loss: 3.474027633666992\n",
      "Step 983: Training loss: 2.292067289352417\n",
      "Step 984: Training loss: 4.981838703155518\n",
      "Step 985: Training loss: 3.7570266723632812\n",
      "Step 986: Training loss: 3.4054558277130127\n",
      "Step 987: Training loss: 4.745448589324951\n",
      "Step 988: Training loss: 2.2945716381073\n",
      "Step 989: Training loss: 4.256102085113525\n",
      "Step 990: Training loss: 2.104058027267456\n",
      "Step 991: Training loss: 2.7087299823760986\n",
      "Step 992: Training loss: 5.154524803161621\n",
      "Step 993: Training loss: 3.4381840229034424\n",
      "Step 994: Training loss: 3.37290358543396\n",
      "Step 995: Training loss: 5.09647274017334\n",
      "Step 996: Training loss: 3.08187198638916\n",
      "Step 997: Training loss: 3.3044686317443848\n",
      "Step 998: Training loss: 2.8532772064208984\n",
      "Step 999: Training loss: 2.3872878551483154\n",
      "Step 1000: Training loss: 2.23470139503479\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1000\n",
      "Step 1001: Training loss: 2.328843593597412\n",
      "Step 1002: Training loss: 3.3952770233154297\n",
      "Step 1003: Training loss: 2.24997615814209\n",
      "Step 1004: Training loss: 2.8333613872528076\n",
      "Step 1005: Training loss: 4.452771186828613\n",
      "Step 1006: Training loss: 2.772085189819336\n",
      "Step 1007: Training loss: 5.539977073669434\n",
      "Step 1008: Training loss: 2.29495906829834\n",
      "Step 1009: Training loss: 2.787773609161377\n",
      "Step 1010: Training loss: 2.364816904067993\n",
      "Step 1011: Training loss: 2.390317440032959\n",
      "Step 1012: Training loss: 2.624912738800049\n",
      "Step 1013: Training loss: 2.9952752590179443\n",
      "Step 1014: Training loss: 3.4226670265197754\n",
      "Step 1015: Training loss: 3.994015693664551\n",
      "Step 1016: Training loss: 2.083544969558716\n",
      "Step 1017: Training loss: 3.114372730255127\n",
      "Step 1018: Training loss: 3.4867329597473145\n",
      "Step 1019: Training loss: 2.168504476547241\n",
      "Step 1020: Training loss: 2.1513094902038574\n",
      "Step 1021: Training loss: 2.3863158226013184\n",
      "Step 1022: Training loss: 4.164407253265381\n",
      "Step 1023: Training loss: 2.9796199798583984\n",
      "Step 1024: Training loss: 4.493019104003906\n",
      "Step 1025: Training loss: 2.3835620880126953\n",
      "Step 1026: Training loss: 2.4330708980560303\n",
      "Step 1027: Training loss: 3.0099966526031494\n",
      "Step 1028: Training loss: 2.104184865951538\n",
      "Step 1029: Training loss: 2.157442569732666\n",
      "Step 1030: Training loss: 4.6146464347839355\n",
      "Step 1031: Training loss: 2.415501117706299\n",
      "Step 1032: Training loss: 2.2583999633789062\n",
      "Step 1033: Training loss: 3.490924596786499\n",
      "Step 1034: Training loss: 2.7874655723571777\n",
      "Step 1035: Training loss: 2.9190149307250977\n",
      "Step 1036: Training loss: 4.440365314483643\n",
      "Step 1037: Training loss: 2.935377597808838\n",
      "Step 1038: Training loss: 2.282313585281372\n",
      "Step 1039: Training loss: 2.229475259780884\n",
      "Step 1040: Training loss: 3.4999351501464844\n",
      "Step 1041: Training loss: 2.5085248947143555\n",
      "Step 1042: Training loss: 2.920585870742798\n",
      "Step 1043: Training loss: 3.212759017944336\n",
      "Step 1044: Training loss: 3.080007791519165\n",
      "Step 1045: Training loss: 3.8001890182495117\n",
      "Step 1046: Training loss: 2.567612886428833\n",
      "Step 1047: Training loss: 3.072779893875122\n",
      "Step 1048: Training loss: 3.424351215362549\n",
      "Step 1049: Training loss: 4.1494669914245605\n",
      "Step 1050: Training loss: 2.9795775413513184\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1050\n",
      "Step 1051: Training loss: 2.1131229400634766\n",
      "Step 1052: Training loss: 3.1483688354492188\n",
      "Step 1053: Training loss: 3.3943793773651123\n",
      "Step 1054: Training loss: 2.158815860748291\n",
      "Step 1055: Training loss: 2.304219961166382\n",
      "Step 1056: Training loss: 3.192337989807129\n",
      "Step 1057: Training loss: 2.2011935710906982\n",
      "Step 1058: Training loss: 2.1325604915618896\n",
      "Step 1059: Training loss: 2.6955554485321045\n",
      "Step 1060: Training loss: 2.691683530807495\n",
      "Step 1061: Training loss: 4.348477840423584\n",
      "Step 1062: Training loss: 2.236736297607422\n",
      "Step 1063: Training loss: 4.656142711639404\n",
      "Step 1064: Training loss: 2.1265640258789062\n",
      "Step 1065: Training loss: 4.547019958496094\n",
      "Step 1066: Training loss: 3.8306021690368652\n",
      "Step 1067: Training loss: 2.5167460441589355\n",
      "Step 1068: Training loss: 3.8747615814208984\n",
      "Step 1069: Training loss: 3.797621726989746\n",
      "Step 1070: Training loss: 3.143251895904541\n",
      "Step 1071: Training loss: 3.1704518795013428\n",
      "Step 1072: Training loss: 2.2971179485321045\n",
      "Step 1073: Training loss: 4.556135177612305\n",
      "Step 1074: Training loss: 3.7331063747406006\n",
      "Step 1075: Training loss: 2.2081193923950195\n",
      "Step 1076: Training loss: 4.200439929962158\n",
      "Step 1077: Training loss: 2.7514970302581787\n",
      "Step 1078: Training loss: 3.979551315307617\n",
      "Step 1079: Training loss: 2.1544246673583984\n",
      "Step 1080: Training loss: 2.2075772285461426\n",
      "Step 1081: Training loss: 5.4579010009765625\n",
      "Step 1082: Training loss: 2.675462484359741\n",
      "Step 1083: Training loss: 3.3382625579833984\n",
      "Step 1084: Training loss: 2.317631483078003\n",
      "Step 1085: Training loss: 3.066920518875122\n",
      "Step 1086: Training loss: 3.0294885635375977\n",
      "Step 1087: Training loss: 3.6774582862854004\n",
      "Step 1088: Training loss: 3.4288456439971924\n",
      "Step 1089: Training loss: 2.6628975868225098\n",
      "Step 1090: Training loss: 2.167109966278076\n",
      "Step 1091: Training loss: 3.6543049812316895\n",
      "Step 1092: Training loss: 2.2209486961364746\n",
      "Step 1093: Training loss: 4.722240447998047\n",
      "Step 1094: Training loss: 3.265687942504883\n",
      "Step 1095: Training loss: 2.348414659500122\n",
      "Step 1096: Training loss: 2.2366139888763428\n",
      "Step 1097: Training loss: 3.1796786785125732\n",
      "Step 1098: Training loss: 2.444213628768921\n",
      "Step 1099: Training loss: 3.319772958755493\n",
      "Step 1100: Training loss: 3.785059928894043\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1100\n",
      "Step 1101: Training loss: 2.3132991790771484\n",
      "Step 1102: Training loss: 2.3235647678375244\n",
      "Step 1103: Training loss: 3.4363489151000977\n",
      "Step 1104: Training loss: 3.160202980041504\n",
      "Step 1105: Training loss: 2.0630369186401367\n",
      "Step 1106: Training loss: 3.4919588565826416\n",
      "Step 1107: Training loss: 2.427459955215454\n",
      "Step 1108: Training loss: 2.490135669708252\n",
      "Step 1109: Training loss: 2.408900260925293\n",
      "Step 1110: Training loss: 2.2188844680786133\n",
      "Step 1111: Training loss: 2.6882448196411133\n",
      "Step 1112: Training loss: 2.2498462200164795\n",
      "Step 1113: Training loss: 2.383183240890503\n",
      "Step 1114: Training loss: 3.394841194152832\n",
      "Step 1115: Training loss: 2.8935821056365967\n",
      "Step 1116: Training loss: 2.314162254333496\n",
      "Step 1117: Training loss: 2.8714728355407715\n",
      "Step 1118: Training loss: 4.708681583404541\n",
      "Step 1119: Training loss: 2.157170534133911\n",
      "Step 1120: Training loss: 2.2675015926361084\n",
      "Step 1121: Training loss: 2.1344425678253174\n",
      "Step 1122: Training loss: 2.7857236862182617\n",
      "Step 1123: Training loss: 3.604172706604004\n",
      "Step 1124: Training loss: 3.3421707153320312\n",
      "Step 1125: Training loss: 3.44553804397583\n",
      "Step 1126: Training loss: 4.048490047454834\n",
      "Step 1127: Training loss: 4.218816757202148\n",
      "Step 1128: Training loss: 3.4631853103637695\n",
      "Step 1129: Training loss: 3.278607130050659\n",
      "Step 1130: Training loss: 2.4348583221435547\n",
      "Step 1131: Training loss: 2.127200126647949\n",
      "Step 1132: Training loss: 4.770158290863037\n",
      "Step 1133: Training loss: 2.594597816467285\n",
      "Step 1134: Training loss: 3.4942948818206787\n",
      "Step 1135: Training loss: 3.2963857650756836\n",
      "Step 1136: Training loss: 3.461379051208496\n",
      "Step 1137: Training loss: 2.2118148803710938\n",
      "Step 1138: Training loss: 3.459423065185547\n",
      "Step 1139: Training loss: 3.635408639907837\n",
      "Step 1140: Training loss: 4.760663986206055\n",
      "Step 1141: Training loss: 3.9399352073669434\n",
      "Step 1142: Training loss: 2.960557699203491\n",
      "Step 1143: Training loss: 3.421628713607788\n",
      "Step 1144: Training loss: 4.283036708831787\n",
      "Step 1145: Training loss: 3.5917413234710693\n",
      "Step 1146: Training loss: 3.368238687515259\n",
      "Step 1147: Training loss: 2.289517641067505\n",
      "Step 1148: Training loss: 3.5873477458953857\n",
      "Step 1149: Training loss: 1.9820400476455688\n",
      "Step 1150: Training loss: 2.0722756385803223\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1150\n",
      "Step 1151: Training loss: 4.435505390167236\n",
      "Step 1152: Training loss: 2.3763275146484375\n",
      "Step 1153: Training loss: 2.190643787384033\n",
      "Step 1154: Training loss: 2.612405300140381\n",
      "Step 1155: Training loss: 3.5463972091674805\n",
      "Step 1156: Training loss: 3.4581634998321533\n",
      "Step 1157: Training loss: 2.0728518962860107\n",
      "Step 1158: Training loss: 2.204277276992798\n",
      "Step 1159: Training loss: 2.4828765392303467\n",
      "Step 1160: Training loss: 3.2011070251464844\n",
      "Step 1161: Training loss: 2.5975589752197266\n",
      "Step 1162: Training loss: 3.4055559635162354\n",
      "Step 1163: Training loss: 2.3084471225738525\n",
      "Step 1164: Training loss: 2.5507209300994873\n",
      "Step 1165: Training loss: 3.017918586730957\n",
      "Step 1166: Training loss: 2.356402635574341\n",
      "Step 1167: Training loss: 3.8510420322418213\n",
      "Step 1168: Training loss: 2.3625364303588867\n",
      "Step 1169: Training loss: 2.124180555343628\n",
      "Step 1170: Training loss: 2.866014242172241\n",
      "Step 1171: Training loss: 4.784774303436279\n",
      "Step 1172: Training loss: 3.359347343444824\n",
      "Step 1173: Training loss: 2.984137773513794\n",
      "Step 1174: Training loss: 2.738070011138916\n",
      "Step 1175: Training loss: 3.3285393714904785\n",
      "Step 1176: Training loss: 4.310770034790039\n",
      "Step 1177: Training loss: 3.1290318965911865\n",
      "Step 1178: Training loss: 3.917595863342285\n",
      "Step 1179: Training loss: 3.2479588985443115\n",
      "Step 1180: Training loss: 2.585277795791626\n",
      "Step 1181: Training loss: 3.5769054889678955\n",
      "Step 1182: Training loss: 3.366931676864624\n",
      "Step 1183: Training loss: 2.9743950366973877\n",
      "Step 1184: Training loss: 3.1048483848571777\n",
      "Step 1185: Training loss: 2.1601686477661133\n",
      "Step 1186: Training loss: 2.786653518676758\n",
      "Step 1187: Training loss: 3.965841293334961\n",
      "Step 1188: Training loss: 2.231391429901123\n",
      "Step 1189: Training loss: 2.1204802989959717\n",
      "Step 1190: Training loss: 2.9279730319976807\n",
      "Step 1191: Training loss: 2.204137086868286\n",
      "Step 1192: Training loss: 2.624532461166382\n",
      "Step 1193: Training loss: 4.361330986022949\n",
      "Step 1194: Training loss: 3.815824508666992\n",
      "Step 1195: Training loss: 3.9148824214935303\n",
      "Step 1196: Training loss: 3.6112990379333496\n",
      "Step 1197: Training loss: 4.88881778717041\n",
      "Step 1198: Training loss: 2.9390017986297607\n",
      "Step 1199: Training loss: 3.0299501419067383\n",
      "Step 1200: Training loss: 3.5682692527770996\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1200\n",
      "Step 1201: Training loss: 3.9081149101257324\n",
      "Step 1202: Training loss: 2.600775957107544\n",
      "Step 1203: Training loss: 2.7443904876708984\n",
      "Step 1204: Training loss: 3.210890769958496\n",
      "Step 1205: Training loss: 4.8151655197143555\n",
      "Step 1206: Training loss: 3.7064151763916016\n",
      "Step 1207: Training loss: 3.203339099884033\n",
      "Step 1208: Training loss: 3.326617956161499\n",
      "Step 1209: Training loss: 4.584341049194336\n",
      "Step 1210: Training loss: 3.0124294757843018\n",
      "Step 1211: Training loss: 3.6936023235321045\n",
      "Step 1212: Training loss: 2.8221445083618164\n",
      "Step 1213: Training loss: 2.2325637340545654\n",
      "Step 1214: Training loss: 2.6321535110473633\n",
      "Step 1215: Training loss: 2.451627731323242\n",
      "Step 1216: Training loss: 3.1842076778411865\n",
      "Step 1217: Training loss: 2.1800460815429688\n",
      "Step 1218: Training loss: 2.57705020904541\n",
      "Step 1219: Training loss: 5.681756019592285\n",
      "Step 1220: Training loss: 2.41534161567688\n",
      "Step 1221: Training loss: 4.359705924987793\n",
      "Step 1222: Training loss: 5.0667829513549805\n",
      "Step 1223: Training loss: 3.0313141345977783\n",
      "Step 1224: Training loss: 2.2713353633880615\n",
      "Step 1225: Training loss: 4.306878089904785\n",
      "Step 1226: Training loss: 2.316650152206421\n",
      "Step 1227: Training loss: 1.9824185371398926\n",
      "Step 1228: Training loss: 2.230648994445801\n",
      "Step 1229: Training loss: 2.6068079471588135\n",
      "Step 1230: Training loss: 2.1709437370300293\n",
      "Step 1231: Training loss: 4.925763130187988\n",
      "Step 1232: Training loss: 5.045569896697998\n",
      "Step 1233: Training loss: 2.5293102264404297\n",
      "Step 1234: Training loss: 3.436471939086914\n",
      "Step 1235: Training loss: 2.48976731300354\n",
      "Step 1236: Training loss: 3.4121994972229004\n",
      "Step 1237: Training loss: 3.3995590209960938\n",
      "Step 1238: Training loss: 4.039433002471924\n",
      "Step 1239: Training loss: 3.3062057495117188\n",
      "Step 1240: Training loss: 4.653799057006836\n",
      "Step 1241: Training loss: 3.483205556869507\n",
      "Step 1242: Training loss: 3.839538097381592\n",
      "Step 1243: Training loss: 2.146073579788208\n",
      "Step 1244: Training loss: 5.689192771911621\n",
      "Step 1245: Training loss: 3.950984239578247\n",
      "Step 1246: Training loss: 2.0696752071380615\n",
      "Step 1247: Training loss: 3.4691240787506104\n",
      "Step 1248: Training loss: 2.4163050651550293\n",
      "Step 1249: Training loss: 4.562646865844727\n",
      "Step 1250: Training loss: 2.7308452129364014\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1250\n",
      "Step 1251: Training loss: 3.014177083969116\n",
      "Step 1252: Training loss: 3.0495035648345947\n",
      "Step 1253: Training loss: 1.9303032159805298\n",
      "Step 1254: Training loss: 2.1141631603240967\n",
      "Step 1255: Training loss: 2.07673716545105\n",
      "Step 1256: Training loss: 2.5519962310791016\n",
      "Step 1257: Training loss: 3.794959783554077\n",
      "Step 1258: Training loss: 2.262218475341797\n",
      "Step 1259: Training loss: 2.0790605545043945\n",
      "Step 1260: Training loss: 2.1014461517333984\n",
      "Step 1261: Training loss: 2.419011354446411\n",
      "Step 1262: Training loss: 2.9744086265563965\n",
      "Step 1263: Training loss: 5.374085426330566\n",
      "Step 1264: Training loss: 3.17618465423584\n",
      "Step 1265: Training loss: 3.292666435241699\n",
      "Step 1266: Training loss: 2.2514867782592773\n",
      "Step 1267: Training loss: 2.9405925273895264\n",
      "Step 1268: Training loss: 2.3323299884796143\n",
      "Step 1269: Training loss: 3.2478208541870117\n",
      "Step 1270: Training loss: 3.3198931217193604\n",
      "Step 1271: Training loss: 2.0847675800323486\n",
      "Step 1272: Training loss: 4.731213092803955\n",
      "Step 1273: Training loss: 4.564191818237305\n",
      "Step 1274: Training loss: 2.764589548110962\n",
      "Step 1275: Training loss: 3.187882900238037\n",
      "Step 1276: Training loss: 3.189556837081909\n",
      "Step 1277: Training loss: 4.122929096221924\n",
      "Step 1278: Training loss: 2.6166913509368896\n",
      "Step 1279: Training loss: 3.8926749229431152\n",
      "Step 1280: Training loss: 2.3928232192993164\n",
      "Step 1281: Training loss: 3.0399017333984375\n",
      "Step 1282: Training loss: 2.0867340564727783\n",
      "Step 1283: Training loss: 4.820080757141113\n",
      "Step 1284: Training loss: 4.188422203063965\n",
      "Step 1285: Training loss: 1.9862176179885864\n",
      "Step 1286: Training loss: 2.96897292137146\n",
      "Step 1287: Training loss: 4.623629093170166\n",
      "Step 1288: Training loss: 2.503831386566162\n",
      "Step 1289: Training loss: 6.516716480255127\n",
      "Step 1290: Training loss: 4.0161662101745605\n",
      "Step 1291: Training loss: 2.0387675762176514\n",
      "Step 1292: Training loss: 3.708700180053711\n",
      "Step 1293: Training loss: 2.8640949726104736\n",
      "Step 1294: Training loss: 2.797515869140625\n",
      "Step 1295: Training loss: 2.88332462310791\n",
      "Step 1296: Training loss: 4.543365478515625\n",
      "Step 1297: Training loss: 2.3102898597717285\n",
      "Step 1298: Training loss: 3.120194435119629\n",
      "Step 1299: Training loss: 3.208559513092041\n",
      "Step 1300: Training loss: 4.007091045379639\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1300\n",
      "Step 1301: Training loss: 3.834386110305786\n",
      "Step 1302: Training loss: 3.1716361045837402\n",
      "Step 1303: Training loss: 3.3771114349365234\n",
      "Step 1304: Training loss: 4.470373153686523\n",
      "Step 1305: Training loss: 2.070159435272217\n",
      "Step 1306: Training loss: 2.1833744049072266\n",
      "Step 1307: Training loss: 3.1296684741973877\n",
      "Step 1308: Training loss: 3.984623908996582\n",
      "Step 1309: Training loss: 3.1804885864257812\n",
      "Step 1310: Training loss: 2.4460532665252686\n",
      "Step 1311: Training loss: 3.161729097366333\n",
      "Step 1312: Training loss: 2.626314401626587\n",
      "Step 1313: Training loss: 3.2364628314971924\n",
      "Step 1314: Training loss: 3.752797842025757\n",
      "Step 1315: Training loss: 3.060209035873413\n",
      "Step 1316: Training loss: 3.2919068336486816\n",
      "Step 1317: Training loss: 3.288344144821167\n",
      "Step 1318: Training loss: 3.184931755065918\n",
      "Step 1319: Training loss: 3.4641687870025635\n",
      "Step 1320: Training loss: 4.036294937133789\n",
      "Step 1321: Training loss: 1.9630995988845825\n",
      "Step 1322: Training loss: 3.8037965297698975\n",
      "Step 1323: Training loss: 4.1262006759643555\n",
      "Step 1324: Training loss: 2.98571515083313\n",
      "Step 1325: Training loss: 1.9944345951080322\n",
      "Step 1326: Training loss: 2.426771640777588\n",
      "Step 1327: Training loss: 2.0055060386657715\n",
      "Step 1328: Training loss: 4.349505424499512\n",
      "Step 1329: Training loss: 2.4263455867767334\n",
      "Step 1330: Training loss: 2.5577425956726074\n",
      "Step 1331: Training loss: 3.0422022342681885\n",
      "Step 1332: Training loss: 2.9261016845703125\n",
      "Step 1333: Training loss: 2.6548523902893066\n",
      "Step 1334: Training loss: 3.2710163593292236\n",
      "Step 1335: Training loss: 3.0078773498535156\n",
      "Step 1336: Training loss: 3.2844948768615723\n",
      "Step 1337: Training loss: 2.7694530487060547\n",
      "Step 1338: Training loss: 2.1877431869506836\n",
      "Step 1339: Training loss: 4.11862850189209\n",
      "Step 1340: Training loss: 3.8256442546844482\n",
      "Step 1341: Training loss: 1.9079885482788086\n",
      "Step 1342: Training loss: 4.206536293029785\n",
      "Step 1343: Training loss: 2.012812376022339\n",
      "Step 1344: Training loss: 2.3620307445526123\n",
      "Step 1345: Training loss: 2.4626619815826416\n",
      "Step 1346: Training loss: 4.484877586364746\n",
      "Step 1347: Training loss: 2.1322624683380127\n",
      "Step 1348: Training loss: 3.0816142559051514\n",
      "Step 1349: Training loss: 2.1047213077545166\n",
      "Step 1350: Training loss: 2.794715642929077\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1350\n",
      "Step 1351: Training loss: 2.4298806190490723\n",
      "Step 1352: Training loss: 3.0233020782470703\n",
      "Step 1353: Training loss: 3.541930913925171\n",
      "Step 1354: Training loss: 2.5456550121307373\n",
      "Step 1355: Training loss: 1.9919674396514893\n",
      "Step 1356: Training loss: 3.2416622638702393\n",
      "Step 1357: Training loss: 4.258354663848877\n",
      "Step 1358: Training loss: 2.721226930618286\n",
      "Step 1359: Training loss: 3.8112030029296875\n",
      "Step 1360: Training loss: 2.9440882205963135\n",
      "Step 1361: Training loss: 3.2737250328063965\n",
      "Step 1362: Training loss: 2.1208136081695557\n",
      "Step 1363: Training loss: 2.2254698276519775\n",
      "Step 1364: Training loss: 3.0243964195251465\n",
      "Step 1365: Training loss: 4.105169296264648\n",
      "Step 1366: Training loss: 3.7191107273101807\n",
      "Step 1367: Training loss: 2.0305542945861816\n",
      "Step 1368: Training loss: 2.024352550506592\n",
      "Step 1369: Training loss: 3.669632911682129\n",
      "Step 1370: Training loss: 4.5291266441345215\n",
      "Step 1371: Training loss: 3.3256173133850098\n",
      "Step 1372: Training loss: 2.7626798152923584\n",
      "Step 1373: Training loss: 2.295886754989624\n",
      "Step 1374: Training loss: 3.748812437057495\n",
      "Step 1375: Training loss: 3.3254361152648926\n",
      "Step 1376: Training loss: 3.121558666229248\n",
      "Step 1377: Training loss: 1.9489514827728271\n",
      "Step 1378: Training loss: 2.5848500728607178\n",
      "Step 1379: Training loss: 4.465725898742676\n",
      "Step 1380: Training loss: 4.588808059692383\n",
      "Step 1381: Training loss: 3.0890259742736816\n",
      "Step 1382: Training loss: 2.7809488773345947\n",
      "Step 1383: Training loss: 4.46763277053833\n",
      "Step 1384: Training loss: 1.8990871906280518\n",
      "Step 1385: Training loss: 2.871628999710083\n",
      "Step 1386: Training loss: 2.4904346466064453\n",
      "Step 1387: Training loss: 2.0159542560577393\n",
      "Step 1388: Training loss: 3.2994256019592285\n",
      "Step 1389: Training loss: 3.585477828979492\n",
      "Step 1390: Training loss: 1.9995733499526978\n",
      "Step 1391: Training loss: 2.9443185329437256\n",
      "Step 1392: Training loss: 3.030501365661621\n",
      "Step 1393: Training loss: 1.928652286529541\n",
      "Step 1394: Training loss: 2.486785650253296\n",
      "Step 1395: Training loss: 4.667092800140381\n",
      "Step 1396: Training loss: 2.8909478187561035\n",
      "Step 1397: Training loss: 4.236701488494873\n",
      "Step 1398: Training loss: 4.8528876304626465\n",
      "Step 1399: Training loss: 1.9332793951034546\n",
      "Step 1400: Training loss: 2.3066787719726562\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1400\n",
      "Step 1401: Training loss: 2.920876979827881\n",
      "Step 1402: Training loss: 3.192301034927368\n",
      "Step 1403: Training loss: 2.074897289276123\n",
      "Step 1404: Training loss: 3.0046446323394775\n",
      "Step 1405: Training loss: 2.7666821479797363\n",
      "Step 1406: Training loss: 3.3184995651245117\n",
      "Step 1407: Training loss: 1.9021607637405396\n",
      "Step 1408: Training loss: 3.5132205486297607\n",
      "Step 1409: Training loss: 2.9194724559783936\n",
      "Step 1410: Training loss: 2.006584405899048\n",
      "Step 1411: Training loss: 3.73293399810791\n",
      "Step 1412: Training loss: 2.170319080352783\n",
      "Step 1413: Training loss: 2.4834747314453125\n",
      "Step 1414: Training loss: 2.4067718982696533\n",
      "Step 1415: Training loss: 2.475320816040039\n",
      "Step 1416: Training loss: 2.2023582458496094\n",
      "Step 1417: Training loss: 4.743035793304443\n",
      "Step 1418: Training loss: 2.755558490753174\n",
      "Step 1419: Training loss: 3.30128812789917\n",
      "Step 1420: Training loss: 2.7826647758483887\n",
      "Step 1421: Training loss: 2.150385618209839\n",
      "Step 1422: Training loss: 2.387390375137329\n",
      "Step 1423: Training loss: 2.0444796085357666\n",
      "Step 1424: Training loss: 3.6184990406036377\n",
      "Step 1425: Training loss: 4.061924457550049\n",
      "Step 1426: Training loss: 2.254122018814087\n",
      "Step 1427: Training loss: 3.754960060119629\n",
      "Step 1428: Training loss: 3.134730339050293\n",
      "Step 1429: Training loss: 2.0403215885162354\n",
      "Step 1430: Training loss: 2.955427646636963\n",
      "Step 1431: Training loss: 1.8855501413345337\n",
      "Step 1432: Training loss: 2.9898481369018555\n",
      "Step 1433: Training loss: 1.9887561798095703\n",
      "Step 1434: Training loss: 4.356564044952393\n",
      "Step 1435: Training loss: 3.5040605068206787\n",
      "Step 1436: Training loss: 2.6646523475646973\n",
      "Step 1437: Training loss: 3.4634294509887695\n",
      "Step 1438: Training loss: 3.0588936805725098\n",
      "Step 1439: Training loss: 3.7216320037841797\n",
      "Step 1440: Training loss: 2.5471978187561035\n",
      "Step 1441: Training loss: 3.187939167022705\n",
      "Step 1442: Training loss: 3.194899559020996\n",
      "Step 1443: Training loss: 2.7588438987731934\n",
      "Step 1444: Training loss: 2.7881205081939697\n",
      "Step 1445: Training loss: 1.8671536445617676\n",
      "Step 1446: Training loss: 2.0910873413085938\n",
      "Step 1447: Training loss: 2.117920398712158\n",
      "Step 1448: Training loss: 2.9480183124542236\n",
      "Step 1449: Training loss: 3.518097162246704\n",
      "Step 1450: Training loss: 3.0614287853240967\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1450\n",
      "Step 1451: Training loss: 2.4511139392852783\n",
      "Step 1452: Training loss: 4.24088191986084\n",
      "Step 1453: Training loss: 1.973182201385498\n",
      "Step 1454: Training loss: 1.8115429878234863\n",
      "Step 1455: Training loss: 3.2170801162719727\n",
      "Step 1456: Training loss: 2.320995330810547\n",
      "Step 1457: Training loss: 4.837401866912842\n",
      "Step 1458: Training loss: 3.284367561340332\n",
      "Step 1459: Training loss: 4.508058547973633\n",
      "Step 1460: Training loss: 2.0975899696350098\n",
      "Step 1461: Training loss: 2.686082363128662\n",
      "Step 1462: Training loss: 3.143235921859741\n",
      "Step 1463: Training loss: 3.12457537651062\n",
      "Step 1464: Training loss: 1.8564344644546509\n",
      "Step 1465: Training loss: 4.009525299072266\n",
      "Step 1466: Training loss: 3.326131582260132\n",
      "Step 1467: Training loss: 3.7872562408447266\n",
      "Step 1468: Training loss: 5.572680950164795\n",
      "Step 1469: Training loss: 4.067773818969727\n",
      "Step 1470: Training loss: 2.700417995452881\n",
      "Step 1471: Training loss: 2.3102874755859375\n",
      "Step 1472: Training loss: 1.9434449672698975\n",
      "Step 1473: Training loss: 2.8883254528045654\n",
      "Step 1474: Training loss: 3.9262425899505615\n",
      "Step 1475: Training loss: 1.9134560823440552\n",
      "Step 1476: Training loss: 3.066431760787964\n",
      "Step 1477: Training loss: 1.9679317474365234\n",
      "Step 1478: Training loss: 2.0643978118896484\n",
      "Step 1479: Training loss: 3.390000820159912\n",
      "Step 1480: Training loss: 3.5271189212799072\n",
      "Step 1481: Training loss: 2.897926092147827\n",
      "Step 1482: Training loss: 2.0401806831359863\n",
      "Step 1483: Training loss: 2.265077829360962\n",
      "Step 1484: Training loss: 2.311859130859375\n",
      "Step 1485: Training loss: 2.0451934337615967\n",
      "Step 1486: Training loss: 2.762397050857544\n",
      "Step 1487: Training loss: 3.6417934894561768\n",
      "Step 1488: Training loss: 2.5472261905670166\n",
      "Step 1489: Training loss: 4.163735866546631\n",
      "Step 1490: Training loss: 2.4418201446533203\n",
      "Step 1491: Training loss: 3.165553092956543\n",
      "Step 1492: Training loss: 1.9208145141601562\n",
      "Step 1493: Training loss: 1.8681999444961548\n",
      "Step 1494: Training loss: 3.122199535369873\n",
      "Step 1495: Training loss: 4.120953559875488\n",
      "Step 1496: Training loss: 2.7181787490844727\n",
      "Step 1497: Training loss: 2.0693423748016357\n",
      "Step 1498: Training loss: 3.947740316390991\n",
      "Step 1499: Training loss: 3.1018431186676025\n",
      "Step 1500: Training loss: 2.054274559020996\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1500\n",
      "Step 1501: Training loss: 3.2121176719665527\n",
      "Step 1502: Training loss: 2.9345953464508057\n",
      "Step 1503: Training loss: 4.085621356964111\n",
      "Step 1504: Training loss: 2.3523082733154297\n",
      "Step 1505: Training loss: 3.1427485942840576\n",
      "Step 1506: Training loss: 3.371427059173584\n",
      "Step 1507: Training loss: 2.3411455154418945\n",
      "Step 1508: Training loss: 3.979708433151245\n",
      "Step 1509: Training loss: 2.1439671516418457\n",
      "Step 1510: Training loss: 3.04296875\n",
      "Step 1511: Training loss: 1.928868293762207\n",
      "Step 1512: Training loss: 2.021650552749634\n",
      "Step 1513: Training loss: 3.2800838947296143\n",
      "Step 1514: Training loss: 3.080519199371338\n",
      "Step 1515: Training loss: 4.242720127105713\n",
      "Step 1516: Training loss: 3.941500663757324\n",
      "Step 1517: Training loss: 3.1918387413024902\n",
      "Step 1518: Training loss: 2.0499627590179443\n",
      "Step 1519: Training loss: 2.096737861633301\n",
      "Step 1520: Training loss: 2.131258964538574\n",
      "Step 1521: Training loss: 3.0751607418060303\n",
      "Step 1522: Training loss: 1.7852039337158203\n",
      "Step 1523: Training loss: 1.9044355154037476\n",
      "Step 1524: Training loss: 3.9375650882720947\n",
      "Step 1525: Training loss: 3.847734212875366\n",
      "Step 1526: Training loss: 3.7731807231903076\n",
      "Step 1527: Training loss: 2.775054454803467\n",
      "Step 1528: Training loss: 2.3795745372772217\n",
      "Step 1529: Training loss: 4.952812194824219\n",
      "Step 1530: Training loss: 3.1760542392730713\n",
      "Step 1531: Training loss: 3.2336959838867188\n",
      "Step 1532: Training loss: 3.8458306789398193\n",
      "Step 1533: Training loss: 2.7649288177490234\n",
      "Step 1534: Training loss: 1.8904194831848145\n",
      "Step 1535: Training loss: 2.9921555519104004\n",
      "Step 1536: Training loss: 1.9959012269973755\n",
      "Step 1537: Training loss: 1.9284189939498901\n",
      "Step 1538: Training loss: 2.4032647609710693\n",
      "Step 1539: Training loss: 3.4308881759643555\n",
      "Step 1540: Training loss: 2.693561553955078\n",
      "Step 1541: Training loss: 4.189655780792236\n",
      "Step 1542: Training loss: 2.860557794570923\n",
      "Step 1543: Training loss: 3.572944164276123\n",
      "Step 1544: Training loss: 3.6751811504364014\n",
      "Step 1545: Training loss: 3.128053903579712\n",
      "Step 1546: Training loss: 1.810835361480713\n",
      "Step 1547: Training loss: 2.873746395111084\n",
      "Step 1548: Training loss: 3.7139804363250732\n",
      "Step 1549: Training loss: 2.502152919769287\n",
      "Step 1550: Training loss: 3.453303813934326\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1550\n",
      "Step 1551: Training loss: 1.9634944200515747\n",
      "Step 1552: Training loss: 3.086998462677002\n",
      "Step 1553: Training loss: 2.4276609420776367\n",
      "Step 1554: Training loss: 2.7184455394744873\n",
      "Step 1555: Training loss: 2.911853075027466\n",
      "Step 1556: Training loss: 4.12440299987793\n",
      "Step 1557: Training loss: 2.8562448024749756\n",
      "Step 1558: Training loss: 2.530499219894409\n",
      "Step 1559: Training loss: 1.7163159847259521\n",
      "Step 1560: Training loss: 3.7789201736450195\n",
      "Step 1561: Training loss: 2.175618886947632\n",
      "Step 1562: Training loss: 1.8924793004989624\n",
      "Step 1563: Training loss: 1.7368866205215454\n",
      "Step 1564: Training loss: 1.9468733072280884\n",
      "Step 1565: Training loss: 3.8172965049743652\n",
      "Step 1566: Training loss: 3.233057737350464\n",
      "Step 1567: Training loss: 3.5505144596099854\n",
      "Step 1568: Training loss: 3.3579368591308594\n",
      "Step 1569: Training loss: 3.2383298873901367\n",
      "Step 1570: Training loss: 3.841230869293213\n",
      "Step 1571: Training loss: 3.9035286903381348\n",
      "Step 1572: Training loss: 3.1021814346313477\n",
      "Step 1573: Training loss: 2.004542589187622\n",
      "Step 1574: Training loss: 2.8065216541290283\n",
      "Step 1575: Training loss: 2.7173004150390625\n",
      "Step 1576: Training loss: 4.04762077331543\n",
      "Step 1577: Training loss: 1.8409956693649292\n",
      "Step 1578: Training loss: 2.5592596530914307\n",
      "Step 1579: Training loss: 5.334335803985596\n",
      "Step 1580: Training loss: 3.123385429382324\n",
      "Step 1581: Training loss: 3.0518956184387207\n",
      "Step 1582: Training loss: 2.0693323612213135\n",
      "Step 1583: Training loss: 1.8722916841506958\n",
      "Step 1584: Training loss: 3.6690332889556885\n",
      "Step 1585: Training loss: 3.272453546524048\n",
      "Step 1586: Training loss: 2.2804737091064453\n",
      "Step 1587: Training loss: 2.9068636894226074\n",
      "Step 1588: Training loss: 4.095704078674316\n",
      "Step 1589: Training loss: 4.5544843673706055\n",
      "Step 1590: Training loss: 3.1825132369995117\n",
      "Step 1591: Training loss: 1.9869048595428467\n",
      "Step 1592: Training loss: 1.8747786283493042\n",
      "Step 1593: Training loss: 2.4306931495666504\n",
      "Step 1594: Training loss: 4.74603271484375\n",
      "Step 1595: Training loss: 3.8077356815338135\n",
      "Step 1596: Training loss: 1.8769047260284424\n",
      "Step 1597: Training loss: 3.0766489505767822\n",
      "Step 1598: Training loss: 2.606213092803955\n",
      "Step 1599: Training loss: 2.1646602153778076\n",
      "Step 1600: Training loss: 3.2697701454162598\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1600\n",
      "Step 1601: Training loss: 1.9280132055282593\n",
      "Step 1602: Training loss: 2.2607860565185547\n",
      "Step 1603: Training loss: 2.5355618000030518\n",
      "Step 1604: Training loss: 2.589193820953369\n",
      "Step 1605: Training loss: 3.1676666736602783\n",
      "Step 1606: Training loss: 5.11780309677124\n",
      "Step 1607: Training loss: 5.985034465789795\n",
      "Step 1608: Training loss: 1.9116426706314087\n",
      "Step 1609: Training loss: 2.376840353012085\n",
      "Step 1610: Training loss: 2.3833584785461426\n",
      "Step 1611: Training loss: 1.952745795249939\n",
      "Step 1612: Training loss: 2.753460168838501\n",
      "Step 1613: Training loss: 1.8337023258209229\n",
      "Step 1614: Training loss: 2.7169957160949707\n",
      "Step 1615: Training loss: 3.5708699226379395\n",
      "Step 1616: Training loss: 4.192473888397217\n",
      "Step 1617: Training loss: 3.000814914703369\n",
      "Step 1618: Training loss: 1.8545714616775513\n",
      "Step 1619: Training loss: 3.2275044918060303\n",
      "Step 1620: Training loss: 2.981844902038574\n",
      "Step 1621: Training loss: 2.5869057178497314\n",
      "Step 1622: Training loss: 2.963630437850952\n",
      "Step 1623: Training loss: 1.9772759675979614\n",
      "Step 1624: Training loss: 3.2862448692321777\n",
      "Step 1625: Training loss: 2.7479329109191895\n",
      "Step 1626: Training loss: 3.52126145362854\n",
      "Step 1627: Training loss: 2.9720938205718994\n",
      "Step 1628: Training loss: 1.9274059534072876\n",
      "Step 1629: Training loss: 4.119368553161621\n",
      "Step 1630: Training loss: 2.3623321056365967\n",
      "Step 1631: Training loss: 3.104429244995117\n",
      "Step 1632: Training loss: 2.6501500606536865\n",
      "Step 1633: Training loss: 1.9440892934799194\n",
      "Step 1634: Training loss: 2.3103528022766113\n",
      "Step 1635: Training loss: 2.534200668334961\n",
      "Step 1636: Training loss: 3.2934682369232178\n",
      "Step 1637: Training loss: 1.9157028198242188\n",
      "Step 1638: Training loss: 2.24927020072937\n",
      "Step 1639: Training loss: 2.3583269119262695\n",
      "Step 1640: Training loss: 2.7010533809661865\n",
      "Step 1641: Training loss: 3.5975420475006104\n",
      "Step 1642: Training loss: 2.346545457839966\n",
      "Step 1643: Training loss: 1.9471256732940674\n",
      "Step 1644: Training loss: 2.1099507808685303\n",
      "Step 1645: Training loss: 3.4975428581237793\n",
      "Step 1646: Training loss: 1.7520829439163208\n",
      "Step 1647: Training loss: 1.9598726034164429\n",
      "Step 1648: Training loss: 3.4897913932800293\n",
      "Step 1649: Training loss: 2.894176721572876\n",
      "Step 1650: Training loss: 1.855929970741272\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1650\n",
      "Step 1651: Training loss: 2.398336172103882\n",
      "Step 1652: Training loss: 3.4749224185943604\n",
      "Step 1653: Training loss: 4.01654052734375\n",
      "Step 1654: Training loss: 2.935039520263672\n",
      "Step 1655: Training loss: 4.48289680480957\n",
      "Step 1656: Training loss: 1.947913408279419\n",
      "Step 1657: Training loss: 1.7396844625473022\n",
      "Step 1658: Training loss: 3.427945852279663\n",
      "Step 1659: Training loss: 2.7863142490386963\n",
      "Step 1660: Training loss: 5.05673360824585\n",
      "Step 1661: Training loss: 1.8697441816329956\n",
      "Step 1662: Training loss: 2.9060986042022705\n",
      "Step 1663: Training loss: 2.7816085815429688\n",
      "Step 1664: Training loss: 1.7616193294525146\n",
      "Step 1665: Training loss: 2.6430504322052\n",
      "Step 1666: Training loss: 2.9404423236846924\n",
      "Step 1667: Training loss: 1.9084354639053345\n",
      "Step 1668: Training loss: 3.2037038803100586\n",
      "Step 1669: Training loss: 2.8370275497436523\n",
      "Step 1670: Training loss: 1.787765622138977\n",
      "Step 1671: Training loss: 3.2768733501434326\n",
      "Step 1672: Training loss: 2.0015969276428223\n",
      "Step 1673: Training loss: 1.8262176513671875\n",
      "Step 1674: Training loss: 1.8399368524551392\n",
      "Step 1675: Training loss: 3.131197929382324\n",
      "Step 1676: Training loss: 4.503299713134766\n",
      "Step 1677: Training loss: 2.5585219860076904\n",
      "Step 1678: Training loss: 2.567018747329712\n",
      "Step 1679: Training loss: 2.1062533855438232\n",
      "Step 1680: Training loss: 2.6960229873657227\n",
      "Step 1681: Training loss: 1.754417061805725\n",
      "Step 1682: Training loss: 4.162121295928955\n",
      "Step 1683: Training loss: 1.7175620794296265\n",
      "Step 1684: Training loss: 2.050713062286377\n",
      "Step 1685: Training loss: 1.950121283531189\n",
      "Step 1686: Training loss: 4.171514511108398\n",
      "Step 1687: Training loss: 3.3773958683013916\n",
      "Step 1688: Training loss: 3.083024740219116\n",
      "Step 1689: Training loss: 2.6285321712493896\n",
      "Step 1690: Training loss: 3.4904086589813232\n",
      "Step 1691: Training loss: 2.189267635345459\n",
      "Step 1692: Training loss: 3.7773115634918213\n",
      "Step 1693: Training loss: 3.3174350261688232\n",
      "Step 1694: Training loss: 3.262758255004883\n",
      "Step 1695: Training loss: 2.639289140701294\n",
      "Step 1696: Training loss: 3.9440062046051025\n",
      "Step 1697: Training loss: 2.8223791122436523\n",
      "Step 1698: Training loss: 2.854335308074951\n",
      "Step 1699: Training loss: 3.9462473392486572\n",
      "Step 1700: Training loss: 2.550900459289551\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1700\n",
      "Step 1701: Training loss: 1.7395082712173462\n",
      "Step 1702: Training loss: 3.850820779800415\n",
      "Step 1703: Training loss: 1.9252761602401733\n",
      "Step 1704: Training loss: 1.8722478151321411\n",
      "Step 1705: Training loss: 2.5395407676696777\n",
      "Step 1706: Training loss: 3.5131430625915527\n",
      "Step 1707: Training loss: 2.8480026721954346\n",
      "Step 1708: Training loss: 1.9263371229171753\n",
      "Step 1709: Training loss: 3.139204978942871\n",
      "Step 1710: Training loss: 2.929788827896118\n",
      "Step 1711: Training loss: 2.688936233520508\n",
      "Step 1712: Training loss: 2.7816197872161865\n",
      "Step 1713: Training loss: 2.6471476554870605\n",
      "Step 1714: Training loss: 3.3199455738067627\n",
      "Step 1715: Training loss: 3.42986798286438\n",
      "Step 1716: Training loss: 3.0060510635375977\n",
      "Step 1717: Training loss: 3.2758188247680664\n",
      "Step 1718: Training loss: 3.659980297088623\n",
      "Step 1719: Training loss: 2.765686511993408\n",
      "Step 1720: Training loss: 1.826918363571167\n",
      "Step 1721: Training loss: 2.613121747970581\n",
      "Step 1722: Training loss: 1.8316580057144165\n",
      "Step 1723: Training loss: 1.9679094552993774\n",
      "Step 1724: Training loss: 1.6852246522903442\n",
      "Step 1725: Training loss: 2.649531126022339\n",
      "Step 1726: Training loss: 3.2419180870056152\n",
      "Step 1727: Training loss: 2.2699756622314453\n",
      "Step 1728: Training loss: 1.885414958000183\n",
      "Step 1729: Training loss: 1.7945115566253662\n",
      "Step 1730: Training loss: 1.835581660270691\n",
      "Step 1731: Training loss: 1.7637015581130981\n",
      "Step 1732: Training loss: 2.760322093963623\n",
      "Step 1733: Training loss: 3.124375104904175\n",
      "Step 1734: Training loss: 1.9605568647384644\n",
      "Step 1735: Training loss: 1.8937015533447266\n",
      "Step 1736: Training loss: 1.70372474193573\n",
      "Step 1737: Training loss: 1.7098212242126465\n",
      "Step 1738: Training loss: 3.8853676319122314\n",
      "Step 1739: Training loss: 1.7840977907180786\n",
      "Step 1740: Training loss: 2.0250742435455322\n",
      "Step 1741: Training loss: 2.532457113265991\n",
      "Step 1742: Training loss: 2.9864182472229004\n",
      "Step 1743: Training loss: 2.723369836807251\n",
      "Step 1744: Training loss: 3.0284392833709717\n",
      "Step 1745: Training loss: 2.9123873710632324\n",
      "Step 1746: Training loss: 4.369109153747559\n",
      "Step 1747: Training loss: 3.0158755779266357\n",
      "Step 1748: Training loss: 2.372051477432251\n",
      "Step 1749: Training loss: 2.0941555500030518\n",
      "Step 1750: Training loss: 1.834943413734436\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1750\n",
      "Step 1751: Training loss: 2.666405200958252\n",
      "Step 1752: Training loss: 2.0700831413269043\n",
      "Step 1753: Training loss: 3.8825950622558594\n",
      "Step 1754: Training loss: 1.8539589643478394\n",
      "Step 1755: Training loss: 2.2432713508605957\n",
      "Step 1756: Training loss: 3.245596170425415\n",
      "Step 1757: Training loss: 2.380772590637207\n",
      "Step 1758: Training loss: 3.142028570175171\n",
      "Step 1759: Training loss: 2.6325912475585938\n",
      "Step 1760: Training loss: 3.151712417602539\n",
      "Step 1761: Training loss: 1.8972722291946411\n",
      "Step 1762: Training loss: 2.744382619857788\n",
      "Step 1763: Training loss: 3.870027542114258\n",
      "Step 1764: Training loss: 2.823519229888916\n",
      "Step 1765: Training loss: 2.8164565563201904\n",
      "Step 1766: Training loss: 1.6700292825698853\n",
      "Step 1767: Training loss: 1.7252960205078125\n",
      "Step 1768: Training loss: 1.9082870483398438\n",
      "Step 1769: Training loss: 2.896092414855957\n",
      "Step 1770: Training loss: 2.830888032913208\n",
      "Step 1771: Training loss: 2.1743791103363037\n",
      "Step 1772: Training loss: 2.8975272178649902\n",
      "Step 1773: Training loss: 2.9669487476348877\n",
      "Step 1774: Training loss: 2.8611035346984863\n",
      "Step 1775: Training loss: 2.6010305881500244\n",
      "Step 1776: Training loss: 2.7452778816223145\n",
      "Step 1777: Training loss: 4.040796756744385\n",
      "Step 1778: Training loss: 2.0805411338806152\n",
      "Step 1779: Training loss: 1.8324004411697388\n",
      "Step 1780: Training loss: 1.9569244384765625\n",
      "Step 1781: Training loss: 1.6763685941696167\n",
      "Step 1782: Training loss: 4.179940223693848\n",
      "Step 1783: Training loss: 2.890435218811035\n",
      "Step 1784: Training loss: 3.995560884475708\n",
      "Step 1785: Training loss: 2.3582088947296143\n",
      "Step 1786: Training loss: 3.369915008544922\n",
      "Step 1787: Training loss: 3.6771047115325928\n",
      "Step 1788: Training loss: 4.062448501586914\n",
      "Step 1789: Training loss: 2.0351476669311523\n",
      "Step 1790: Training loss: 2.980297327041626\n",
      "Step 1791: Training loss: 1.805332899093628\n",
      "Step 1792: Training loss: 3.1254310607910156\n",
      "Step 1793: Training loss: 1.9047523736953735\n",
      "Step 1794: Training loss: 2.731548309326172\n",
      "Step 1795: Training loss: 1.7766329050064087\n",
      "Step 1796: Training loss: 1.7027158737182617\n",
      "Step 1797: Training loss: 2.663233757019043\n",
      "Step 1798: Training loss: 2.137941598892212\n",
      "Step 1799: Training loss: 2.520404100418091\n",
      "Step 1800: Training loss: 2.4520680904388428\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1800\n",
      "Step 1801: Training loss: 4.496367454528809\n",
      "Step 1802: Training loss: 2.9394495487213135\n",
      "Step 1803: Training loss: 2.1200528144836426\n",
      "Step 1804: Training loss: 3.6817879676818848\n",
      "Step 1805: Training loss: 1.7075344324111938\n",
      "Step 1806: Training loss: 1.6539034843444824\n",
      "Step 1807: Training loss: 1.7316316366195679\n",
      "Step 1808: Training loss: 2.411614179611206\n",
      "Step 1809: Training loss: 2.5775935649871826\n",
      "Step 1810: Training loss: 2.9207370281219482\n",
      "Step 1811: Training loss: 3.5307209491729736\n",
      "Step 1812: Training loss: 3.9771127700805664\n",
      "Step 1813: Training loss: 1.9872052669525146\n",
      "Step 1814: Training loss: 1.662488341331482\n",
      "Step 1815: Training loss: 1.6301325559616089\n",
      "Step 1816: Training loss: 3.8166847229003906\n",
      "Step 1817: Training loss: 1.6990957260131836\n",
      "Step 1818: Training loss: 2.0949270725250244\n",
      "Step 1819: Training loss: 2.1371829509735107\n",
      "Step 1820: Training loss: 1.920235276222229\n",
      "Step 1821: Training loss: 2.5407662391662598\n",
      "Step 1822: Training loss: 2.9160373210906982\n",
      "Step 1823: Training loss: 2.5715739727020264\n",
      "Step 1824: Training loss: 2.149111032485962\n",
      "Step 1825: Training loss: 1.8378221988677979\n",
      "Step 1826: Training loss: 1.5691057443618774\n",
      "Step 1827: Training loss: 2.8849751949310303\n",
      "Step 1828: Training loss: 2.4951391220092773\n",
      "Step 1829: Training loss: 2.681520938873291\n",
      "Step 1830: Training loss: 3.6236801147460938\n",
      "Step 1831: Training loss: 2.8153810501098633\n",
      "Step 1832: Training loss: 2.269948959350586\n",
      "Step 1833: Training loss: 1.9143961668014526\n",
      "Step 1834: Training loss: 2.678306818008423\n",
      "Step 1835: Training loss: 1.9331631660461426\n",
      "Step 1836: Training loss: 2.616218328475952\n",
      "Step 1837: Training loss: 4.322568416595459\n",
      "Step 1838: Training loss: 1.7223893404006958\n",
      "Step 1839: Training loss: 2.3685007095336914\n",
      "Step 1840: Training loss: 2.7109220027923584\n",
      "Step 1841: Training loss: 2.8060672283172607\n",
      "Step 1842: Training loss: 2.973881959915161\n",
      "Step 1843: Training loss: 4.0061821937561035\n",
      "Step 1844: Training loss: 2.021449327468872\n",
      "Step 1845: Training loss: 2.3491146564483643\n",
      "Step 1846: Training loss: 2.1520073413848877\n",
      "Step 1847: Training loss: 1.8128443956375122\n",
      "Step 1848: Training loss: 2.3113107681274414\n",
      "Step 1849: Training loss: 2.640484571456909\n",
      "Step 1850: Training loss: 1.7293211221694946\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1850\n",
      "Step 1851: Training loss: 1.7219507694244385\n",
      "Step 1852: Training loss: 2.8013761043548584\n",
      "Step 1853: Training loss: 1.771811604499817\n",
      "Step 1854: Training loss: 2.750866174697876\n",
      "Step 1855: Training loss: 3.242401361465454\n",
      "Step 1856: Training loss: 2.446169137954712\n",
      "Step 1857: Training loss: 2.0146114826202393\n",
      "Step 1858: Training loss: 2.3124499320983887\n",
      "Step 1859: Training loss: 1.7046056985855103\n",
      "Step 1860: Training loss: 2.5019381046295166\n",
      "Step 1861: Training loss: 2.635150671005249\n",
      "Step 1862: Training loss: 1.9332656860351562\n",
      "Step 1863: Training loss: 1.702751874923706\n",
      "Step 1864: Training loss: 1.7125531435012817\n",
      "Step 1865: Training loss: 2.422631025314331\n",
      "Step 1866: Training loss: 1.701262354850769\n",
      "Step 1867: Training loss: 2.6807808876037598\n",
      "Step 1868: Training loss: 3.0007832050323486\n",
      "Step 1869: Training loss: 2.7549889087677\n",
      "Step 1870: Training loss: 3.0208396911621094\n",
      "Step 1871: Training loss: 3.1974000930786133\n",
      "Step 1872: Training loss: 2.4474539756774902\n",
      "Step 1873: Training loss: 2.462506055831909\n",
      "Step 1874: Training loss: 1.644914150238037\n",
      "Step 1875: Training loss: 2.84669828414917\n",
      "Step 1876: Training loss: 1.597822666168213\n",
      "Step 1877: Training loss: 2.7704057693481445\n",
      "Step 1878: Training loss: 2.2734925746917725\n",
      "Step 1879: Training loss: 2.1231119632720947\n",
      "Step 1880: Training loss: 1.675779938697815\n",
      "Step 1881: Training loss: 2.590280771255493\n",
      "Step 1882: Training loss: 3.045663356781006\n",
      "Step 1883: Training loss: 2.969586133956909\n",
      "Step 1884: Training loss: 2.5532727241516113\n",
      "Step 1885: Training loss: 1.9268896579742432\n",
      "Step 1886: Training loss: 4.195674896240234\n",
      "Step 1887: Training loss: 1.7620705366134644\n",
      "Step 1888: Training loss: 2.5365850925445557\n",
      "Step 1889: Training loss: 1.7608462572097778\n",
      "Step 1890: Training loss: 2.6769607067108154\n",
      "Step 1891: Training loss: 2.6877763271331787\n",
      "Step 1892: Training loss: 2.665402412414551\n",
      "Step 1893: Training loss: 3.896040439605713\n",
      "Step 1894: Training loss: 2.309183359146118\n",
      "Step 1895: Training loss: 1.6536885499954224\n",
      "Step 1896: Training loss: 2.352182626724243\n",
      "Step 1897: Training loss: 3.3803045749664307\n",
      "Step 1898: Training loss: 1.883972406387329\n",
      "Step 1899: Training loss: 2.578444242477417\n",
      "Step 1900: Training loss: 1.782002568244934\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1900\n",
      "Step 1901: Training loss: 2.3354883193969727\n",
      "Step 1902: Training loss: 2.266350507736206\n",
      "Step 1903: Training loss: 3.601062536239624\n",
      "Step 1904: Training loss: 3.1607985496520996\n",
      "Step 1905: Training loss: 2.6214027404785156\n",
      "Step 1906: Training loss: 2.497492790222168\n",
      "Step 1907: Training loss: 1.7178324460983276\n",
      "Step 1908: Training loss: 2.522603750228882\n",
      "Step 1909: Training loss: 2.7682878971099854\n",
      "Step 1910: Training loss: 4.640194416046143\n",
      "Step 1911: Training loss: 2.4873359203338623\n",
      "Step 1912: Training loss: 1.8236687183380127\n",
      "Step 1913: Training loss: 2.623279094696045\n",
      "Step 1914: Training loss: 1.740635871887207\n",
      "Step 1915: Training loss: 3.26499605178833\n",
      "Step 1916: Training loss: 2.967594623565674\n",
      "Step 1917: Training loss: 1.5432071685791016\n",
      "Step 1918: Training loss: 2.452617645263672\n",
      "Step 1919: Training loss: 1.99761164188385\n",
      "Step 1920: Training loss: 1.716315746307373\n",
      "Step 1921: Training loss: 2.072681427001953\n",
      "Step 1922: Training loss: 3.780426263809204\n",
      "Step 1923: Training loss: 2.7624666690826416\n",
      "Step 1924: Training loss: 2.749983310699463\n",
      "Step 1925: Training loss: 3.154623508453369\n",
      "Step 1926: Training loss: 2.7375338077545166\n",
      "Step 1927: Training loss: 1.864001750946045\n",
      "Step 1928: Training loss: 2.320859670639038\n",
      "Step 1929: Training loss: 1.7376779317855835\n",
      "Step 1930: Training loss: 4.282384872436523\n",
      "Step 1931: Training loss: 2.2560834884643555\n",
      "Step 1932: Training loss: 2.0985019207000732\n",
      "Step 1933: Training loss: 2.988407850265503\n",
      "Step 1934: Training loss: 2.698584794998169\n",
      "Step 1935: Training loss: 2.9819772243499756\n",
      "Step 1936: Training loss: 1.6395331621170044\n",
      "Step 1937: Training loss: 2.002953290939331\n",
      "Step 1938: Training loss: 2.6756691932678223\n",
      "Step 1939: Training loss: 1.7030595541000366\n",
      "Step 1940: Training loss: 3.6729466915130615\n",
      "Step 1941: Training loss: 1.6593636274337769\n",
      "Step 1942: Training loss: 2.631387710571289\n",
      "Step 1943: Training loss: 2.209324598312378\n",
      "Step 1944: Training loss: 2.5610833168029785\n",
      "Step 1945: Training loss: 2.939256191253662\n",
      "Step 1946: Training loss: 1.6852761507034302\n",
      "Step 1947: Training loss: 1.784019112586975\n",
      "Step 1948: Training loss: 1.8431158065795898\n",
      "Step 1949: Training loss: 3.2976291179656982\n",
      "Step 1950: Training loss: 3.847438335418701\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-1950\n",
      "Step 1951: Training loss: 1.9994466304779053\n",
      "Step 1952: Training loss: 4.162726879119873\n",
      "Step 1953: Training loss: 2.0124130249023438\n",
      "Step 1954: Training loss: 2.7808239459991455\n",
      "Step 1955: Training loss: 2.686243772506714\n",
      "Step 1956: Training loss: 2.0384445190429688\n",
      "Step 1957: Training loss: 1.7056927680969238\n",
      "Step 1958: Training loss: 1.6060301065444946\n",
      "Step 1959: Training loss: 3.5288782119750977\n",
      "Step 1960: Training loss: 1.5156141519546509\n",
      "Step 1961: Training loss: 2.5811941623687744\n",
      "Step 1962: Training loss: 1.6731133460998535\n",
      "Step 1963: Training loss: 1.5858526229858398\n",
      "Step 1964: Training loss: 1.6271615028381348\n",
      "Step 1965: Training loss: 2.7239456176757812\n",
      "Step 1966: Training loss: 4.078985214233398\n",
      "Step 1967: Training loss: 1.931649088859558\n",
      "Step 1968: Training loss: 2.9432168006896973\n",
      "Step 1969: Training loss: 2.099788188934326\n",
      "Step 1970: Training loss: 3.3144149780273438\n",
      "Step 1971: Training loss: 1.7668510675430298\n",
      "Step 1972: Training loss: 2.2121024131774902\n",
      "Step 1973: Training loss: 2.5162317752838135\n",
      "Step 1974: Training loss: 2.815699577331543\n",
      "Step 1975: Training loss: 2.833965539932251\n",
      "Step 1976: Training loss: 2.696255683898926\n",
      "Step 1977: Training loss: 2.523160219192505\n",
      "Step 1978: Training loss: 2.7464020252227783\n",
      "Step 1979: Training loss: 2.3355085849761963\n",
      "Step 1980: Training loss: 2.360459327697754\n",
      "Step 1981: Training loss: 3.68400239944458\n",
      "Step 1982: Training loss: 2.4356346130371094\n",
      "Step 1983: Training loss: 1.5979083776474\n",
      "Step 1984: Training loss: 2.5120012760162354\n",
      "Step 1985: Training loss: 2.521186113357544\n",
      "Step 1986: Training loss: 2.7588729858398438\n",
      "Step 1987: Training loss: 1.842296838760376\n",
      "Step 1988: Training loss: 2.5604019165039062\n",
      "Step 1989: Training loss: 2.6387417316436768\n",
      "Step 1990: Training loss: 1.7251923084259033\n",
      "Step 1991: Training loss: 3.0370941162109375\n",
      "Step 1992: Training loss: 1.7385607957839966\n",
      "Step 1993: Training loss: 1.7248804569244385\n",
      "Step 1994: Training loss: 2.7171998023986816\n",
      "Step 1995: Training loss: 1.637367606163025\n",
      "Step 1996: Training loss: 2.909489393234253\n",
      "Step 1997: Training loss: 1.4832804203033447\n",
      "Step 1998: Training loss: 1.539986252784729\n",
      "Step 1999: Training loss: 4.609356880187988\n",
      "Step 2000: Training loss: 2.748091697692871\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2000\n",
      "Step 2001: Training loss: 2.710165500640869\n",
      "Step 2002: Training loss: 1.5349444150924683\n",
      "Step 2003: Training loss: 1.7948540449142456\n",
      "Step 2004: Training loss: 2.7924225330352783\n",
      "Step 2005: Training loss: 1.7696137428283691\n",
      "Step 2006: Training loss: 2.660825252532959\n",
      "Step 2007: Training loss: 2.702303409576416\n",
      "Step 2008: Training loss: 3.1588101387023926\n",
      "Step 2009: Training loss: 2.713852882385254\n",
      "Step 2010: Training loss: 2.7020726203918457\n",
      "Step 2011: Training loss: 1.7934155464172363\n",
      "Step 2012: Training loss: 2.2623202800750732\n",
      "Step 2013: Training loss: 2.110518217086792\n",
      "Step 2014: Training loss: 1.6962902545928955\n",
      "Step 2015: Training loss: 2.6442627906799316\n",
      "Step 2016: Training loss: 3.212273597717285\n",
      "Step 2017: Training loss: 1.9140205383300781\n",
      "Step 2018: Training loss: 2.7141172885894775\n",
      "Step 2019: Training loss: 2.203352689743042\n",
      "Step 2020: Training loss: 2.370352029800415\n",
      "Step 2021: Training loss: 2.8416173458099365\n",
      "Step 2022: Training loss: 2.1970582008361816\n",
      "Step 2023: Training loss: 3.16192364692688\n",
      "Step 2024: Training loss: 4.054488182067871\n",
      "Step 2025: Training loss: 3.2908406257629395\n",
      "Step 2026: Training loss: 2.1959891319274902\n",
      "Step 2027: Training loss: 2.792356491088867\n",
      "Step 2028: Training loss: 1.9751874208450317\n",
      "Step 2029: Training loss: 3.614964008331299\n",
      "Step 2030: Training loss: 1.8254841566085815\n",
      "Step 2031: Training loss: 1.8904622793197632\n",
      "Step 2032: Training loss: 2.6576998233795166\n",
      "Step 2033: Training loss: 2.671851634979248\n",
      "Step 2034: Training loss: 1.7970068454742432\n",
      "Step 2035: Training loss: 2.300261974334717\n",
      "Step 2036: Training loss: 2.6194770336151123\n",
      "Step 2037: Training loss: 1.625014305114746\n",
      "Step 2038: Training loss: 2.0269155502319336\n",
      "Step 2039: Training loss: 2.198660135269165\n",
      "Step 2040: Training loss: 1.9799833297729492\n",
      "Step 2041: Training loss: 3.2156600952148438\n",
      "Step 2042: Training loss: 3.622131824493408\n",
      "Step 2043: Training loss: 3.607942581176758\n",
      "Step 2044: Training loss: 3.2768611907958984\n",
      "Step 2045: Training loss: 2.593076705932617\n",
      "Step 2046: Training loss: 2.3564276695251465\n",
      "Step 2047: Training loss: 1.6628471612930298\n",
      "Step 2048: Training loss: 2.52729868888855\n",
      "Step 2049: Training loss: 2.5657317638397217\n",
      "Step 2050: Training loss: 3.599170684814453\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2050\n",
      "Step 2051: Training loss: 1.6204167604446411\n",
      "Step 2052: Training loss: 2.6911985874176025\n",
      "Step 2053: Training loss: 1.8376952409744263\n",
      "Step 2054: Training loss: 1.6630809307098389\n",
      "Step 2055: Training loss: 2.962175130844116\n",
      "Step 2056: Training loss: 2.7720322608947754\n",
      "Step 2057: Training loss: 1.6611886024475098\n",
      "Step 2058: Training loss: 1.7246068716049194\n",
      "Step 2059: Training loss: 2.4104654788970947\n",
      "Step 2060: Training loss: 2.536104202270508\n",
      "Step 2061: Training loss: 2.0587124824523926\n",
      "Step 2062: Training loss: 2.5322890281677246\n",
      "Step 2063: Training loss: 2.3302292823791504\n",
      "Step 2064: Training loss: 3.4775753021240234\n",
      "Step 2065: Training loss: 2.4651739597320557\n",
      "Step 2066: Training loss: 2.4048657417297363\n",
      "Step 2067: Training loss: 2.566741943359375\n",
      "Step 2068: Training loss: 2.043220281600952\n",
      "Step 2069: Training loss: 1.7097634077072144\n",
      "Step 2070: Training loss: 1.765090823173523\n",
      "Step 2071: Training loss: 2.226755142211914\n",
      "Step 2072: Training loss: 2.767557382583618\n",
      "Step 2073: Training loss: 2.8543546199798584\n",
      "Step 2074: Training loss: 2.0334975719451904\n",
      "Step 2075: Training loss: 1.6159029006958008\n",
      "Step 2076: Training loss: 2.3009183406829834\n",
      "Step 2077: Training loss: 2.247580051422119\n",
      "Step 2078: Training loss: 4.113113880157471\n",
      "Step 2079: Training loss: 1.8666315078735352\n",
      "Step 2080: Training loss: 3.408040761947632\n",
      "Step 2081: Training loss: 2.440883159637451\n",
      "Step 2082: Training loss: 1.7584282159805298\n",
      "Step 2083: Training loss: 2.8437695503234863\n",
      "Step 2084: Training loss: 1.597576379776001\n",
      "Step 2085: Training loss: 1.599716067314148\n",
      "Step 2086: Training loss: 2.5307183265686035\n",
      "Step 2087: Training loss: 4.139318466186523\n",
      "Step 2088: Training loss: 3.4165287017822266\n",
      "Step 2089: Training loss: 3.792996883392334\n",
      "Step 2090: Training loss: 1.8269299268722534\n",
      "Step 2091: Training loss: 2.610703468322754\n",
      "Step 2092: Training loss: 1.6968375444412231\n",
      "Step 2093: Training loss: 1.8803043365478516\n",
      "Step 2094: Training loss: 1.6904270648956299\n",
      "Step 2095: Training loss: 2.453718662261963\n",
      "Step 2096: Training loss: 2.2132320404052734\n",
      "Step 2097: Training loss: 1.7202820777893066\n",
      "Step 2098: Training loss: 1.646785855293274\n",
      "Step 2099: Training loss: 2.663811683654785\n",
      "Step 2100: Training loss: 2.4767911434173584\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2100\n",
      "Step 2101: Training loss: 1.6411997079849243\n",
      "Step 2102: Training loss: 2.2803776264190674\n",
      "Step 2103: Training loss: 1.4932547807693481\n",
      "Step 2104: Training loss: 2.571629047393799\n",
      "Step 2105: Training loss: 2.5133047103881836\n",
      "Step 2106: Training loss: 2.3094680309295654\n",
      "Step 2107: Training loss: 2.589728355407715\n",
      "Step 2108: Training loss: 3.3010060787200928\n",
      "Step 2109: Training loss: 2.0863046646118164\n",
      "Step 2110: Training loss: 1.585647463798523\n",
      "Step 2111: Training loss: 3.797755718231201\n",
      "Step 2112: Training loss: 1.6319655179977417\n",
      "Step 2113: Training loss: 2.2559854984283447\n",
      "Step 2114: Training loss: 2.5607047080993652\n",
      "Step 2115: Training loss: 1.6759421825408936\n",
      "Step 2116: Training loss: 1.5940474271774292\n",
      "Step 2117: Training loss: 1.621302843093872\n",
      "Step 2118: Training loss: 3.7125158309936523\n",
      "Step 2119: Training loss: 2.3890295028686523\n",
      "Step 2120: Training loss: 3.4294376373291016\n",
      "Step 2121: Training loss: 2.7747480869293213\n",
      "Step 2122: Training loss: 1.6600841283798218\n",
      "Step 2123: Training loss: 3.6283769607543945\n",
      "Step 2124: Training loss: 3.666043519973755\n",
      "Step 2125: Training loss: 2.643768787384033\n",
      "Step 2126: Training loss: 1.5934115648269653\n",
      "Step 2127: Training loss: 2.3286337852478027\n",
      "Step 2128: Training loss: 1.7518874406814575\n",
      "Step 2129: Training loss: 2.16003680229187\n",
      "Step 2130: Training loss: 2.4602205753326416\n",
      "Step 2131: Training loss: 1.4936350584030151\n",
      "Step 2132: Training loss: 2.988736391067505\n",
      "Step 2133: Training loss: 2.0203616619110107\n",
      "Step 2134: Training loss: 2.1104416847229004\n",
      "Step 2135: Training loss: 1.700659155845642\n",
      "Step 2136: Training loss: 2.306180238723755\n",
      "Step 2137: Training loss: 2.506371021270752\n",
      "Step 2138: Training loss: 2.4687628746032715\n",
      "Step 2139: Training loss: 3.8830246925354004\n",
      "Step 2140: Training loss: 1.8546885251998901\n",
      "Step 2141: Training loss: 2.3900818824768066\n",
      "Step 2142: Training loss: 3.5462565422058105\n",
      "Step 2143: Training loss: 1.5752480030059814\n",
      "Step 2144: Training loss: 1.625903606414795\n",
      "Step 2145: Training loss: 2.356004238128662\n",
      "Step 2146: Training loss: 2.8139002323150635\n",
      "Step 2147: Training loss: 3.1421377658843994\n",
      "Step 2148: Training loss: 1.9412921667099\n",
      "Step 2149: Training loss: 1.9513921737670898\n",
      "Step 2150: Training loss: 1.576371669769287\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2150\n",
      "Step 2151: Training loss: 2.0661139488220215\n",
      "Step 2152: Training loss: 3.129248857498169\n",
      "Step 2153: Training loss: 2.865074396133423\n",
      "Step 2154: Training loss: 2.4756176471710205\n",
      "Step 2155: Training loss: 3.1070730686187744\n",
      "Step 2156: Training loss: 2.5700039863586426\n",
      "Step 2157: Training loss: 2.2982168197631836\n",
      "Step 2158: Training loss: 3.075300693511963\n",
      "Step 2159: Training loss: 3.058692216873169\n",
      "Step 2160: Training loss: 1.7681660652160645\n",
      "Step 2161: Training loss: 2.9218027591705322\n",
      "Step 2162: Training loss: 2.5875608921051025\n",
      "Step 2163: Training loss: 1.760147213935852\n",
      "Step 2164: Training loss: 1.6214731931686401\n",
      "Step 2165: Training loss: 2.3880550861358643\n",
      "Step 2166: Training loss: 1.944899082183838\n",
      "Step 2167: Training loss: 2.6182844638824463\n",
      "Step 2168: Training loss: 1.6304856538772583\n",
      "Step 2169: Training loss: 3.288336992263794\n",
      "Step 2170: Training loss: 2.0439655780792236\n",
      "Step 2171: Training loss: 1.636061429977417\n",
      "Step 2172: Training loss: 1.8939837217330933\n",
      "Step 2173: Training loss: 3.1609344482421875\n",
      "Step 2174: Training loss: 2.374932050704956\n",
      "Step 2175: Training loss: 1.7912263870239258\n",
      "Step 2176: Training loss: 2.7350239753723145\n",
      "Step 2177: Training loss: 1.5860861539840698\n",
      "Step 2178: Training loss: 3.7206480503082275\n",
      "Step 2179: Training loss: 2.1007604598999023\n",
      "Step 2180: Training loss: 2.0580570697784424\n",
      "Step 2181: Training loss: 2.7365305423736572\n",
      "Step 2182: Training loss: 2.5182044506073\n",
      "Step 2183: Training loss: 2.181802988052368\n",
      "Step 2184: Training loss: 2.6471986770629883\n",
      "Step 2185: Training loss: 1.4782605171203613\n",
      "Step 2186: Training loss: 1.9272000789642334\n",
      "Step 2187: Training loss: 2.583611011505127\n",
      "Step 2188: Training loss: 1.6244523525238037\n",
      "Step 2189: Training loss: 3.3709263801574707\n",
      "Step 2190: Training loss: 1.5152195692062378\n",
      "Step 2191: Training loss: 1.7410191297531128\n",
      "Step 2192: Training loss: 2.251767873764038\n",
      "Step 2193: Training loss: 1.5610270500183105\n",
      "Step 2194: Training loss: 1.7870028018951416\n",
      "Step 2195: Training loss: 1.768810749053955\n",
      "Step 2196: Training loss: 3.3157503604888916\n",
      "Step 2197: Training loss: 1.7216218709945679\n",
      "Step 2198: Training loss: 1.9835916757583618\n",
      "Step 2199: Training loss: 2.191131353378296\n",
      "Step 2200: Training loss: 2.22627592086792\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2200\n",
      "Step 2201: Training loss: 2.296987533569336\n",
      "Step 2202: Training loss: 2.5338921546936035\n",
      "Step 2203: Training loss: 2.4765465259552\n",
      "Step 2204: Training loss: 2.426111936569214\n",
      "Step 2205: Training loss: 1.8429439067840576\n",
      "Step 2206: Training loss: 3.456408977508545\n",
      "Step 2207: Training loss: 1.602776050567627\n",
      "Step 2208: Training loss: 2.806546211242676\n",
      "Step 2209: Training loss: 2.111008882522583\n",
      "Step 2210: Training loss: 2.1323437690734863\n",
      "Step 2211: Training loss: 1.6473397016525269\n",
      "Step 2212: Training loss: 2.509768486022949\n",
      "Step 2213: Training loss: 2.1109702587127686\n",
      "Step 2214: Training loss: 2.2636935710906982\n",
      "Step 2215: Training loss: 2.501375198364258\n",
      "Step 2216: Training loss: 2.4557580947875977\n",
      "Step 2217: Training loss: 2.53393292427063\n",
      "Step 2218: Training loss: 3.2452847957611084\n",
      "Step 2219: Training loss: 2.9406895637512207\n",
      "Step 2220: Training loss: 1.4827429056167603\n",
      "Step 2221: Training loss: 1.5829524993896484\n",
      "Step 2222: Training loss: 1.5718544721603394\n",
      "Step 2223: Training loss: 2.443861246109009\n",
      "Step 2224: Training loss: 1.8805463314056396\n",
      "Step 2225: Training loss: 2.464428186416626\n",
      "Step 2226: Training loss: 1.9889559745788574\n",
      "Step 2227: Training loss: 2.4462473392486572\n",
      "Step 2228: Training loss: 3.3808703422546387\n",
      "Step 2229: Training loss: 2.7794365882873535\n",
      "Step 2230: Training loss: 2.5752131938934326\n",
      "Step 2231: Training loss: 1.5457229614257812\n",
      "Step 2232: Training loss: 1.746176838874817\n",
      "Step 2233: Training loss: 1.5509588718414307\n",
      "Step 2234: Training loss: 2.323824644088745\n",
      "Step 2235: Training loss: 2.6077895164489746\n",
      "Step 2236: Training loss: 1.45550537109375\n",
      "Step 2237: Training loss: 4.029478073120117\n",
      "Step 2238: Training loss: 2.6502153873443604\n",
      "Step 2239: Training loss: 3.269559621810913\n",
      "Step 2240: Training loss: 2.5166637897491455\n",
      "Step 2241: Training loss: 2.5325491428375244\n",
      "Step 2242: Training loss: 2.222625970840454\n",
      "Step 2243: Training loss: 4.111860275268555\n",
      "Step 2244: Training loss: 2.6537058353424072\n",
      "Step 2245: Training loss: 1.667493224143982\n",
      "Step 2246: Training loss: 3.585456371307373\n",
      "Step 2247: Training loss: 1.9670625925064087\n",
      "Step 2248: Training loss: 2.571617603302002\n",
      "Step 2249: Training loss: 2.3960649967193604\n",
      "Step 2250: Training loss: 1.4652725458145142\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2250\n",
      "Step 2251: Training loss: 2.1706430912017822\n",
      "Step 2252: Training loss: 3.6674060821533203\n",
      "Step 2253: Training loss: 2.6202216148376465\n",
      "Step 2254: Training loss: 1.7177691459655762\n",
      "Step 2255: Training loss: 1.928215503692627\n",
      "Step 2256: Training loss: 1.8259958028793335\n",
      "Step 2257: Training loss: 1.9334068298339844\n",
      "Step 2258: Training loss: 2.17891526222229\n",
      "Step 2259: Training loss: 2.342379093170166\n",
      "Step 2260: Training loss: 3.237643003463745\n",
      "Step 2261: Training loss: 2.6011064052581787\n",
      "Step 2262: Training loss: 2.855994939804077\n",
      "Step 2263: Training loss: 2.403567314147949\n",
      "Step 2264: Training loss: 2.775669574737549\n",
      "Step 2265: Training loss: 1.8883140087127686\n",
      "Step 2266: Training loss: 2.418462038040161\n",
      "Step 2267: Training loss: 2.5797228813171387\n",
      "Step 2268: Training loss: 2.7440967559814453\n",
      "Step 2269: Training loss: 2.0774195194244385\n",
      "Step 2270: Training loss: 1.5116575956344604\n",
      "Step 2271: Training loss: 2.5450503826141357\n",
      "Step 2272: Training loss: 1.729447603225708\n",
      "Step 2273: Training loss: 1.4832546710968018\n",
      "Step 2274: Training loss: 1.5855698585510254\n",
      "Step 2275: Training loss: 1.653390884399414\n",
      "Step 2276: Training loss: 3.3087620735168457\n",
      "Step 2277: Training loss: 2.6883585453033447\n",
      "Step 2278: Training loss: 2.8859312534332275\n",
      "Step 2279: Training loss: 2.2337467670440674\n",
      "Step 2280: Training loss: 2.7500264644622803\n",
      "Step 2281: Training loss: 3.3544161319732666\n",
      "Step 2282: Training loss: 2.0665223598480225\n",
      "Step 2283: Training loss: 1.5688447952270508\n",
      "Step 2284: Training loss: 2.6383156776428223\n",
      "Step 2285: Training loss: 1.5469720363616943\n",
      "Step 2286: Training loss: 2.059490203857422\n",
      "Step 2287: Training loss: 2.093919277191162\n",
      "Step 2288: Training loss: 3.9503438472747803\n",
      "Step 2289: Training loss: 2.518380641937256\n",
      "Step 2290: Training loss: 2.3767638206481934\n",
      "Step 2291: Training loss: 1.523919939994812\n",
      "Step 2292: Training loss: 2.2744624614715576\n",
      "Step 2293: Training loss: 2.850517511367798\n",
      "Step 2294: Training loss: 2.4088761806488037\n",
      "Step 2295: Training loss: 2.2999725341796875\n",
      "Step 2296: Training loss: 2.3807332515716553\n",
      "Step 2297: Training loss: 2.0485334396362305\n",
      "Step 2298: Training loss: 2.3504364490509033\n",
      "Step 2299: Training loss: 1.6702065467834473\n",
      "Step 2300: Training loss: 1.6854323148727417\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2300\n",
      "Step 2301: Training loss: 1.6650009155273438\n",
      "Step 2302: Training loss: 1.9879837036132812\n",
      "Step 2303: Training loss: 2.124349594116211\n",
      "Step 2304: Training loss: 2.114091157913208\n",
      "Step 2305: Training loss: 1.7939671277999878\n",
      "Step 2306: Training loss: 2.942206382751465\n",
      "Step 2307: Training loss: 3.2111265659332275\n",
      "Step 2308: Training loss: 1.510263204574585\n",
      "Step 2309: Training loss: 1.5855437517166138\n",
      "Step 2310: Training loss: 2.840182065963745\n",
      "Step 2311: Training loss: 1.4990609884262085\n",
      "Step 2312: Training loss: 2.736847162246704\n",
      "Step 2313: Training loss: 2.528956890106201\n",
      "Step 2314: Training loss: 2.1743147373199463\n",
      "Step 2315: Training loss: 1.5624639987945557\n",
      "Step 2316: Training loss: 2.310781478881836\n",
      "Step 2317: Training loss: 1.5782748460769653\n",
      "Step 2318: Training loss: 2.2024660110473633\n",
      "Step 2319: Training loss: 1.4956072568893433\n",
      "Step 2320: Training loss: 1.6630491018295288\n",
      "Step 2321: Training loss: 1.4843897819519043\n",
      "Step 2322: Training loss: 4.291751861572266\n",
      "Step 2323: Training loss: 2.3931775093078613\n",
      "Step 2324: Training loss: 2.0841190814971924\n",
      "Step 2325: Training loss: 2.1022231578826904\n",
      "Step 2326: Training loss: 1.5077948570251465\n",
      "Step 2327: Training loss: 2.897895574569702\n",
      "Step 2328: Training loss: 1.8579840660095215\n",
      "Step 2329: Training loss: 1.7936960458755493\n",
      "Step 2330: Training loss: 1.6939088106155396\n",
      "Step 2331: Training loss: 2.8447272777557373\n",
      "Step 2332: Training loss: 3.4203622341156006\n",
      "Step 2333: Training loss: 2.4188411235809326\n",
      "Step 2334: Training loss: 2.3389525413513184\n",
      "Step 2335: Training loss: 2.1056759357452393\n",
      "Step 2336: Training loss: 1.5202833414077759\n",
      "Step 2337: Training loss: 3.8687729835510254\n",
      "Step 2338: Training loss: 2.573875904083252\n",
      "Step 2339: Training loss: 3.040163040161133\n",
      "Step 2340: Training loss: 2.3922131061553955\n",
      "Step 2341: Training loss: 1.9825048446655273\n",
      "Step 2342: Training loss: 1.6493053436279297\n",
      "Step 2343: Training loss: 1.4640134572982788\n",
      "Step 2344: Training loss: 1.6911404132843018\n",
      "Step 2345: Training loss: 1.6226392984390259\n",
      "Step 2346: Training loss: 2.7235395908355713\n",
      "Step 2347: Training loss: 3.0759944915771484\n",
      "Step 2348: Training loss: 2.478562831878662\n",
      "Step 2349: Training loss: 2.239976167678833\n",
      "Step 2350: Training loss: 2.858790159225464\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2350\n",
      "Step 2351: Training loss: 1.606741189956665\n",
      "Step 2352: Training loss: 1.452894926071167\n",
      "Step 2353: Training loss: 1.451605200767517\n",
      "Step 2354: Training loss: 1.5649311542510986\n",
      "Step 2355: Training loss: 1.9590259790420532\n",
      "Step 2356: Training loss: 1.7234729528427124\n",
      "Step 2357: Training loss: 2.633384943008423\n",
      "Step 2358: Training loss: 2.4950990676879883\n",
      "Step 2359: Training loss: 2.579828977584839\n",
      "Step 2360: Training loss: 3.2034943103790283\n",
      "Step 2361: Training loss: 2.3194315433502197\n",
      "Step 2362: Training loss: 2.481114625930786\n",
      "Step 2363: Training loss: 1.7243441343307495\n",
      "Step 2364: Training loss: 1.5881352424621582\n",
      "Step 2365: Training loss: 2.701265573501587\n",
      "Step 2366: Training loss: 2.945838689804077\n",
      "Step 2367: Training loss: 1.4717187881469727\n",
      "Step 2368: Training loss: 2.4298295974731445\n",
      "Step 2369: Training loss: 1.587327003479004\n",
      "Step 2370: Training loss: 1.4923574924468994\n",
      "Step 2371: Training loss: 1.5777398347854614\n",
      "Step 2372: Training loss: 2.476975679397583\n",
      "Step 2373: Training loss: 1.9356952905654907\n",
      "Step 2374: Training loss: 2.1737401485443115\n",
      "Step 2375: Training loss: 1.5311990976333618\n",
      "Step 2376: Training loss: 1.8263654708862305\n",
      "Step 2377: Training loss: 1.3938554525375366\n",
      "Step 2378: Training loss: 2.5452277660369873\n",
      "Step 2379: Training loss: 2.206761121749878\n",
      "Step 2380: Training loss: 2.4024250507354736\n",
      "Step 2381: Training loss: 2.249171495437622\n",
      "Step 2382: Training loss: 1.4346779584884644\n",
      "Step 2383: Training loss: 3.178493022918701\n",
      "Step 2384: Training loss: 2.4646310806274414\n",
      "Step 2385: Training loss: 1.3777920007705688\n",
      "Step 2386: Training loss: 2.503986358642578\n",
      "Step 2387: Training loss: 2.2925028800964355\n",
      "Step 2388: Training loss: 3.051987886428833\n",
      "Step 2389: Training loss: 1.4852451086044312\n",
      "Step 2390: Training loss: 2.7474260330200195\n",
      "Step 2391: Training loss: 2.6260268688201904\n",
      "Step 2392: Training loss: 2.1844074726104736\n",
      "Step 2393: Training loss: 3.151156425476074\n",
      "Step 2394: Training loss: 2.452072858810425\n",
      "Step 2395: Training loss: 1.7552287578582764\n",
      "Step 2396: Training loss: 2.4641263484954834\n",
      "Step 2397: Training loss: 1.7941734790802002\n",
      "Step 2398: Training loss: 2.4544546604156494\n",
      "Step 2399: Training loss: 1.6723058223724365\n",
      "Step 2400: Training loss: 2.2813591957092285\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2400\n",
      "Step 2401: Training loss: 2.336921215057373\n",
      "Step 2402: Training loss: 2.3173413276672363\n",
      "Step 2403: Training loss: 1.9983900785446167\n",
      "Step 2404: Training loss: 2.2790513038635254\n",
      "Step 2405: Training loss: 1.6073969602584839\n",
      "Step 2406: Training loss: 2.361809015274048\n",
      "Step 2407: Training loss: 2.107529878616333\n",
      "Step 2408: Training loss: 2.350348949432373\n",
      "Step 2409: Training loss: 2.1548147201538086\n",
      "Step 2410: Training loss: 1.8055567741394043\n",
      "Step 2411: Training loss: 2.3105809688568115\n",
      "Step 2412: Training loss: 1.9670854806900024\n",
      "Step 2413: Training loss: 1.6740442514419556\n",
      "Step 2414: Training loss: 2.5336625576019287\n",
      "Step 2415: Training loss: 1.3866257667541504\n",
      "Step 2416: Training loss: 2.4564754962921143\n",
      "Step 2417: Training loss: 2.0010740756988525\n",
      "Step 2418: Training loss: 2.4703221321105957\n",
      "Step 2419: Training loss: 2.320446491241455\n",
      "Step 2420: Training loss: 2.4226608276367188\n",
      "Step 2421: Training loss: 2.2833268642425537\n",
      "Step 2422: Training loss: 1.9689568281173706\n",
      "Step 2423: Training loss: 1.509587287902832\n",
      "Step 2424: Training loss: 1.5494873523712158\n",
      "Step 2425: Training loss: 1.4615403413772583\n",
      "Step 2426: Training loss: 1.967670202255249\n",
      "Step 2427: Training loss: 2.1615750789642334\n",
      "Step 2428: Training loss: 2.2019317150115967\n",
      "Step 2429: Training loss: 2.4459521770477295\n",
      "Step 2430: Training loss: 1.8321305513381958\n",
      "Step 2431: Training loss: 1.4547476768493652\n",
      "Step 2432: Training loss: 2.218977212905884\n",
      "Step 2433: Training loss: 1.3402119874954224\n",
      "Step 2434: Training loss: 1.4772868156433105\n",
      "Step 2435: Training loss: 2.7679944038391113\n",
      "Step 2436: Training loss: 2.996091842651367\n",
      "Step 2437: Training loss: 2.3208978176116943\n",
      "Step 2438: Training loss: 1.3870148658752441\n",
      "Step 2439: Training loss: 1.4003241062164307\n",
      "Step 2440: Training loss: 1.5678032636642456\n",
      "Step 2441: Training loss: 2.2746963500976562\n",
      "Step 2442: Training loss: 1.3049864768981934\n",
      "Step 2443: Training loss: 2.3433446884155273\n",
      "Step 2444: Training loss: 1.8704917430877686\n",
      "Step 2445: Training loss: 2.28741455078125\n",
      "Step 2446: Training loss: 1.383444905281067\n",
      "Step 2447: Training loss: 3.1333134174346924\n",
      "Step 2448: Training loss: 1.4812722206115723\n",
      "Step 2449: Training loss: 1.5057355165481567\n",
      "Step 2450: Training loss: 1.7309190034866333\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2450\n",
      "Step 2451: Training loss: 2.6660380363464355\n",
      "Step 2452: Training loss: 1.6819406747817993\n",
      "Step 2453: Training loss: 1.7291321754455566\n",
      "Step 2454: Training loss: 1.4932096004486084\n",
      "Step 2455: Training loss: 2.272639751434326\n",
      "Step 2456: Training loss: 3.9986801147460938\n",
      "Step 2457: Training loss: 1.5431451797485352\n",
      "Step 2458: Training loss: 1.7098712921142578\n",
      "Step 2459: Training loss: 1.3676141500473022\n",
      "Step 2460: Training loss: 1.4981670379638672\n",
      "Step 2461: Training loss: 1.382660150527954\n",
      "Step 2462: Training loss: 1.7428282499313354\n",
      "Step 2463: Training loss: 1.5487927198410034\n",
      "Step 2464: Training loss: 2.2437920570373535\n",
      "Step 2465: Training loss: 1.5056757926940918\n",
      "Step 2466: Training loss: 1.5247663259506226\n",
      "Step 2467: Training loss: 2.066505193710327\n",
      "Step 2468: Training loss: 1.9454597234725952\n",
      "Step 2469: Training loss: 1.4348233938217163\n",
      "Step 2470: Training loss: 2.910341262817383\n",
      "Step 2471: Training loss: 2.57792067527771\n",
      "Step 2472: Training loss: 1.6418581008911133\n",
      "Step 2473: Training loss: 1.6106089353561401\n",
      "Step 2474: Training loss: 2.389918804168701\n",
      "Step 2475: Training loss: 3.1834611892700195\n",
      "Step 2476: Training loss: 2.1285743713378906\n",
      "Step 2477: Training loss: 3.1099720001220703\n",
      "Step 2478: Training loss: 2.044374942779541\n",
      "Step 2479: Training loss: 3.2632997035980225\n",
      "Step 2480: Training loss: 1.339756727218628\n",
      "Step 2481: Training loss: 2.7838075160980225\n",
      "Step 2482: Training loss: 2.166002035140991\n",
      "Step 2483: Training loss: 1.6207020282745361\n",
      "Step 2484: Training loss: 2.721390724182129\n",
      "Step 2485: Training loss: 2.1221425533294678\n",
      "Step 2486: Training loss: 2.0385732650756836\n",
      "Step 2487: Training loss: 1.978149652481079\n",
      "Step 2488: Training loss: 2.323406457901001\n",
      "Step 2489: Training loss: 1.9757046699523926\n",
      "Step 2490: Training loss: 1.6813150644302368\n",
      "Step 2491: Training loss: 1.4866011142730713\n",
      "Step 2492: Training loss: 2.0653061866760254\n",
      "Step 2493: Training loss: 1.8156027793884277\n",
      "Step 2494: Training loss: 1.4846770763397217\n",
      "Step 2495: Training loss: 2.5941379070281982\n",
      "Step 2496: Training loss: 2.270124673843384\n",
      "Step 2497: Training loss: 2.5634613037109375\n",
      "Step 2498: Training loss: 1.597453236579895\n",
      "Step 2499: Training loss: 2.199207067489624\n",
      "Step 2500: Training loss: 1.3262033462524414\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2500\n",
      "Step 2501: Training loss: 3.192474842071533\n",
      "Step 2502: Training loss: 1.6157058477401733\n",
      "Step 2503: Training loss: 3.280459403991699\n",
      "Step 2504: Training loss: 2.2115566730499268\n",
      "Step 2505: Training loss: 1.4425277709960938\n",
      "Step 2506: Training loss: 2.424595594406128\n",
      "Step 2507: Training loss: 1.340495228767395\n",
      "Step 2508: Training loss: 1.7265108823776245\n",
      "Step 2509: Training loss: 1.5484471321105957\n",
      "Step 2510: Training loss: 2.2975411415100098\n",
      "Step 2511: Training loss: 2.475429058074951\n",
      "Step 2512: Training loss: 2.10394549369812\n",
      "Step 2513: Training loss: 1.9417468309402466\n",
      "Step 2514: Training loss: 2.2591347694396973\n",
      "Step 2515: Training loss: 2.176504373550415\n",
      "Step 2516: Training loss: 2.197591781616211\n",
      "Step 2517: Training loss: 3.7285666465759277\n",
      "Step 2518: Training loss: 2.0807130336761475\n",
      "Step 2519: Training loss: 1.3616433143615723\n",
      "Step 2520: Training loss: 1.3699989318847656\n",
      "Step 2521: Training loss: 1.3958855867385864\n",
      "Step 2522: Training loss: 1.5759449005126953\n",
      "Step 2523: Training loss: 1.3088594675064087\n",
      "Step 2524: Training loss: 1.4480608701705933\n",
      "Step 2525: Training loss: 2.692152738571167\n",
      "Step 2526: Training loss: 3.0561468601226807\n",
      "Step 2527: Training loss: 4.025394439697266\n",
      "Step 2528: Training loss: 1.9112273454666138\n",
      "Step 2529: Training loss: 1.479091763496399\n",
      "Step 2530: Training loss: 1.8818531036376953\n",
      "Step 2531: Training loss: 1.9919358491897583\n",
      "Step 2532: Training loss: 2.1211838722229004\n",
      "Step 2533: Training loss: 2.2235910892486572\n",
      "Step 2534: Training loss: 1.9700218439102173\n",
      "Step 2535: Training loss: 1.9402199983596802\n",
      "Step 2536: Training loss: 1.4319241046905518\n",
      "Step 2537: Training loss: 1.401219129562378\n",
      "Step 2538: Training loss: 1.3518515825271606\n",
      "Step 2539: Training loss: 1.642541766166687\n",
      "Step 2540: Training loss: 2.2778265476226807\n",
      "Step 2541: Training loss: 1.5313401222229004\n",
      "Step 2542: Training loss: 1.97344172000885\n",
      "Step 2543: Training loss: 2.7768218517303467\n",
      "Step 2544: Training loss: 1.9153611660003662\n",
      "Step 2545: Training loss: 1.4276561737060547\n",
      "Step 2546: Training loss: 2.3657097816467285\n",
      "Step 2547: Training loss: 2.21443772315979\n",
      "Step 2548: Training loss: 1.7388041019439697\n",
      "Step 2549: Training loss: 2.6501941680908203\n",
      "Step 2550: Training loss: 2.100247383117676\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2550\n",
      "Step 2551: Training loss: 1.8255140781402588\n",
      "Step 2552: Training loss: 2.2136595249176025\n",
      "Step 2553: Training loss: 2.3542041778564453\n",
      "Step 2554: Training loss: 1.3745372295379639\n",
      "Step 2555: Training loss: 1.4374581575393677\n",
      "Step 2556: Training loss: 4.349673748016357\n",
      "Step 2557: Training loss: 1.5727119445800781\n",
      "Step 2558: Training loss: 1.4763518571853638\n",
      "Step 2559: Training loss: 2.1126976013183594\n",
      "Step 2560: Training loss: 2.1627256870269775\n",
      "Step 2561: Training loss: 2.217440366744995\n",
      "Step 2562: Training loss: 1.6976730823516846\n",
      "Step 2563: Training loss: 1.524005651473999\n",
      "Step 2564: Training loss: 2.2506802082061768\n",
      "Step 2565: Training loss: 2.672527313232422\n",
      "Step 2566: Training loss: 3.5951106548309326\n",
      "Step 2567: Training loss: 2.795084238052368\n",
      "Step 2568: Training loss: 1.41355562210083\n",
      "Step 2569: Training loss: 2.100836753845215\n",
      "Step 2570: Training loss: 1.316353678703308\n",
      "Step 2571: Training loss: 2.427212715148926\n",
      "Step 2572: Training loss: 2.164292097091675\n",
      "Step 2573: Training loss: 2.078852415084839\n",
      "Step 2574: Training loss: 1.4887914657592773\n",
      "Step 2575: Training loss: 3.0632758140563965\n",
      "Step 2576: Training loss: 2.1442339420318604\n",
      "Step 2577: Training loss: 2.883662462234497\n",
      "Step 2578: Training loss: 2.0116891860961914\n",
      "Step 2579: Training loss: 1.4307414293289185\n",
      "Step 2580: Training loss: 1.407782793045044\n",
      "Step 2581: Training loss: 1.640592098236084\n",
      "Step 2582: Training loss: 2.19111704826355\n",
      "Step 2583: Training loss: 2.912222146987915\n",
      "Step 2584: Training loss: 1.845245599746704\n",
      "Step 2585: Training loss: 2.6568408012390137\n",
      "Step 2586: Training loss: 1.8118256330490112\n",
      "Step 2587: Training loss: 2.880563259124756\n",
      "Step 2588: Training loss: 1.6444810628890991\n",
      "Step 2589: Training loss: 1.4978474378585815\n",
      "Step 2590: Training loss: 2.7435739040374756\n",
      "Step 2591: Training loss: 1.4862812757492065\n",
      "Step 2592: Training loss: 2.210886240005493\n",
      "Step 2593: Training loss: 1.8827344179153442\n",
      "Step 2594: Training loss: 2.851931095123291\n",
      "Step 2595: Training loss: 1.41292142868042\n",
      "Step 2596: Training loss: 2.4317166805267334\n",
      "Step 2597: Training loss: 2.357999801635742\n",
      "Step 2598: Training loss: 1.9973818063735962\n",
      "Step 2599: Training loss: 2.6228439807891846\n",
      "Step 2600: Training loss: 1.871327519416809\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2600\n",
      "Step 2601: Training loss: 2.2579596042633057\n",
      "Step 2602: Training loss: 1.968154788017273\n",
      "Step 2603: Training loss: 1.5891072750091553\n",
      "Step 2604: Training loss: 2.04933762550354\n",
      "Step 2605: Training loss: 1.5947262048721313\n",
      "Step 2606: Training loss: 1.851820707321167\n",
      "Step 2607: Training loss: 2.17629337310791\n",
      "Step 2608: Training loss: 1.9495489597320557\n",
      "Step 2609: Training loss: 1.857293725013733\n",
      "Step 2610: Training loss: 2.036766529083252\n",
      "Step 2611: Training loss: 3.1529884338378906\n",
      "Step 2612: Training loss: 2.937954902648926\n",
      "Step 2613: Training loss: 1.484594464302063\n",
      "Step 2614: Training loss: 1.9934780597686768\n",
      "Step 2615: Training loss: 3.0071957111358643\n",
      "Step 2616: Training loss: 1.4450929164886475\n",
      "Step 2617: Training loss: 1.4233832359313965\n",
      "Step 2618: Training loss: 1.5522992610931396\n",
      "Step 2619: Training loss: 1.9851183891296387\n",
      "Step 2620: Training loss: 1.382614254951477\n",
      "Step 2621: Training loss: 1.3446805477142334\n",
      "Step 2622: Training loss: 1.4495967626571655\n",
      "Step 2623: Training loss: 3.451874256134033\n",
      "Step 2624: Training loss: 1.434786081314087\n",
      "Step 2625: Training loss: 2.1520028114318848\n",
      "Step 2626: Training loss: 1.9628560543060303\n",
      "Step 2627: Training loss: 1.3751386404037476\n",
      "Step 2628: Training loss: 1.5670579671859741\n",
      "Step 2629: Training loss: 1.3095310926437378\n",
      "Step 2630: Training loss: 1.4925117492675781\n",
      "Step 2631: Training loss: 2.171656608581543\n",
      "Step 2632: Training loss: 1.3066294193267822\n",
      "Step 2633: Training loss: 2.481348991394043\n",
      "Step 2634: Training loss: 2.2907004356384277\n",
      "Step 2635: Training loss: 2.2663938999176025\n",
      "Step 2636: Training loss: 1.7814810276031494\n",
      "Step 2637: Training loss: 2.4668447971343994\n",
      "Step 2638: Training loss: 2.432074546813965\n",
      "Step 2639: Training loss: 2.817021608352661\n",
      "Step 2640: Training loss: 3.403092861175537\n",
      "Step 2641: Training loss: 2.96359920501709\n",
      "Step 2642: Training loss: 3.59801983833313\n",
      "Step 2643: Training loss: 1.3218308687210083\n",
      "Step 2644: Training loss: 1.4914129972457886\n",
      "Step 2645: Training loss: 1.3266352415084839\n",
      "Step 2646: Training loss: 1.9122977256774902\n",
      "Step 2647: Training loss: 1.593739628791809\n",
      "Step 2648: Training loss: 2.3863182067871094\n",
      "Step 2649: Training loss: 2.102555990219116\n",
      "Step 2650: Training loss: 1.9913402795791626\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2650\n",
      "Step 2651: Training loss: 3.18510365486145\n",
      "Step 2652: Training loss: 2.0673604011535645\n",
      "Step 2653: Training loss: 3.243551731109619\n",
      "Step 2654: Training loss: 2.584479331970215\n",
      "Step 2655: Training loss: 2.5348970890045166\n",
      "Step 2656: Training loss: 1.3354036808013916\n",
      "Step 2657: Training loss: 2.3262674808502197\n",
      "Step 2658: Training loss: 1.9230707883834839\n",
      "Step 2659: Training loss: 2.7662718296051025\n",
      "Step 2660: Training loss: 2.0853021144866943\n",
      "Step 2661: Training loss: 1.445803165435791\n",
      "Step 2662: Training loss: 1.3211859464645386\n",
      "Step 2663: Training loss: 1.3247754573822021\n",
      "Step 2664: Training loss: 1.4675289392471313\n",
      "Step 2665: Training loss: 1.5749045610427856\n",
      "Step 2666: Training loss: 1.4348186254501343\n",
      "Step 2667: Training loss: 1.473972201347351\n",
      "Step 2668: Training loss: 2.344902992248535\n",
      "Step 2669: Training loss: 1.8953269720077515\n",
      "Step 2670: Training loss: 1.2743819952011108\n",
      "Step 2671: Training loss: 2.105731964111328\n",
      "Step 2672: Training loss: 2.0697031021118164\n",
      "Step 2673: Training loss: 1.3452430963516235\n",
      "Step 2674: Training loss: 1.5776928663253784\n",
      "Step 2675: Training loss: 1.2050795555114746\n",
      "Step 2676: Training loss: 2.1533780097961426\n",
      "Step 2677: Training loss: 2.0186235904693604\n",
      "Step 2678: Training loss: 1.558194875717163\n",
      "Step 2679: Training loss: 2.0472755432128906\n",
      "Step 2680: Training loss: 1.2364567518234253\n",
      "Step 2681: Training loss: 2.131206512451172\n",
      "Step 2682: Training loss: 2.1917104721069336\n",
      "Step 2683: Training loss: 2.709230661392212\n",
      "Step 2684: Training loss: 1.817430019378662\n",
      "Step 2685: Training loss: 2.2102975845336914\n",
      "Step 2686: Training loss: 1.8183048963546753\n",
      "Step 2687: Training loss: 2.4661307334899902\n",
      "Step 2688: Training loss: 1.3145101070404053\n",
      "Step 2689: Training loss: 1.8101674318313599\n",
      "Step 2690: Training loss: 2.611557722091675\n",
      "Step 2691: Training loss: 1.414384126663208\n",
      "Step 2692: Training loss: 1.3796191215515137\n",
      "Step 2693: Training loss: 2.6995580196380615\n",
      "Step 2694: Training loss: 1.8416459560394287\n",
      "Step 2695: Training loss: 1.5906004905700684\n",
      "Step 2696: Training loss: 2.0855071544647217\n",
      "Step 2697: Training loss: 1.289772868156433\n",
      "Step 2698: Training loss: 2.067963123321533\n",
      "Step 2699: Training loss: 1.7004308700561523\n",
      "Step 2700: Training loss: 2.753729820251465\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2700\n",
      "Step 2701: Training loss: 1.40825617313385\n",
      "Step 2702: Training loss: 2.7018892765045166\n",
      "Step 2703: Training loss: 2.3065106868743896\n",
      "Step 2704: Training loss: 1.803734302520752\n",
      "Step 2705: Training loss: 1.3389390707015991\n",
      "Step 2706: Training loss: 2.5995378494262695\n",
      "Step 2707: Training loss: 1.3659963607788086\n",
      "Step 2708: Training loss: 3.2518632411956787\n",
      "Step 2709: Training loss: 2.4372332096099854\n",
      "Step 2710: Training loss: 1.1884820461273193\n",
      "Step 2711: Training loss: 1.368404507637024\n",
      "Step 2712: Training loss: 1.4617400169372559\n",
      "Step 2713: Training loss: 1.438718318939209\n",
      "Step 2714: Training loss: 2.7371292114257812\n",
      "Step 2715: Training loss: 1.6835274696350098\n",
      "Step 2716: Training loss: 1.38787043094635\n",
      "Step 2717: Training loss: 1.3050981760025024\n",
      "Step 2718: Training loss: 3.276280641555786\n",
      "Step 2719: Training loss: 1.915482521057129\n",
      "Step 2720: Training loss: 1.6677477359771729\n",
      "Step 2721: Training loss: 1.3383452892303467\n",
      "Step 2722: Training loss: 1.9212214946746826\n",
      "Step 2723: Training loss: 2.737229108810425\n",
      "Step 2724: Training loss: 1.2441154718399048\n",
      "Step 2725: Training loss: 2.224022150039673\n",
      "Step 2726: Training loss: 2.0816898345947266\n",
      "Step 2727: Training loss: 1.8673220872879028\n",
      "Step 2728: Training loss: 2.463395357131958\n",
      "Step 2729: Training loss: 2.009298086166382\n",
      "Step 2730: Training loss: 1.3843830823898315\n",
      "Step 2731: Training loss: 1.9978299140930176\n",
      "Step 2732: Training loss: 2.6974542140960693\n",
      "Step 2733: Training loss: 2.282418727874756\n",
      "Step 2734: Training loss: 2.159823417663574\n",
      "Step 2735: Training loss: 1.415594458580017\n",
      "Step 2736: Training loss: 1.7054816484451294\n",
      "Step 2737: Training loss: 3.0798637866973877\n",
      "Step 2738: Training loss: 1.4586111307144165\n",
      "Step 2739: Training loss: 1.452488660812378\n",
      "Step 2740: Training loss: 2.4507124423980713\n",
      "Step 2741: Training loss: 3.309429407119751\n",
      "Step 2742: Training loss: 1.9740595817565918\n",
      "Step 2743: Training loss: 2.516824245452881\n",
      "Step 2744: Training loss: 2.102736473083496\n",
      "Step 2745: Training loss: 1.9804505109786987\n",
      "Step 2746: Training loss: 1.8653409481048584\n",
      "Step 2747: Training loss: 1.8661140203475952\n",
      "Step 2748: Training loss: 1.241702675819397\n",
      "Step 2749: Training loss: 1.4213945865631104\n",
      "Step 2750: Training loss: 1.3630248308181763\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2750\n",
      "Step 2751: Training loss: 1.719385027885437\n",
      "Step 2752: Training loss: 1.8938497304916382\n",
      "Step 2753: Training loss: 2.1052401065826416\n",
      "Step 2754: Training loss: 1.8181421756744385\n",
      "Step 2755: Training loss: 2.1864938735961914\n",
      "Step 2756: Training loss: 1.933923363685608\n",
      "Step 2757: Training loss: 1.8316903114318848\n",
      "Step 2758: Training loss: 1.3606839179992676\n",
      "Step 2759: Training loss: 1.3378976583480835\n",
      "Step 2760: Training loss: 1.826005220413208\n",
      "Step 2761: Training loss: 2.220242500305176\n",
      "Step 2762: Training loss: 1.2451881170272827\n",
      "Step 2763: Training loss: 2.176283359527588\n",
      "Step 2764: Training loss: 1.482218861579895\n",
      "Step 2765: Training loss: 3.0169026851654053\n",
      "Step 2766: Training loss: 2.005044937133789\n",
      "Step 2767: Training loss: 1.6539734601974487\n",
      "Step 2768: Training loss: 2.5223233699798584\n",
      "Step 2769: Training loss: 2.0874969959259033\n",
      "Step 2770: Training loss: 1.802655577659607\n",
      "Step 2771: Training loss: 1.7328603267669678\n",
      "Step 2772: Training loss: 1.7009904384613037\n",
      "Step 2773: Training loss: 1.3447558879852295\n",
      "Step 2774: Training loss: 1.8603906631469727\n",
      "Step 2775: Training loss: 2.0260868072509766\n",
      "Step 2776: Training loss: 2.4494566917419434\n",
      "Step 2777: Training loss: 1.6649391651153564\n",
      "Step 2778: Training loss: 3.2686853408813477\n",
      "Step 2779: Training loss: 1.283754587173462\n",
      "Step 2780: Training loss: 2.0615270137786865\n",
      "Step 2781: Training loss: 2.441861867904663\n",
      "Step 2782: Training loss: 1.2809240818023682\n",
      "Step 2783: Training loss: 2.331921100616455\n",
      "Step 2784: Training loss: 2.024970769882202\n",
      "Step 2785: Training loss: 2.6269469261169434\n",
      "Step 2786: Training loss: 2.0385076999664307\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00030e1198e4c538e3fd6c2dd190fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2787: Training loss: 2.6262729167938232\n",
      "Step 2788: Training loss: 1.904886245727539\n",
      "Step 2789: Training loss: 1.6338891983032227\n",
      "Step 2790: Training loss: 1.8357161283493042\n",
      "Step 2791: Training loss: 1.494949221611023\n",
      "Step 2792: Training loss: 1.9159682989120483\n",
      "Step 2793: Training loss: 1.199481725692749\n",
      "Step 2794: Training loss: 1.3795397281646729\n",
      "Step 2795: Training loss: 1.2983553409576416\n",
      "Step 2796: Training loss: 1.6555415391921997\n",
      "Step 2797: Training loss: 1.9164968729019165\n",
      "Step 2798: Training loss: 2.6130330562591553\n",
      "Step 2799: Training loss: 2.732544422149658\n",
      "Step 2800: Training loss: 1.8473492860794067\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2800\n",
      "Step 2801: Training loss: 1.962080478668213\n",
      "Step 2802: Training loss: 1.3498528003692627\n",
      "Step 2803: Training loss: 1.2391992807388306\n",
      "Step 2804: Training loss: 1.5613069534301758\n",
      "Step 2805: Training loss: 1.7448294162750244\n",
      "Step 2806: Training loss: 1.6199204921722412\n",
      "Step 2807: Training loss: 1.8577125072479248\n",
      "Step 2808: Training loss: 1.1752798557281494\n",
      "Step 2809: Training loss: 1.9499140977859497\n",
      "Step 2810: Training loss: 2.313133478164673\n",
      "Step 2811: Training loss: 2.145862340927124\n",
      "Step 2812: Training loss: 2.0450475215911865\n",
      "Step 2813: Training loss: 1.8121598958969116\n",
      "Step 2814: Training loss: 1.2441895008087158\n",
      "Step 2815: Training loss: 2.3728249073028564\n",
      "Step 2816: Training loss: 1.4609148502349854\n",
      "Step 2817: Training loss: 1.5374765396118164\n",
      "Step 2818: Training loss: 1.8360106945037842\n",
      "Step 2819: Training loss: 2.132171630859375\n",
      "Step 2820: Training loss: 1.905828595161438\n",
      "Step 2821: Training loss: 2.074742317199707\n",
      "Step 2822: Training loss: 1.4245930910110474\n",
      "Step 2823: Training loss: 2.520426034927368\n",
      "Step 2824: Training loss: 1.6558765172958374\n",
      "Step 2825: Training loss: 1.2110750675201416\n",
      "Step 2826: Training loss: 3.09245228767395\n",
      "Step 2827: Training loss: 1.3367702960968018\n",
      "Step 2828: Training loss: 1.9976825714111328\n",
      "Step 2829: Training loss: 1.9445000886917114\n",
      "Step 2830: Training loss: 2.633683681488037\n",
      "Step 2831: Training loss: 1.2211918830871582\n",
      "Step 2832: Training loss: 1.2756664752960205\n",
      "Step 2833: Training loss: 2.1425998210906982\n",
      "Step 2834: Training loss: 2.8435733318328857\n",
      "Step 2835: Training loss: 1.887386441230774\n",
      "Step 2836: Training loss: 1.9525110721588135\n",
      "Step 2837: Training loss: 1.2813973426818848\n",
      "Step 2838: Training loss: 1.5110445022583008\n",
      "Step 2839: Training loss: 1.4060927629470825\n",
      "Step 2840: Training loss: 1.335118055343628\n",
      "Step 2841: Training loss: 2.518216133117676\n",
      "Step 2842: Training loss: 1.8024910688400269\n",
      "Step 2843: Training loss: 1.2037972211837769\n",
      "Step 2844: Training loss: 2.825481653213501\n",
      "Step 2845: Training loss: 1.454680323600769\n",
      "Step 2846: Training loss: 1.5030910968780518\n",
      "Step 2847: Training loss: 1.264906883239746\n",
      "Step 2848: Training loss: 1.350600004196167\n",
      "Step 2849: Training loss: 1.958245873451233\n",
      "Step 2850: Training loss: 2.0847387313842773\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2850\n",
      "Step 2851: Training loss: 2.8030459880828857\n",
      "Step 2852: Training loss: 1.2643508911132812\n",
      "Step 2853: Training loss: 1.5598444938659668\n",
      "Step 2854: Training loss: 1.8258200883865356\n",
      "Step 2855: Training loss: 1.6765189170837402\n",
      "Step 2856: Training loss: 2.375779151916504\n",
      "Step 2857: Training loss: 1.6097443103790283\n",
      "Step 2858: Training loss: 1.4574885368347168\n",
      "Step 2859: Training loss: 1.7518572807312012\n",
      "Step 2860: Training loss: 2.931774139404297\n",
      "Step 2861: Training loss: 2.070073127746582\n",
      "Step 2862: Training loss: 2.0589091777801514\n",
      "Step 2863: Training loss: 1.772824764251709\n",
      "Step 2864: Training loss: 1.9301360845565796\n",
      "Step 2865: Training loss: 1.7868876457214355\n",
      "Step 2866: Training loss: 2.684889793395996\n",
      "Step 2867: Training loss: 2.013808012008667\n",
      "Step 2868: Training loss: 1.2236549854278564\n",
      "Step 2869: Training loss: 1.2181117534637451\n",
      "Step 2870: Training loss: 1.3439677953720093\n",
      "Step 2871: Training loss: 2.7551653385162354\n",
      "Step 2872: Training loss: 1.390615701675415\n",
      "Step 2873: Training loss: 1.3715684413909912\n",
      "Step 2874: Training loss: 1.25942063331604\n",
      "Step 2875: Training loss: 1.8392184972763062\n",
      "Step 2876: Training loss: 1.1731445789337158\n",
      "Step 2877: Training loss: 1.8660221099853516\n",
      "Step 2878: Training loss: 2.4619979858398438\n",
      "Step 2879: Training loss: 2.1959900856018066\n",
      "Step 2880: Training loss: 2.5991907119750977\n",
      "Step 2881: Training loss: 2.102602481842041\n",
      "Step 2882: Training loss: 2.1104445457458496\n",
      "Step 2883: Training loss: 3.2059836387634277\n",
      "Step 2884: Training loss: 1.4013793468475342\n",
      "Step 2885: Training loss: 2.5473504066467285\n",
      "Step 2886: Training loss: 2.0536186695098877\n",
      "Step 2887: Training loss: 2.793856620788574\n",
      "Step 2888: Training loss: 3.13924241065979\n",
      "Step 2889: Training loss: 1.282333493232727\n",
      "Step 2890: Training loss: 1.841795802116394\n",
      "Step 2891: Training loss: 1.2679803371429443\n",
      "Step 2892: Training loss: 1.4540526866912842\n",
      "Step 2893: Training loss: 1.9834017753601074\n",
      "Step 2894: Training loss: 1.1428426504135132\n",
      "Step 2895: Training loss: 1.9806705713272095\n",
      "Step 2896: Training loss: 1.2000712156295776\n",
      "Step 2897: Training loss: 1.9714120626449585\n",
      "Step 2898: Training loss: 2.4937984943389893\n",
      "Step 2899: Training loss: 1.5815032720565796\n",
      "Step 2900: Training loss: 2.684265375137329\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2900\n",
      "Step 2901: Training loss: 1.9301202297210693\n",
      "Step 2902: Training loss: 1.2684533596038818\n",
      "Step 2903: Training loss: 1.9680126905441284\n",
      "Step 2904: Training loss: 2.148672342300415\n",
      "Step 2905: Training loss: 1.9033308029174805\n",
      "Step 2906: Training loss: 2.4720757007598877\n",
      "Step 2907: Training loss: 2.1918575763702393\n",
      "Step 2908: Training loss: 1.432597041130066\n",
      "Step 2909: Training loss: 1.4203485250473022\n",
      "Step 2910: Training loss: 2.048586130142212\n",
      "Step 2911: Training loss: 1.899835467338562\n",
      "Step 2912: Training loss: 2.03224515914917\n",
      "Step 2913: Training loss: 1.83043372631073\n",
      "Step 2914: Training loss: 1.3820360898971558\n",
      "Step 2915: Training loss: 1.772825837135315\n",
      "Step 2916: Training loss: 2.1837496757507324\n",
      "Step 2917: Training loss: 1.4244440793991089\n",
      "Step 2918: Training loss: 1.6387412548065186\n",
      "Step 2919: Training loss: 1.242285132408142\n",
      "Step 2920: Training loss: 1.2676600217819214\n",
      "Step 2921: Training loss: 1.4344860315322876\n",
      "Step 2922: Training loss: 2.5938735008239746\n",
      "Step 2923: Training loss: 1.9136531352996826\n",
      "Step 2924: Training loss: 1.6445043087005615\n",
      "Step 2925: Training loss: 1.926410436630249\n",
      "Step 2926: Training loss: 1.2926429510116577\n",
      "Step 2927: Training loss: 2.266547918319702\n",
      "Step 2928: Training loss: 1.2723147869110107\n",
      "Step 2929: Training loss: 1.3487133979797363\n",
      "Step 2930: Training loss: 2.516852617263794\n",
      "Step 2931: Training loss: 1.573148250579834\n",
      "Step 2932: Training loss: 2.5025882720947266\n",
      "Step 2933: Training loss: 1.587937831878662\n",
      "Step 2934: Training loss: 2.0881664752960205\n",
      "Step 2935: Training loss: 1.7655316591262817\n",
      "Step 2936: Training loss: 1.2889877557754517\n",
      "Step 2937: Training loss: 1.5557451248168945\n",
      "Step 2938: Training loss: 2.3355648517608643\n",
      "Step 2939: Training loss: 1.6600910425186157\n",
      "Step 2940: Training loss: 2.3879263401031494\n",
      "Step 2941: Training loss: 1.2066025733947754\n",
      "Step 2942: Training loss: 2.5317649841308594\n",
      "Step 2943: Training loss: 1.5644209384918213\n",
      "Step 2944: Training loss: 1.9173413515090942\n",
      "Step 2945: Training loss: 1.7421252727508545\n",
      "Step 2946: Training loss: 1.324938178062439\n",
      "Step 2947: Training loss: 1.3021349906921387\n",
      "Step 2948: Training loss: 1.668160319328308\n",
      "Step 2949: Training loss: 1.8040001392364502\n",
      "Step 2950: Training loss: 1.8215887546539307\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-2950\n",
      "Step 2951: Training loss: 1.2881683111190796\n",
      "Step 2952: Training loss: 2.030231237411499\n",
      "Step 2953: Training loss: 2.2236220836639404\n",
      "Step 2954: Training loss: 1.7343724966049194\n",
      "Step 2955: Training loss: 2.5242438316345215\n",
      "Step 2956: Training loss: 1.9781643152236938\n",
      "Step 2957: Training loss: 2.57631516456604\n",
      "Step 2958: Training loss: 2.1035468578338623\n",
      "Step 2959: Training loss: 2.153789758682251\n",
      "Step 2960: Training loss: 1.834646224975586\n",
      "Step 2961: Training loss: 1.402583360671997\n",
      "Step 2962: Training loss: 1.9877312183380127\n",
      "Step 2963: Training loss: 1.3016959428787231\n",
      "Step 2964: Training loss: 1.7248694896697998\n",
      "Step 2965: Training loss: 1.911567211151123\n",
      "Step 2966: Training loss: 2.4655423164367676\n",
      "Step 2967: Training loss: 1.7257338762283325\n",
      "Step 2968: Training loss: 1.9084882736206055\n",
      "Step 2969: Training loss: 1.9142965078353882\n",
      "Step 2970: Training loss: 1.826036810874939\n",
      "Step 2971: Training loss: 1.8268048763275146\n",
      "Step 2972: Training loss: 1.8694864511489868\n",
      "Step 2973: Training loss: 1.7971601486206055\n",
      "Step 2974: Training loss: 1.3698288202285767\n",
      "Step 2975: Training loss: 1.8369873762130737\n",
      "Step 2976: Training loss: 2.0929481983184814\n",
      "Step 2977: Training loss: 1.2028664350509644\n",
      "Step 2978: Training loss: 1.592880368232727\n",
      "Step 2979: Training loss: 2.5287892818450928\n",
      "Step 2980: Training loss: 1.3263165950775146\n",
      "Step 2981: Training loss: 1.8787963390350342\n",
      "Step 2982: Training loss: 1.817989468574524\n",
      "Step 2983: Training loss: 1.7901719808578491\n",
      "Step 2984: Training loss: 2.2621915340423584\n",
      "Step 2985: Training loss: 2.4979724884033203\n",
      "Step 2986: Training loss: 1.3456034660339355\n",
      "Step 2987: Training loss: 2.1827266216278076\n",
      "Step 2988: Training loss: 1.5275071859359741\n",
      "Step 2989: Training loss: 1.3860632181167603\n",
      "Step 2990: Training loss: 1.1823277473449707\n",
      "Step 2991: Training loss: 2.3675899505615234\n",
      "Step 2992: Training loss: 1.793619990348816\n",
      "Step 2993: Training loss: 1.405272126197815\n",
      "Step 2994: Training loss: 2.630460023880005\n",
      "Step 2995: Training loss: 1.8525898456573486\n",
      "Step 2996: Training loss: 2.2805593013763428\n",
      "Step 2997: Training loss: 1.6020822525024414\n",
      "Step 2998: Training loss: 1.8045783042907715\n",
      "Step 2999: Training loss: 1.3091567754745483\n",
      "Step 3000: Training loss: 1.8908889293670654\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3000\n",
      "Step 3001: Training loss: 1.2660036087036133\n",
      "Step 3002: Training loss: 1.2984967231750488\n",
      "Step 3003: Training loss: 1.7335048913955688\n",
      "Step 3004: Training loss: 2.1647086143493652\n",
      "Step 3005: Training loss: 1.9228332042694092\n",
      "Step 3006: Training loss: 1.410096287727356\n",
      "Step 3007: Training loss: 1.2400000095367432\n",
      "Step 3008: Training loss: 1.6398112773895264\n",
      "Step 3009: Training loss: 1.7053146362304688\n",
      "Step 3010: Training loss: 2.3878095149993896\n",
      "Step 3011: Training loss: 1.7029187679290771\n",
      "Step 3012: Training loss: 2.1912245750427246\n",
      "Step 3013: Training loss: 1.2143763303756714\n",
      "Step 3014: Training loss: 1.3049225807189941\n",
      "Step 3015: Training loss: 1.7804234027862549\n",
      "Step 3016: Training loss: 1.7157756090164185\n",
      "Step 3017: Training loss: 2.0283102989196777\n",
      "Step 3018: Training loss: 1.7111616134643555\n",
      "Step 3019: Training loss: 1.1971198320388794\n",
      "Step 3020: Training loss: 2.2076680660247803\n",
      "Step 3021: Training loss: 1.4458839893341064\n",
      "Step 3022: Training loss: 1.2501904964447021\n",
      "Step 3023: Training loss: 2.174546480178833\n",
      "Step 3024: Training loss: 2.0599379539489746\n",
      "Step 3025: Training loss: 2.04972243309021\n",
      "Step 3026: Training loss: 1.3425877094268799\n",
      "Step 3027: Training loss: 1.230954647064209\n",
      "Step 3028: Training loss: 1.9880071878433228\n",
      "Step 3029: Training loss: 1.3952500820159912\n",
      "Step 3030: Training loss: 1.7578151226043701\n",
      "Step 3031: Training loss: 1.3389793634414673\n",
      "Step 3032: Training loss: 1.934402585029602\n",
      "Step 3033: Training loss: 1.9262524843215942\n",
      "Step 3034: Training loss: 1.9177358150482178\n",
      "Step 3035: Training loss: 1.1422861814498901\n",
      "Step 3036: Training loss: 1.3570845127105713\n",
      "Step 3037: Training loss: 2.027858018875122\n",
      "Step 3038: Training loss: 1.1331220865249634\n",
      "Step 3039: Training loss: 1.2306677103042603\n",
      "Step 3040: Training loss: 1.8137129545211792\n",
      "Step 3041: Training loss: 1.5212346315383911\n",
      "Step 3042: Training loss: 1.6780182123184204\n",
      "Step 3043: Training loss: 1.9415671825408936\n",
      "Step 3044: Training loss: 2.300816535949707\n",
      "Step 3045: Training loss: 1.752090334892273\n",
      "Step 3046: Training loss: 1.4492104053497314\n",
      "Step 3047: Training loss: 1.9290993213653564\n",
      "Step 3048: Training loss: 1.8134289979934692\n",
      "Step 3049: Training loss: 1.8976696729660034\n",
      "Step 3050: Training loss: 1.1719670295715332\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3050\n",
      "Step 3051: Training loss: 2.195343017578125\n",
      "Step 3052: Training loss: 1.7177993059158325\n",
      "Step 3053: Training loss: 1.3138502836227417\n",
      "Step 3054: Training loss: 1.167633295059204\n",
      "Step 3055: Training loss: 1.7076736688613892\n",
      "Step 3056: Training loss: 1.7776947021484375\n",
      "Step 3057: Training loss: 1.1763408184051514\n",
      "Step 3058: Training loss: 1.623646855354309\n",
      "Step 3059: Training loss: 1.7558948993682861\n",
      "Step 3060: Training loss: 1.1671475172042847\n",
      "Step 3061: Training loss: 1.2399426698684692\n",
      "Step 3062: Training loss: 1.2186344861984253\n",
      "Step 3063: Training loss: 1.1993310451507568\n",
      "Step 3064: Training loss: 1.456710696220398\n",
      "Step 3065: Training loss: 2.0468897819519043\n",
      "Step 3066: Training loss: 1.170234203338623\n",
      "Step 3067: Training loss: 1.158233404159546\n",
      "Step 3068: Training loss: 1.7564115524291992\n",
      "Step 3069: Training loss: 1.1396484375\n",
      "Step 3070: Training loss: 2.1894612312316895\n",
      "Step 3071: Training loss: 2.0211594104766846\n",
      "Step 3072: Training loss: 1.6080708503723145\n",
      "Step 3073: Training loss: 1.1536401510238647\n",
      "Step 3074: Training loss: 1.6897270679473877\n",
      "Step 3075: Training loss: 1.316636323928833\n",
      "Step 3076: Training loss: 2.131272315979004\n",
      "Step 3077: Training loss: 1.1603877544403076\n",
      "Step 3078: Training loss: 1.461806297302246\n",
      "Step 3079: Training loss: 1.8235082626342773\n",
      "Step 3080: Training loss: 1.9878696203231812\n",
      "Step 3081: Training loss: 1.770462155342102\n",
      "Step 3082: Training loss: 1.960368037223816\n",
      "Step 3083: Training loss: 1.7434852123260498\n",
      "Step 3084: Training loss: 1.2904791831970215\n",
      "Step 3085: Training loss: 1.7669814825057983\n",
      "Step 3086: Training loss: 1.851212501525879\n",
      "Step 3087: Training loss: 1.720000147819519\n",
      "Step 3088: Training loss: 2.1530470848083496\n",
      "Step 3089: Training loss: 1.293241024017334\n",
      "Step 3090: Training loss: 2.269580125808716\n",
      "Step 3091: Training loss: 2.3436193466186523\n",
      "Step 3092: Training loss: 1.801232099533081\n",
      "Step 3093: Training loss: 2.153524875640869\n",
      "Step 3094: Training loss: 1.7987345457077026\n",
      "Step 3095: Training loss: 1.3747178316116333\n",
      "Step 3096: Training loss: 1.7298158407211304\n",
      "Step 3097: Training loss: 1.2991217374801636\n",
      "Step 3098: Training loss: 1.9919108152389526\n",
      "Step 3099: Training loss: 1.5886684656143188\n",
      "Step 3100: Training loss: 1.245343804359436\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3100\n",
      "Step 3101: Training loss: 1.5957648754119873\n",
      "Step 3102: Training loss: 2.421600580215454\n",
      "Step 3103: Training loss: 1.3627877235412598\n",
      "Step 3104: Training loss: 1.2241673469543457\n",
      "Step 3105: Training loss: 1.8114581108093262\n",
      "Step 3106: Training loss: 1.1507842540740967\n",
      "Step 3107: Training loss: 1.8425464630126953\n",
      "Step 3108: Training loss: 1.175559639930725\n",
      "Step 3109: Training loss: 1.680535912513733\n",
      "Step 3110: Training loss: 1.1410223245620728\n",
      "Step 3111: Training loss: 1.807923436164856\n",
      "Step 3112: Training loss: 1.8901715278625488\n",
      "Step 3113: Training loss: 2.480130672454834\n",
      "Step 3114: Training loss: 1.5726290941238403\n",
      "Step 3115: Training loss: 1.4629788398742676\n",
      "Step 3116: Training loss: 2.0891244411468506\n",
      "Step 3117: Training loss: 1.7130120992660522\n",
      "Step 3118: Training loss: 1.9803489446640015\n",
      "Step 3119: Training loss: 2.052320957183838\n",
      "Step 3120: Training loss: 1.6608998775482178\n",
      "Step 3121: Training loss: 1.5274956226348877\n",
      "Step 3122: Training loss: 1.287765622138977\n",
      "Step 3123: Training loss: 1.4101799726486206\n",
      "Step 3124: Training loss: 1.263418197631836\n",
      "Step 3125: Training loss: 1.5586628913879395\n",
      "Step 3126: Training loss: 1.6861588954925537\n",
      "Step 3127: Training loss: 2.0625507831573486\n",
      "Step 3128: Training loss: 1.326579213142395\n",
      "Step 3129: Training loss: 1.158130407333374\n",
      "Step 3130: Training loss: 1.5796538591384888\n",
      "Step 3131: Training loss: 1.7720592021942139\n",
      "Step 3132: Training loss: 2.066671133041382\n",
      "Step 3133: Training loss: 1.1557451486587524\n",
      "Step 3134: Training loss: 1.1966547966003418\n",
      "Step 3135: Training loss: 1.8245365619659424\n",
      "Step 3136: Training loss: 1.245766043663025\n",
      "Step 3137: Training loss: 1.9002630710601807\n",
      "Step 3138: Training loss: 1.682936668395996\n",
      "Step 3139: Training loss: 1.6237819194793701\n",
      "Step 3140: Training loss: 1.214996337890625\n",
      "Step 3141: Training loss: 1.8601312637329102\n",
      "Step 3142: Training loss: 1.1871967315673828\n",
      "Step 3143: Training loss: 2.0826096534729004\n",
      "Step 3144: Training loss: 1.4921973943710327\n",
      "Step 3145: Training loss: 1.5621284246444702\n",
      "Step 3146: Training loss: 2.3991141319274902\n",
      "Step 3147: Training loss: 1.1419126987457275\n",
      "Step 3148: Training loss: 2.323745012283325\n",
      "Step 3149: Training loss: 1.6712372303009033\n",
      "Step 3150: Training loss: 1.2394261360168457\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3150\n",
      "Step 3151: Training loss: 1.469515085220337\n",
      "Step 3152: Training loss: 1.1295092105865479\n",
      "Step 3153: Training loss: 2.041804313659668\n",
      "Step 3154: Training loss: 1.7740150690078735\n",
      "Step 3155: Training loss: 1.1820728778839111\n",
      "Step 3156: Training loss: 1.4552277326583862\n",
      "Step 3157: Training loss: 1.80568528175354\n",
      "Step 3158: Training loss: 2.2231452465057373\n",
      "Step 3159: Training loss: 1.2742348909378052\n",
      "Step 3160: Training loss: 1.5783874988555908\n",
      "Step 3161: Training loss: 1.670741081237793\n",
      "Step 3162: Training loss: 1.997177004814148\n",
      "Step 3163: Training loss: 1.1457786560058594\n",
      "Step 3164: Training loss: 1.430716633796692\n",
      "Step 3165: Training loss: 1.5412343740463257\n",
      "Step 3166: Training loss: 1.4699004888534546\n",
      "Step 3167: Training loss: 2.141563653945923\n",
      "Step 3168: Training loss: 3.4739441871643066\n",
      "Step 3169: Training loss: 1.8452496528625488\n",
      "Step 3170: Training loss: 1.7402560710906982\n",
      "Step 3171: Training loss: 1.2523390054702759\n",
      "Step 3172: Training loss: 1.4024158716201782\n",
      "Step 3173: Training loss: 1.6164084672927856\n",
      "Step 3174: Training loss: 1.4498012065887451\n",
      "Step 3175: Training loss: 1.1525486707687378\n",
      "Step 3176: Training loss: 1.2156144380569458\n",
      "Step 3177: Training loss: 1.80709707736969\n",
      "Step 3178: Training loss: 2.3274765014648438\n",
      "Step 3179: Training loss: 1.2058494091033936\n",
      "Step 3180: Training loss: 1.576991319656372\n",
      "Step 3181: Training loss: 1.3968894481658936\n",
      "Step 3182: Training loss: 1.7868276834487915\n",
      "Step 3183: Training loss: 1.1337246894836426\n",
      "Step 3184: Training loss: 1.8576699495315552\n",
      "Step 3185: Training loss: 1.7096372842788696\n",
      "Step 3186: Training loss: 1.6427735090255737\n",
      "Step 3187: Training loss: 1.9870216846466064\n",
      "Step 3188: Training loss: 1.1355351209640503\n",
      "Step 3189: Training loss: 1.6361857652664185\n",
      "Step 3190: Training loss: 1.6031835079193115\n",
      "Step 3191: Training loss: 2.032212495803833\n",
      "Step 3192: Training loss: 2.0621840953826904\n",
      "Step 3193: Training loss: 1.732595443725586\n",
      "Step 3194: Training loss: 1.574522852897644\n",
      "Step 3195: Training loss: 1.4026931524276733\n",
      "Step 3196: Training loss: 1.98804771900177\n",
      "Step 3197: Training loss: 1.2676100730895996\n",
      "Step 3198: Training loss: 1.7204127311706543\n",
      "Step 3199: Training loss: 1.4498422145843506\n",
      "Step 3200: Training loss: 1.8332056999206543\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3200\n",
      "Step 3201: Training loss: 2.1607108116149902\n",
      "Step 3202: Training loss: 1.1216505765914917\n",
      "Step 3203: Training loss: 1.4324134588241577\n",
      "Step 3204: Training loss: 1.154420256614685\n",
      "Step 3205: Training loss: 1.395937442779541\n",
      "Step 3206: Training loss: 1.7346585988998413\n",
      "Step 3207: Training loss: 1.7464280128479004\n",
      "Step 3208: Training loss: 2.2861194610595703\n",
      "Step 3209: Training loss: 1.2855937480926514\n",
      "Step 3210: Training loss: 1.0710792541503906\n",
      "Step 3211: Training loss: 1.1831815242767334\n",
      "Step 3212: Training loss: 2.0944325923919678\n",
      "Step 3213: Training loss: 1.7699543237686157\n",
      "Step 3214: Training loss: 1.1721876859664917\n",
      "Step 3215: Training loss: 1.555670976638794\n",
      "Step 3216: Training loss: 2.1693544387817383\n",
      "Step 3217: Training loss: 1.2321476936340332\n",
      "Step 3218: Training loss: 2.043010711669922\n",
      "Step 3219: Training loss: 1.483717918395996\n",
      "Step 3220: Training loss: 1.4794245958328247\n",
      "Step 3221: Training loss: 1.2981505393981934\n",
      "Step 3222: Training loss: 1.1200816631317139\n",
      "Step 3223: Training loss: 1.3399860858917236\n",
      "Step 3224: Training loss: 1.263700246810913\n",
      "Step 3225: Training loss: 2.103893518447876\n",
      "Step 3226: Training loss: 1.5198736190795898\n",
      "Step 3227: Training loss: 1.8488863706588745\n",
      "Step 3228: Training loss: 1.3268743753433228\n",
      "Step 3229: Training loss: 1.3142545223236084\n",
      "Step 3230: Training loss: 1.1246980428695679\n",
      "Step 3231: Training loss: 2.2765965461730957\n",
      "Step 3232: Training loss: 1.8517723083496094\n",
      "Step 3233: Training loss: 1.0798661708831787\n",
      "Step 3234: Training loss: 1.217095971107483\n",
      "Step 3235: Training loss: 2.087412118911743\n",
      "Step 3236: Training loss: 1.47625732421875\n",
      "Step 3237: Training loss: 1.6166987419128418\n",
      "Step 3238: Training loss: 1.3669275045394897\n",
      "Step 3239: Training loss: 1.7246043682098389\n",
      "Step 3240: Training loss: 1.581910490989685\n",
      "Step 3241: Training loss: 1.977925181388855\n",
      "Step 3242: Training loss: 1.2542036771774292\n",
      "Step 3243: Training loss: 2.238260269165039\n",
      "Step 3244: Training loss: 1.725810170173645\n",
      "Step 3245: Training loss: 1.8640496730804443\n",
      "Step 3246: Training loss: 1.2351012229919434\n",
      "Step 3247: Training loss: 1.3786253929138184\n",
      "Step 3248: Training loss: 1.9848531484603882\n",
      "Step 3249: Training loss: 1.4460431337356567\n",
      "Step 3250: Training loss: 1.8267672061920166\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3250\n",
      "Step 3251: Training loss: 1.492055058479309\n",
      "Step 3252: Training loss: 1.7208313941955566\n",
      "Step 3253: Training loss: 1.3066606521606445\n",
      "Step 3254: Training loss: 1.594799280166626\n",
      "Step 3255: Training loss: 1.0777215957641602\n",
      "Step 3256: Training loss: 1.0792306661605835\n",
      "Step 3257: Training loss: 1.694180965423584\n",
      "Step 3258: Training loss: 1.6023937463760376\n",
      "Step 3259: Training loss: 1.6515917778015137\n",
      "Step 3260: Training loss: 1.8119580745697021\n",
      "Step 3261: Training loss: 1.8019909858703613\n",
      "Step 3262: Training loss: 1.1163928508758545\n",
      "Step 3263: Training loss: 1.5911216735839844\n",
      "Step 3264: Training loss: 1.5164273977279663\n",
      "Step 3265: Training loss: 1.6202927827835083\n",
      "Step 3266: Training loss: 1.5293710231781006\n",
      "Step 3267: Training loss: 2.057281255722046\n",
      "Step 3268: Training loss: 1.1236636638641357\n",
      "Step 3269: Training loss: 1.2064894437789917\n",
      "Step 3270: Training loss: 1.3349562883377075\n",
      "Step 3271: Training loss: 2.023789882659912\n",
      "Step 3272: Training loss: 1.519481897354126\n",
      "Step 3273: Training loss: 2.5939183235168457\n",
      "Step 3274: Training loss: 1.4927642345428467\n",
      "Step 3275: Training loss: 1.5964617729187012\n",
      "Step 3276: Training loss: 1.5469812154769897\n",
      "Step 3277: Training loss: 1.6401417255401611\n",
      "Step 3278: Training loss: 1.8917912244796753\n",
      "Step 3279: Training loss: 1.6243767738342285\n",
      "Step 3280: Training loss: 1.5872396230697632\n",
      "Step 3281: Training loss: 1.2789700031280518\n",
      "Step 3282: Training loss: 2.1513051986694336\n",
      "Step 3283: Training loss: 1.916771650314331\n",
      "Step 3284: Training loss: 1.9547230005264282\n",
      "Step 3285: Training loss: 1.230586290359497\n",
      "Step 3286: Training loss: 1.3834457397460938\n",
      "Step 3287: Training loss: 1.7811753749847412\n",
      "Step 3288: Training loss: 1.7467241287231445\n",
      "Step 3289: Training loss: 1.6221379041671753\n",
      "Step 3290: Training loss: 1.206098198890686\n",
      "Step 3291: Training loss: 1.3325366973876953\n",
      "Step 3292: Training loss: 1.9588984251022339\n",
      "Step 3293: Training loss: 1.318281888961792\n",
      "Step 3294: Training loss: 1.3610146045684814\n",
      "Step 3295: Training loss: 2.2913811206817627\n",
      "Step 3296: Training loss: 2.0794248580932617\n",
      "Step 3297: Training loss: 1.2454556226730347\n",
      "Step 3298: Training loss: 1.9074488878250122\n",
      "Step 3299: Training loss: 1.4943153858184814\n",
      "Step 3300: Training loss: 1.107824683189392\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3300\n",
      "Step 3301: Training loss: 1.4579954147338867\n",
      "Step 3302: Training loss: 1.4718787670135498\n",
      "Step 3303: Training loss: 1.253067970275879\n",
      "Step 3304: Training loss: 1.1922988891601562\n",
      "Step 3305: Training loss: 1.2128206491470337\n",
      "Step 3306: Training loss: 1.0967165231704712\n",
      "Step 3307: Training loss: 2.2379961013793945\n",
      "Step 3308: Training loss: 1.1386100053787231\n",
      "Step 3309: Training loss: 2.1113204956054688\n",
      "Step 3310: Training loss: 1.6107871532440186\n",
      "Step 3311: Training loss: 1.2028944492340088\n",
      "Step 3312: Training loss: 1.5854324102401733\n",
      "Step 3313: Training loss: 1.3977935314178467\n",
      "Step 3314: Training loss: 1.0865172147750854\n",
      "Step 3315: Training loss: 1.858108401298523\n",
      "Step 3316: Training loss: 1.125975489616394\n",
      "Step 3317: Training loss: 1.3979322910308838\n",
      "Step 3318: Training loss: 1.1579196453094482\n",
      "Step 3319: Training loss: 1.963181972503662\n",
      "Step 3320: Training loss: 1.754143476486206\n",
      "Step 3321: Training loss: 1.2873083353042603\n",
      "Step 3322: Training loss: 1.1633380651474\n",
      "Step 3323: Training loss: 1.4209285974502563\n",
      "Step 3324: Training loss: 1.4407397508621216\n",
      "Step 3325: Training loss: 1.1337834596633911\n",
      "Step 3326: Training loss: 1.5338033437728882\n",
      "Step 3327: Training loss: 1.176540493965149\n",
      "Step 3328: Training loss: 1.2949268817901611\n",
      "Step 3329: Training loss: 2.075523853302002\n",
      "Step 3330: Training loss: 1.5245710611343384\n",
      "Step 3331: Training loss: 1.396030306816101\n",
      "Step 3332: Training loss: 1.714080810546875\n",
      "Step 3333: Training loss: 1.703718662261963\n",
      "Step 3334: Training loss: 1.0997000932693481\n",
      "Step 3335: Training loss: 1.7451844215393066\n",
      "Step 3336: Training loss: 1.1404786109924316\n",
      "Step 3337: Training loss: 1.2822701930999756\n",
      "Step 3338: Training loss: 1.0257489681243896\n",
      "Step 3339: Training loss: 1.3366775512695312\n",
      "Step 3340: Training loss: 2.1963109970092773\n",
      "Step 3341: Training loss: 2.263923168182373\n",
      "Step 3342: Training loss: 1.0962532758712769\n",
      "Step 3343: Training loss: 1.4383362531661987\n",
      "Step 3344: Training loss: 1.0708109140396118\n",
      "Step 3345: Training loss: 1.5780240297317505\n",
      "Step 3346: Training loss: 1.82578444480896\n",
      "Step 3347: Training loss: 1.474166750907898\n",
      "Step 3348: Training loss: 1.2701647281646729\n",
      "Step 3349: Training loss: 2.17760968208313\n",
      "Step 3350: Training loss: 1.2468047142028809\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3350\n",
      "Step 3351: Training loss: 1.0591027736663818\n",
      "Step 3352: Training loss: 1.150802731513977\n",
      "Step 3353: Training loss: 1.2390291690826416\n",
      "Step 3354: Training loss: 2.0343260765075684\n",
      "Step 3355: Training loss: 1.1823811531066895\n",
      "Step 3356: Training loss: 1.6608799695968628\n",
      "Step 3357: Training loss: 1.698509693145752\n",
      "Step 3358: Training loss: 1.895789384841919\n",
      "Step 3359: Training loss: 1.8394948244094849\n",
      "Step 3360: Training loss: 1.9759658575057983\n",
      "Step 3361: Training loss: 1.6380780935287476\n",
      "Step 3362: Training loss: 1.8543951511383057\n",
      "Step 3363: Training loss: 1.2612448930740356\n",
      "Step 3364: Training loss: 1.6909065246582031\n",
      "Step 3365: Training loss: 1.96957266330719\n",
      "Step 3366: Training loss: 1.6492425203323364\n",
      "Step 3367: Training loss: 1.5657724142074585\n",
      "Step 3368: Training loss: 1.7858545780181885\n",
      "Step 3369: Training loss: 1.4277364015579224\n",
      "Step 3370: Training loss: 1.9784437417984009\n",
      "Step 3371: Training loss: 1.4366103410720825\n",
      "Step 3372: Training loss: 1.5434495210647583\n",
      "Step 3373: Training loss: 1.899285078048706\n",
      "Step 3374: Training loss: 1.505447268486023\n",
      "Step 3375: Training loss: 1.687638282775879\n",
      "Step 3376: Training loss: 1.2994871139526367\n",
      "Step 3377: Training loss: 1.5869473218917847\n",
      "Step 3378: Training loss: 1.9732617139816284\n",
      "Step 3379: Training loss: 1.1377556324005127\n",
      "Step 3380: Training loss: 1.3726342916488647\n",
      "Step 3381: Training loss: 1.2137190103530884\n",
      "Step 3382: Training loss: 1.132911205291748\n",
      "Step 3383: Training loss: 1.465900182723999\n",
      "Step 3384: Training loss: 2.0836102962493896\n",
      "Step 3385: Training loss: 1.611707329750061\n",
      "Step 3386: Training loss: 1.241797924041748\n",
      "Step 3387: Training loss: 1.5818229913711548\n",
      "Step 3388: Training loss: 1.50205397605896\n",
      "Step 3389: Training loss: 1.1082141399383545\n",
      "Step 3390: Training loss: 1.5545976161956787\n",
      "Step 3391: Training loss: 1.584104299545288\n",
      "Step 3392: Training loss: 2.2477729320526123\n",
      "Step 3393: Training loss: 1.486746907234192\n",
      "Step 3394: Training loss: 1.0833580493927002\n",
      "Step 3395: Training loss: 2.077423572540283\n",
      "Step 3396: Training loss: 1.2952401638031006\n",
      "Step 3397: Training loss: 1.0905077457427979\n",
      "Step 3398: Training loss: 1.2904094457626343\n",
      "Step 3399: Training loss: 1.1170635223388672\n",
      "Step 3400: Training loss: 1.498548150062561\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3400\n",
      "Step 3401: Training loss: 1.6122711896896362\n",
      "Step 3402: Training loss: 1.6111176013946533\n",
      "Step 3403: Training loss: 1.7251967191696167\n",
      "Step 3404: Training loss: 2.1116786003112793\n",
      "Step 3405: Training loss: 1.5565524101257324\n",
      "Step 3406: Training loss: 1.6240758895874023\n",
      "Step 3407: Training loss: 1.4301203489303589\n",
      "Step 3408: Training loss: 1.2598811388015747\n",
      "Step 3409: Training loss: 1.680800199508667\n",
      "Step 3410: Training loss: 1.6418356895446777\n",
      "Step 3411: Training loss: 1.4969640970230103\n",
      "Step 3412: Training loss: 1.537323236465454\n",
      "Step 3413: Training loss: 1.155495047569275\n",
      "Step 3414: Training loss: 1.1809300184249878\n",
      "Step 3415: Training loss: 2.0826315879821777\n",
      "Step 3416: Training loss: 1.4421768188476562\n",
      "Step 3417: Training loss: 1.2038034200668335\n",
      "Step 3418: Training loss: 1.495319128036499\n",
      "Step 3419: Training loss: 0.9058696627616882\n",
      "Step 3420: Training loss: 1.314684510231018\n",
      "Step 3421: Training loss: 1.573861002922058\n",
      "Step 3422: Training loss: 1.7179104089736938\n",
      "Step 3423: Training loss: 1.4230438470840454\n",
      "Step 3424: Training loss: 1.5434304475784302\n",
      "Step 3425: Training loss: 1.173708438873291\n",
      "Step 3426: Training loss: 1.0509179830551147\n",
      "Step 3427: Training loss: 1.189210295677185\n",
      "Step 3428: Training loss: 1.4519981145858765\n",
      "Step 3429: Training loss: 1.1487376689910889\n",
      "Step 3430: Training loss: 1.2692368030548096\n",
      "Step 3431: Training loss: 1.8077298402786255\n",
      "Step 3432: Training loss: 1.5108237266540527\n",
      "Step 3433: Training loss: 1.5223519802093506\n",
      "Step 3434: Training loss: 1.4133949279785156\n",
      "Step 3435: Training loss: 1.2875782251358032\n",
      "Step 3436: Training loss: 1.169973611831665\n",
      "Step 3437: Training loss: 1.2698421478271484\n",
      "Step 3438: Training loss: 1.2529656887054443\n",
      "Step 3439: Training loss: 1.054234504699707\n",
      "Step 3440: Training loss: 1.026847243309021\n",
      "Step 3441: Training loss: 1.1451265811920166\n",
      "Step 3442: Training loss: 1.6019421815872192\n",
      "Step 3443: Training loss: 1.2433851957321167\n",
      "Step 3444: Training loss: 1.8082695007324219\n",
      "Step 3445: Training loss: 1.6227357387542725\n",
      "Step 3446: Training loss: 1.1928235292434692\n",
      "Step 3447: Training loss: 1.2034817934036255\n",
      "Step 3448: Training loss: 1.9559156894683838\n",
      "Step 3449: Training loss: 1.7479522228240967\n",
      "Step 3450: Training loss: 1.2855713367462158\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3450\n",
      "Step 3451: Training loss: 1.4392362833023071\n",
      "Step 3452: Training loss: 1.0250221490859985\n",
      "Step 3453: Training loss: 1.2632368803024292\n",
      "Step 3454: Training loss: 1.8371933698654175\n",
      "Step 3455: Training loss: 1.2229863405227661\n",
      "Step 3456: Training loss: 1.0691089630126953\n",
      "Step 3457: Training loss: 1.3318462371826172\n",
      "Step 3458: Training loss: 1.0462192296981812\n",
      "Step 3459: Training loss: 1.483492374420166\n",
      "Step 3460: Training loss: 1.5627027750015259\n",
      "Step 3461: Training loss: 0.9903436899185181\n",
      "Step 3462: Training loss: 1.125024676322937\n",
      "Step 3463: Training loss: 1.8584074974060059\n",
      "Step 3464: Training loss: 1.0752897262573242\n",
      "Step 3465: Training loss: 1.4259769916534424\n",
      "Step 3466: Training loss: 1.0337830781936646\n",
      "Step 3467: Training loss: 1.1324113607406616\n",
      "Step 3468: Training loss: 1.0824233293533325\n",
      "Step 3469: Training loss: 1.675018548965454\n",
      "Step 3470: Training loss: 1.0794878005981445\n",
      "Step 3471: Training loss: 1.752346396446228\n",
      "Step 3472: Training loss: 1.3113789558410645\n",
      "Step 3473: Training loss: 1.3826282024383545\n",
      "Step 3474: Training loss: 1.1233313083648682\n",
      "Step 3475: Training loss: 1.1575709581375122\n",
      "Step 3476: Training loss: 1.8365058898925781\n",
      "Step 3477: Training loss: 2.151773691177368\n",
      "Step 3478: Training loss: 1.6407018899917603\n",
      "Step 3479: Training loss: 1.6336325407028198\n",
      "Step 3480: Training loss: 1.0583100318908691\n",
      "Step 3481: Training loss: 1.510465383529663\n",
      "Step 3482: Training loss: 1.299940586090088\n",
      "Step 3483: Training loss: 1.2235393524169922\n",
      "Step 3484: Training loss: 1.0223792791366577\n",
      "Step 3485: Training loss: 1.6416537761688232\n",
      "Step 3486: Training loss: 1.2643564939498901\n",
      "Step 3487: Training loss: 2.1180286407470703\n",
      "Step 3488: Training loss: 1.1793153285980225\n",
      "Step 3489: Training loss: 1.423785924911499\n",
      "Step 3490: Training loss: 1.4745314121246338\n",
      "Step 3491: Training loss: 1.1306363344192505\n",
      "Step 3492: Training loss: 1.3948994874954224\n",
      "Step 3493: Training loss: 1.3109904527664185\n",
      "Step 3494: Training loss: 1.5702813863754272\n",
      "Step 3495: Training loss: 2.1655938625335693\n",
      "Step 3496: Training loss: 1.6395483016967773\n",
      "Step 3497: Training loss: 1.1759288311004639\n",
      "Step 3498: Training loss: 1.0870592594146729\n",
      "Step 3499: Training loss: 1.1783947944641113\n",
      "Step 3500: Training loss: 1.5981436967849731\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3500\n",
      "Step 3501: Training loss: 1.0839951038360596\n",
      "Step 3502: Training loss: 1.5171613693237305\n",
      "Step 3503: Training loss: 1.679921269416809\n",
      "Step 3504: Training loss: 1.3166375160217285\n",
      "Step 3505: Training loss: 1.2829439640045166\n",
      "Step 3506: Training loss: 1.5000224113464355\n",
      "Step 3507: Training loss: 1.120100498199463\n",
      "Step 3508: Training loss: 1.0876226425170898\n",
      "Step 3509: Training loss: 1.3688364028930664\n",
      "Step 3510: Training loss: 1.5516307353973389\n",
      "Step 3511: Training loss: 1.5472445487976074\n",
      "Step 3512: Training loss: 1.4187227487564087\n",
      "Step 3513: Training loss: 2.427661657333374\n",
      "Step 3514: Training loss: 1.0857828855514526\n",
      "Step 3515: Training loss: 1.2718585729599\n",
      "Step 3516: Training loss: 1.324432134628296\n",
      "Step 3517: Training loss: 1.5288845300674438\n",
      "Step 3518: Training loss: 1.3512625694274902\n",
      "Step 3519: Training loss: 1.125527262687683\n",
      "Step 3520: Training loss: 1.4065608978271484\n",
      "Step 3521: Training loss: 1.7520405054092407\n",
      "Step 3522: Training loss: 1.7562850713729858\n",
      "Step 3523: Training loss: 1.199687123298645\n",
      "Step 3524: Training loss: 1.4784406423568726\n",
      "Step 3525: Training loss: 1.0728538036346436\n",
      "Step 3526: Training loss: 1.457512378692627\n",
      "Step 3527: Training loss: 1.046152114868164\n",
      "Step 3528: Training loss: 1.1582552194595337\n",
      "Step 3529: Training loss: 1.2881113290786743\n",
      "Step 3530: Training loss: 1.10845148563385\n",
      "Step 3531: Training loss: 1.2063008546829224\n",
      "Step 3532: Training loss: 1.0909146070480347\n",
      "Step 3533: Training loss: 1.5359317064285278\n",
      "Step 3534: Training loss: 1.2099851369857788\n",
      "Step 3535: Training loss: 1.308701992034912\n",
      "Step 3536: Training loss: 1.5780417919158936\n",
      "Step 3537: Training loss: 1.3375792503356934\n",
      "Step 3538: Training loss: 1.5110576152801514\n",
      "Step 3539: Training loss: 1.0005563497543335\n",
      "Step 3540: Training loss: 1.7017961740493774\n",
      "Step 3541: Training loss: 1.4947618246078491\n",
      "Step 3542: Training loss: 1.0789908170700073\n",
      "Step 3543: Training loss: 1.298417091369629\n",
      "Step 3544: Training loss: 1.4805585145950317\n",
      "Step 3545: Training loss: 1.580668568611145\n",
      "Step 3546: Training loss: 1.452562689781189\n",
      "Step 3547: Training loss: 1.6587181091308594\n",
      "Step 3548: Training loss: 1.3661421537399292\n",
      "Step 3549: Training loss: 1.1813172101974487\n",
      "Step 3550: Training loss: 1.2262182235717773\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3550\n",
      "Step 3551: Training loss: 0.9670917391777039\n",
      "Step 3552: Training loss: 1.4623825550079346\n",
      "Step 3553: Training loss: 1.2774088382720947\n",
      "Step 3554: Training loss: 1.1305912733078003\n",
      "Step 3555: Training loss: 1.39287269115448\n",
      "Step 3556: Training loss: 1.7118349075317383\n",
      "Step 3557: Training loss: 1.1052095890045166\n",
      "Step 3558: Training loss: 1.401598334312439\n",
      "Step 3559: Training loss: 1.132726788520813\n",
      "Step 3560: Training loss: 2.084322214126587\n",
      "Step 3561: Training loss: 1.514979362487793\n",
      "Step 3562: Training loss: 1.2298656702041626\n",
      "Step 3563: Training loss: 1.5038117170333862\n",
      "Step 3564: Training loss: 1.5636096000671387\n",
      "Step 3565: Training loss: 1.1037800312042236\n",
      "Step 3566: Training loss: 1.5487526655197144\n",
      "Step 3567: Training loss: 1.558456540107727\n",
      "Step 3568: Training loss: 1.1984000205993652\n",
      "Step 3569: Training loss: 1.0254079103469849\n",
      "Step 3570: Training loss: 1.2867035865783691\n",
      "Step 3571: Training loss: 1.3910470008850098\n",
      "Step 3572: Training loss: 1.462317705154419\n",
      "Step 3573: Training loss: 1.0279661417007446\n",
      "Step 3574: Training loss: 1.0821634531021118\n",
      "Step 3575: Training loss: 1.1451780796051025\n",
      "Step 3576: Training loss: 1.111050009727478\n",
      "Step 3577: Training loss: 1.2712604999542236\n",
      "Step 3578: Training loss: 1.3029533624649048\n",
      "Step 3579: Training loss: 1.1728650331497192\n",
      "Step 3580: Training loss: 1.3674558401107788\n",
      "Step 3581: Training loss: 1.327483057975769\n",
      "Step 3582: Training loss: 1.3816850185394287\n",
      "Step 3583: Training loss: 1.08858060836792\n",
      "Step 3584: Training loss: 1.0639113187789917\n",
      "Step 3585: Training loss: 1.0417009592056274\n",
      "Step 3586: Training loss: 1.2141587734222412\n",
      "Step 3587: Training loss: 1.237688660621643\n",
      "Step 3588: Training loss: 1.4744808673858643\n",
      "Step 3589: Training loss: 1.5196045637130737\n",
      "Step 3590: Training loss: 1.586876392364502\n",
      "Step 3591: Training loss: 1.306738018989563\n",
      "Step 3592: Training loss: 1.0212843418121338\n",
      "Step 3593: Training loss: 1.0492295026779175\n",
      "Step 3594: Training loss: 1.24931800365448\n",
      "Step 3595: Training loss: 1.2998018264770508\n",
      "Step 3596: Training loss: 1.3735285997390747\n",
      "Step 3597: Training loss: 0.9297664165496826\n",
      "Step 3598: Training loss: 1.22023344039917\n",
      "Step 3599: Training loss: 1.1088385581970215\n",
      "Step 3600: Training loss: 1.5444978475570679\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3600\n",
      "Step 3601: Training loss: 1.329009771347046\n",
      "Step 3602: Training loss: 1.5765210390090942\n",
      "Step 3603: Training loss: 1.4663175344467163\n",
      "Step 3604: Training loss: 1.2490414381027222\n",
      "Step 3605: Training loss: 1.1239917278289795\n",
      "Step 3606: Training loss: 1.2566487789154053\n",
      "Step 3607: Training loss: 1.28447687625885\n",
      "Step 3608: Training loss: 1.7195994853973389\n",
      "Step 3609: Training loss: 1.3076671361923218\n",
      "Step 3610: Training loss: 1.2505275011062622\n",
      "Step 3611: Training loss: 1.8024295568466187\n",
      "Step 3612: Training loss: 1.1399208307266235\n",
      "Step 3613: Training loss: 1.1510108709335327\n",
      "Step 3614: Training loss: 0.9979755878448486\n",
      "Step 3615: Training loss: 1.2308712005615234\n",
      "Step 3616: Training loss: 1.0948683023452759\n",
      "Step 3617: Training loss: 1.2345188856124878\n",
      "Step 3618: Training loss: 1.0415014028549194\n",
      "Step 3619: Training loss: 1.1381925344467163\n",
      "Step 3620: Training loss: 1.1484450101852417\n",
      "Step 3621: Training loss: 1.600188970565796\n",
      "Step 3622: Training loss: 1.1194809675216675\n",
      "Step 3623: Training loss: 1.1029335260391235\n",
      "Step 3624: Training loss: 1.3497204780578613\n",
      "Step 3625: Training loss: 1.2315573692321777\n",
      "Step 3626: Training loss: 1.0263539552688599\n",
      "Step 3627: Training loss: 1.273547887802124\n",
      "Step 3628: Training loss: 0.9130767583847046\n",
      "Step 3629: Training loss: 1.3659898042678833\n",
      "Step 3630: Training loss: 1.269378662109375\n",
      "Step 3631: Training loss: 1.46137273311615\n",
      "Step 3632: Training loss: 1.2798784971237183\n",
      "Step 3633: Training loss: 1.1131901741027832\n",
      "Step 3634: Training loss: 1.1124340295791626\n",
      "Step 3635: Training loss: 1.088049292564392\n",
      "Step 3636: Training loss: 1.2131166458129883\n",
      "Step 3637: Training loss: 1.3754292726516724\n",
      "Step 3638: Training loss: 1.6072702407836914\n",
      "Step 3639: Training loss: 1.2126158475875854\n",
      "Step 3640: Training loss: 1.2217915058135986\n",
      "Step 3641: Training loss: 1.3790479898452759\n",
      "Step 3642: Training loss: 1.2228381633758545\n",
      "Step 3643: Training loss: 1.1800270080566406\n",
      "Step 3644: Training loss: 1.3194911479949951\n",
      "Step 3645: Training loss: 1.3113516569137573\n",
      "Step 3646: Training loss: 1.585716724395752\n",
      "Step 3647: Training loss: 1.0579745769500732\n",
      "Step 3648: Training loss: 1.4623154401779175\n",
      "Step 3649: Training loss: 1.122260332107544\n",
      "Step 3650: Training loss: 1.1646188497543335\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3650\n",
      "Step 3651: Training loss: 1.157981038093567\n",
      "Step 3652: Training loss: 1.1374666690826416\n",
      "Step 3653: Training loss: 1.2878466844558716\n",
      "Step 3654: Training loss: 1.0982869863510132\n",
      "Step 3655: Training loss: 1.3360778093338013\n",
      "Step 3656: Training loss: 1.3911173343658447\n",
      "Step 3657: Training loss: 1.183408498764038\n",
      "Step 3658: Training loss: 1.2736353874206543\n",
      "Step 3659: Training loss: 1.4164481163024902\n",
      "Step 3660: Training loss: 1.294547438621521\n",
      "Step 3661: Training loss: 1.1198945045471191\n",
      "Step 3662: Training loss: 1.2413113117218018\n",
      "Step 3663: Training loss: 1.195507526397705\n",
      "Step 3664: Training loss: 1.0564625263214111\n",
      "Step 3665: Training loss: 0.9974600076675415\n",
      "Step 3666: Training loss: 1.0724601745605469\n",
      "Step 3667: Training loss: 1.3202755451202393\n",
      "Step 3668: Training loss: 1.1918258666992188\n",
      "Step 3669: Training loss: 1.3653688430786133\n",
      "Step 3670: Training loss: 1.0348373651504517\n",
      "Step 3671: Training loss: 1.034899353981018\n",
      "Step 3672: Training loss: 1.1522756814956665\n",
      "Step 3673: Training loss: 1.3859728574752808\n",
      "Step 3674: Training loss: 1.263256549835205\n",
      "Step 3675: Training loss: 1.1667182445526123\n",
      "Step 3676: Training loss: 1.349665641784668\n",
      "Step 3677: Training loss: 1.1506825685501099\n",
      "Step 3678: Training loss: 1.137184500694275\n",
      "Step 3679: Training loss: 1.357560157775879\n",
      "Step 3680: Training loss: 1.356783151626587\n",
      "Step 3681: Training loss: 1.2153561115264893\n",
      "Step 3682: Training loss: 1.192445158958435\n",
      "Step 3683: Training loss: 1.1071126461029053\n",
      "Step 3684: Training loss: 1.1196123361587524\n",
      "Step 3685: Training loss: 1.2194244861602783\n",
      "Step 3686: Training loss: 1.1352300643920898\n",
      "Step 3687: Training loss: 1.421859860420227\n",
      "Step 3688: Training loss: 1.2307785749435425\n",
      "Step 3689: Training loss: 1.1102908849716187\n",
      "Step 3690: Training loss: 1.1981533765792847\n",
      "Step 3691: Training loss: 1.2486178874969482\n",
      "Step 3692: Training loss: 1.286270260810852\n",
      "Step 3693: Training loss: 1.1533547639846802\n",
      "Step 3694: Training loss: 1.1928282976150513\n",
      "Step 3695: Training loss: 1.275522232055664\n",
      "Step 3696: Training loss: 1.384743332862854\n",
      "Step 3697: Training loss: 1.008669376373291\n",
      "Step 3698: Training loss: 1.005548119544983\n",
      "Step 3699: Training loss: 1.3937426805496216\n",
      "Step 3700: Training loss: 1.0335837602615356\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3700\n",
      "Step 3701: Training loss: 1.5416152477264404\n",
      "Step 3702: Training loss: 0.9689565300941467\n",
      "Step 3703: Training loss: 1.5332804918289185\n",
      "Step 3704: Training loss: 1.3163962364196777\n",
      "Step 3705: Training loss: 1.1388990879058838\n",
      "Step 3706: Training loss: 1.2459619045257568\n",
      "Step 3707: Training loss: 1.0906189680099487\n",
      "Step 3708: Training loss: 1.1126410961151123\n",
      "Step 3709: Training loss: 1.6324219703674316\n",
      "Step 3710: Training loss: 1.1514451503753662\n",
      "Step 3711: Training loss: 0.9930582642555237\n",
      "Step 3712: Training loss: 1.3281948566436768\n",
      "Step 3713: Training loss: 1.3028733730316162\n",
      "Step 3714: Training loss: 1.1761661767959595\n",
      "Step 3715: Training loss: 1.1598306894302368\n",
      "Step 3716: Training loss: 1.0998988151550293\n",
      "Step 3717: Training loss: 1.183775544166565\n",
      "Step 3718: Training loss: 1.0483863353729248\n",
      "Step 3719: Training loss: 1.2699580192565918\n",
      "Step 3720: Training loss: 1.3007440567016602\n",
      "Step 3721: Training loss: 1.0191398859024048\n",
      "Step 3722: Training loss: 1.09463369846344\n",
      "Step 3723: Training loss: 0.9997795224189758\n",
      "Step 3724: Training loss: 1.1096289157867432\n",
      "Step 3725: Training loss: 1.212839961051941\n",
      "Step 3726: Training loss: 1.214625358581543\n",
      "Step 3727: Training loss: 1.3241420984268188\n",
      "Step 3728: Training loss: 1.1612666845321655\n",
      "Step 3729: Training loss: 1.358468770980835\n",
      "Step 3730: Training loss: 1.681375503540039\n",
      "Step 3731: Training loss: 1.1850429773330688\n",
      "Step 3732: Training loss: 1.2103430032730103\n",
      "Step 3733: Training loss: 1.4947880506515503\n",
      "Step 3734: Training loss: 1.181045651435852\n",
      "Step 3735: Training loss: 1.22687828540802\n",
      "Step 3736: Training loss: 1.0914757251739502\n",
      "Step 3737: Training loss: 1.419053077697754\n",
      "Step 3738: Training loss: 1.2580723762512207\n",
      "Step 3739: Training loss: 1.0104881525039673\n",
      "Step 3740: Training loss: 1.2229993343353271\n",
      "Step 3741: Training loss: 1.07444167137146\n",
      "Step 3742: Training loss: 1.3490488529205322\n",
      "Step 3743: Training loss: 1.3696765899658203\n",
      "Step 3744: Training loss: 1.070989966392517\n",
      "Step 3745: Training loss: 0.9686205387115479\n",
      "Step 3746: Training loss: 1.1928011178970337\n",
      "Step 3747: Training loss: 1.1792035102844238\n",
      "Step 3748: Training loss: 1.1606186628341675\n",
      "Step 3749: Training loss: 1.0556881427764893\n",
      "Step 3750: Training loss: 1.37546706199646\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3750\n",
      "Step 3751: Training loss: 1.2814104557037354\n",
      "Step 3752: Training loss: 1.183118224143982\n",
      "Step 3753: Training loss: 1.4338558912277222\n",
      "Step 3754: Training loss: 1.6600983142852783\n",
      "Step 3755: Training loss: 0.9947277903556824\n",
      "Step 3756: Training loss: 1.3036537170410156\n",
      "Step 3757: Training loss: 1.1569390296936035\n",
      "Step 3758: Training loss: 1.2707442045211792\n",
      "Step 3759: Training loss: 1.3222695589065552\n",
      "Step 3760: Training loss: 1.0350927114486694\n",
      "Step 3761: Training loss: 1.0876448154449463\n",
      "Step 3762: Training loss: 1.4033385515213013\n",
      "Step 3763: Training loss: 1.286438226699829\n",
      "Step 3764: Training loss: 1.444490671157837\n",
      "Step 3765: Training loss: 1.3408513069152832\n",
      "Step 3766: Training loss: 1.074947714805603\n",
      "Step 3767: Training loss: 1.3437304496765137\n",
      "Step 3768: Training loss: 1.1983561515808105\n",
      "Step 3769: Training loss: 1.1778571605682373\n",
      "Step 3770: Training loss: 1.4833041429519653\n",
      "Step 3771: Training loss: 1.0058563947677612\n",
      "Step 3772: Training loss: 0.9549399018287659\n",
      "Step 3773: Training loss: 1.0911680459976196\n",
      "Step 3774: Training loss: 1.2100346088409424\n",
      "Step 3775: Training loss: 1.0717145204544067\n",
      "Step 3776: Training loss: 0.9215342402458191\n",
      "Step 3777: Training loss: 1.1683248281478882\n",
      "Step 3778: Training loss: 1.3611278533935547\n",
      "Step 3779: Training loss: 1.2308744192123413\n",
      "Step 3780: Training loss: 1.5324499607086182\n",
      "Step 3781: Training loss: 1.0217260122299194\n",
      "Step 3782: Training loss: 1.4805617332458496\n",
      "Step 3783: Training loss: 1.4728975296020508\n",
      "Step 3784: Training loss: 1.1926140785217285\n",
      "Step 3785: Training loss: 1.283110499382019\n",
      "Step 3786: Training loss: 1.2775665521621704\n",
      "Step 3787: Training loss: 1.5966471433639526\n",
      "Step 3788: Training loss: 1.2009211778640747\n",
      "Step 3789: Training loss: 1.0802658796310425\n",
      "Step 3790: Training loss: 1.1949886083602905\n",
      "Step 3791: Training loss: 1.0496079921722412\n",
      "Step 3792: Training loss: 1.3019111156463623\n",
      "Step 3793: Training loss: 1.3304318189620972\n",
      "Step 3794: Training loss: 1.028964877128601\n",
      "Step 3795: Training loss: 1.02754807472229\n",
      "Step 3796: Training loss: 1.227880835533142\n",
      "Step 3797: Training loss: 1.2372865676879883\n",
      "Step 3798: Training loss: 1.1410603523254395\n",
      "Step 3799: Training loss: 1.1425461769104004\n",
      "Step 3800: Training loss: 0.9464335441589355\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3800\n",
      "Step 3801: Training loss: 1.0646847486495972\n",
      "Step 3802: Training loss: 1.1787575483322144\n",
      "Step 3803: Training loss: 1.0922880172729492\n",
      "Step 3804: Training loss: 1.0568327903747559\n",
      "Step 3805: Training loss: 1.2847959995269775\n",
      "Step 3806: Training loss: 1.3194594383239746\n",
      "Step 3807: Training loss: 1.0632635354995728\n",
      "Step 3808: Training loss: 1.289302945137024\n",
      "Step 3809: Training loss: 1.4409618377685547\n",
      "Step 3810: Training loss: 1.354058861732483\n",
      "Step 3811: Training loss: 1.1964144706726074\n",
      "Step 3812: Training loss: 1.2203481197357178\n",
      "Step 3813: Training loss: 1.2294155359268188\n",
      "Step 3814: Training loss: 1.1541413068771362\n",
      "Step 3815: Training loss: 1.0243195295333862\n",
      "Step 3816: Training loss: 1.0878980159759521\n",
      "Step 3817: Training loss: 1.4212079048156738\n",
      "Step 3818: Training loss: 1.1690706014633179\n",
      "Step 3819: Training loss: 1.0647846460342407\n",
      "Step 3820: Training loss: 1.2400147914886475\n",
      "Step 3821: Training loss: 1.1415014266967773\n",
      "Step 3822: Training loss: 1.1490789651870728\n",
      "Step 3823: Training loss: 1.1198421716690063\n",
      "Step 3824: Training loss: 1.351913571357727\n",
      "Step 3825: Training loss: 1.213718295097351\n",
      "Step 3826: Training loss: 1.1656533479690552\n",
      "Step 3827: Training loss: 1.1438210010528564\n",
      "Step 3828: Training loss: 1.0262739658355713\n",
      "Step 3829: Training loss: 1.079967975616455\n",
      "Step 3830: Training loss: 1.1114869117736816\n",
      "Step 3831: Training loss: 1.1796292066574097\n",
      "Step 3832: Training loss: 1.02280592918396\n",
      "Step 3833: Training loss: 1.1310652494430542\n",
      "Step 3834: Training loss: 0.9806028008460999\n",
      "Step 3835: Training loss: 1.3168694972991943\n",
      "Step 3836: Training loss: 1.1572232246398926\n",
      "Step 3837: Training loss: 1.3271058797836304\n",
      "Step 3838: Training loss: 1.073490023612976\n",
      "Step 3839: Training loss: 1.0408746004104614\n",
      "Step 3840: Training loss: 1.275556206703186\n",
      "Step 3841: Training loss: 1.1507657766342163\n",
      "Step 3842: Training loss: 1.0763442516326904\n",
      "Step 3843: Training loss: 1.0814061164855957\n",
      "Step 3844: Training loss: 1.1513603925704956\n",
      "Step 3845: Training loss: 1.1709753274917603\n",
      "Step 3846: Training loss: 1.2841418981552124\n",
      "Step 3847: Training loss: 1.162988543510437\n",
      "Step 3848: Training loss: 0.9773968458175659\n",
      "Step 3849: Training loss: 1.516601800918579\n",
      "Step 3850: Training loss: 0.9687370657920837\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3850\n",
      "Step 3851: Training loss: 1.3829290866851807\n",
      "Step 3852: Training loss: 1.0479514598846436\n",
      "Step 3853: Training loss: 1.1606247425079346\n",
      "Step 3854: Training loss: 0.9400869607925415\n",
      "Step 3855: Training loss: 1.1550908088684082\n",
      "Step 3856: Training loss: 1.183671474456787\n",
      "Step 3857: Training loss: 1.188941240310669\n",
      "Step 3858: Training loss: 1.0076183080673218\n",
      "Step 3859: Training loss: 0.8918485045433044\n",
      "Step 3860: Training loss: 1.2882214784622192\n",
      "Step 3861: Training loss: 1.1645640134811401\n",
      "Step 3862: Training loss: 1.1043802499771118\n",
      "Step 3863: Training loss: 1.0870054960250854\n",
      "Step 3864: Training loss: 1.1129049062728882\n",
      "Step 3865: Training loss: 1.1976425647735596\n",
      "Step 3866: Training loss: 1.2620739936828613\n",
      "Step 3867: Training loss: 1.032448649406433\n",
      "Step 3868: Training loss: 1.0911797285079956\n",
      "Step 3869: Training loss: 1.3035647869110107\n",
      "Step 3870: Training loss: 0.9776397943496704\n",
      "Step 3871: Training loss: 1.0013682842254639\n",
      "Step 3872: Training loss: 1.1394035816192627\n",
      "Step 3873: Training loss: 1.0711239576339722\n",
      "Step 3874: Training loss: 1.0781581401824951\n",
      "Step 3875: Training loss: 1.2253831624984741\n",
      "Step 3876: Training loss: 1.1932677030563354\n",
      "Step 3877: Training loss: 1.3558123111724854\n",
      "Step 3878: Training loss: 1.1100798845291138\n",
      "Step 3879: Training loss: 1.1813418865203857\n",
      "Step 3880: Training loss: 1.1581543684005737\n",
      "Step 3881: Training loss: 1.134468674659729\n",
      "Step 3882: Training loss: 1.160019874572754\n",
      "Step 3883: Training loss: 1.2514393329620361\n",
      "Step 3884: Training loss: 1.1693775653839111\n",
      "Step 3885: Training loss: 1.0782644748687744\n",
      "Step 3886: Training loss: 1.1742218732833862\n",
      "Step 3887: Training loss: 1.0458743572235107\n",
      "Step 3888: Training loss: 0.9502086639404297\n",
      "Step 3889: Training loss: 0.9013447165489197\n",
      "Step 3890: Training loss: 1.0926764011383057\n",
      "Step 3891: Training loss: 1.6067399978637695\n",
      "Step 3892: Training loss: 0.9822854399681091\n",
      "Step 3893: Training loss: 1.1146541833877563\n",
      "Step 3894: Training loss: 1.2604174613952637\n",
      "Step 3895: Training loss: 1.2277956008911133\n",
      "Step 3896: Training loss: 1.103164553642273\n",
      "Step 3897: Training loss: 1.036008358001709\n",
      "Step 3898: Training loss: 1.062995195388794\n",
      "Step 3899: Training loss: 1.0762094259262085\n",
      "Step 3900: Training loss: 1.1571996212005615\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3900\n",
      "Step 3901: Training loss: 1.0706695318222046\n",
      "Step 3902: Training loss: 0.9749569296836853\n",
      "Step 3903: Training loss: 1.133941650390625\n",
      "Step 3904: Training loss: 1.0582319498062134\n",
      "Step 3905: Training loss: 0.9480434656143188\n",
      "Step 3906: Training loss: 1.4128930568695068\n",
      "Step 3907: Training loss: 0.9989767670631409\n",
      "Step 3908: Training loss: 1.3536568880081177\n",
      "Step 3909: Training loss: 1.1056561470031738\n",
      "Step 3910: Training loss: 1.0337574481964111\n",
      "Step 3911: Training loss: 0.9943370819091797\n",
      "Step 3912: Training loss: 1.0111945867538452\n",
      "Step 3913: Training loss: 1.090917706489563\n",
      "Step 3914: Training loss: 1.5279364585876465\n",
      "Step 3915: Training loss: 1.2630010843276978\n",
      "Step 3916: Training loss: 1.1150168180465698\n",
      "Step 3917: Training loss: 1.1367933750152588\n",
      "Step 3918: Training loss: 1.2341705560684204\n",
      "Step 3919: Training loss: 0.9727960228919983\n",
      "Step 3920: Training loss: 1.0962433815002441\n",
      "Step 3921: Training loss: 1.0927826166152954\n",
      "Step 3922: Training loss: 1.082699179649353\n",
      "Step 3923: Training loss: 1.1050574779510498\n",
      "Step 3924: Training loss: 1.362916350364685\n",
      "Step 3925: Training loss: 0.9871180653572083\n",
      "Step 3926: Training loss: 0.9798659086227417\n",
      "Step 3927: Training loss: 1.226966142654419\n",
      "Step 3928: Training loss: 1.155034065246582\n",
      "Step 3929: Training loss: 0.9536662101745605\n",
      "Step 3930: Training loss: 1.1280524730682373\n",
      "Step 3931: Training loss: 1.1443989276885986\n",
      "Step 3932: Training loss: 0.9790328145027161\n",
      "Step 3933: Training loss: 1.2290291786193848\n",
      "Step 3934: Training loss: 1.116241455078125\n",
      "Step 3935: Training loss: 0.8733453750610352\n",
      "Step 3936: Training loss: 1.2534230947494507\n",
      "Step 3937: Training loss: 1.0153508186340332\n",
      "Step 3938: Training loss: 1.1577945947647095\n",
      "Step 3939: Training loss: 0.9575629234313965\n",
      "Step 3940: Training loss: 0.9551047682762146\n",
      "Step 3941: Training loss: 0.9215395450592041\n",
      "Step 3942: Training loss: 0.8988775014877319\n",
      "Step 3943: Training loss: 1.0668967962265015\n",
      "Step 3944: Training loss: 1.1933460235595703\n",
      "Step 3945: Training loss: 1.0267118215560913\n",
      "Step 3946: Training loss: 0.9782658815383911\n",
      "Step 3947: Training loss: 1.034741759300232\n",
      "Step 3948: Training loss: 1.3622632026672363\n",
      "Step 3949: Training loss: 1.1849613189697266\n",
      "Step 3950: Training loss: 0.9417916536331177\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-3950\n",
      "Step 3951: Training loss: 1.0552197694778442\n",
      "Step 3952: Training loss: 1.1847208738327026\n",
      "Step 3953: Training loss: 0.9218155741691589\n",
      "Step 3954: Training loss: 1.0059270858764648\n",
      "Step 3955: Training loss: 1.006961464881897\n",
      "Step 3956: Training loss: 0.8690728545188904\n",
      "Step 3957: Training loss: 1.0653657913208008\n",
      "Step 3958: Training loss: 0.9981974959373474\n",
      "Step 3959: Training loss: 1.09032142162323\n",
      "Step 3960: Training loss: 1.044922947883606\n",
      "Step 3961: Training loss: 1.0618901252746582\n",
      "Step 3962: Training loss: 0.9906736016273499\n",
      "Step 3963: Training loss: 1.013062834739685\n",
      "Step 3964: Training loss: 1.132768988609314\n",
      "Step 3965: Training loss: 1.0286602973937988\n",
      "Step 3966: Training loss: 1.0589988231658936\n",
      "Step 3967: Training loss: 1.0307351350784302\n",
      "Step 3968: Training loss: 1.071389079093933\n",
      "Step 3969: Training loss: 1.0457310676574707\n",
      "Step 3970: Training loss: 1.015411138534546\n",
      "Step 3971: Training loss: 1.1548882722854614\n",
      "Step 3972: Training loss: 1.048335313796997\n",
      "Step 3973: Training loss: 1.3771793842315674\n",
      "Step 3974: Training loss: 1.0763260126113892\n",
      "Step 3975: Training loss: 0.9729101657867432\n",
      "Step 3976: Training loss: 1.0051978826522827\n",
      "Step 3977: Training loss: 1.0021144151687622\n",
      "Step 3978: Training loss: 1.114668369293213\n",
      "Step 3979: Training loss: 1.0249470472335815\n",
      "Step 3980: Training loss: 1.3657526969909668\n",
      "Step 3981: Training loss: 1.1495037078857422\n",
      "Step 3982: Training loss: 1.0472911596298218\n",
      "Step 3983: Training loss: 1.206093668937683\n",
      "Step 3984: Training loss: 0.9506427049636841\n",
      "Step 3985: Training loss: 1.0430006980895996\n",
      "Step 3986: Training loss: 1.1474881172180176\n",
      "Step 3987: Training loss: 0.9906371831893921\n",
      "Step 3988: Training loss: 1.0568288564682007\n",
      "Step 3989: Training loss: 1.0939921140670776\n",
      "Step 3990: Training loss: 0.9636646509170532\n",
      "Step 3991: Training loss: 1.275200366973877\n",
      "Step 3992: Training loss: 1.0900591611862183\n",
      "Step 3993: Training loss: 0.9839791655540466\n",
      "Step 3994: Training loss: 0.9567795991897583\n",
      "Step 3995: Training loss: 0.954123318195343\n",
      "Step 3996: Training loss: 1.0655344724655151\n",
      "Step 3997: Training loss: 1.0145702362060547\n",
      "Step 3998: Training loss: 1.0361242294311523\n",
      "Step 3999: Training loss: 1.0101054906845093\n",
      "Step 4000: Training loss: 1.3019369840621948\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4000\n",
      "Step 4001: Training loss: 0.9463081359863281\n",
      "Step 4002: Training loss: 0.9346622228622437\n",
      "Step 4003: Training loss: 1.096091628074646\n",
      "Step 4004: Training loss: 1.1400033235549927\n",
      "Step 4005: Training loss: 0.910788893699646\n",
      "Step 4006: Training loss: 1.0049418210983276\n",
      "Step 4007: Training loss: 1.6352468729019165\n",
      "Step 4008: Training loss: 1.1116570234298706\n",
      "Step 4009: Training loss: 0.8881561756134033\n",
      "Step 4010: Training loss: 1.0672613382339478\n",
      "Step 4011: Training loss: 1.0745608806610107\n",
      "Step 4012: Training loss: 1.0055674314498901\n",
      "Step 4013: Training loss: 1.038962960243225\n",
      "Step 4014: Training loss: 1.0728795528411865\n",
      "Step 4015: Training loss: 1.0637191534042358\n",
      "Step 4016: Training loss: 0.9264916181564331\n",
      "Step 4017: Training loss: 1.0248386859893799\n",
      "Step 4018: Training loss: 0.9183021783828735\n",
      "Step 4019: Training loss: 0.9384673833847046\n",
      "Step 4020: Training loss: 1.0829191207885742\n",
      "Step 4021: Training loss: 1.0170800685882568\n",
      "Step 4022: Training loss: 1.0426361560821533\n",
      "Step 4023: Training loss: 1.0399315357208252\n",
      "Step 4024: Training loss: 0.9853047132492065\n",
      "Step 4025: Training loss: 0.9113507866859436\n",
      "Step 4026: Training loss: 0.9446201920509338\n",
      "Step 4027: Training loss: 0.9906493425369263\n",
      "Step 4028: Training loss: 1.1037492752075195\n",
      "Step 4029: Training loss: 1.0507526397705078\n",
      "Step 4030: Training loss: 1.0034773349761963\n",
      "Step 4031: Training loss: 0.9713976979255676\n",
      "Step 4032: Training loss: 0.9747475385665894\n",
      "Step 4033: Training loss: 1.002982258796692\n",
      "Step 4034: Training loss: 1.0557811260223389\n",
      "Step 4035: Training loss: 1.0008131265640259\n",
      "Step 4036: Training loss: 0.9946284294128418\n",
      "Step 4037: Training loss: 0.9252749681472778\n",
      "Step 4038: Training loss: 0.9918667078018188\n",
      "Step 4039: Training loss: 1.0185741186141968\n",
      "Step 4040: Training loss: 1.1062086820602417\n",
      "Step 4041: Training loss: 1.0137662887573242\n",
      "Step 4042: Training loss: 0.9280180931091309\n",
      "Step 4043: Training loss: 1.1866984367370605\n",
      "Step 4044: Training loss: 0.9998574256896973\n",
      "Step 4045: Training loss: 0.9906458854675293\n",
      "Step 4046: Training loss: 1.0141010284423828\n",
      "Step 4047: Training loss: 1.0931042432785034\n",
      "Step 4048: Training loss: 0.9365532398223877\n",
      "Step 4049: Training loss: 1.054571270942688\n",
      "Step 4050: Training loss: 1.003740906715393\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4050\n",
      "Step 4051: Training loss: 1.006823182106018\n",
      "Step 4052: Training loss: 0.9984163641929626\n",
      "Step 4053: Training loss: 0.9322313666343689\n",
      "Step 4054: Training loss: 1.065119743347168\n",
      "Step 4055: Training loss: 0.8939250111579895\n",
      "Step 4056: Training loss: 0.9805837869644165\n",
      "Step 4057: Training loss: 0.9443480968475342\n",
      "Step 4058: Training loss: 1.0535894632339478\n",
      "Step 4059: Training loss: 1.1857167482376099\n",
      "Step 4060: Training loss: 0.8519349694252014\n",
      "Step 4061: Training loss: 0.8774088621139526\n",
      "Step 4062: Training loss: 1.5694379806518555\n",
      "Step 4063: Training loss: 0.944291353225708\n",
      "Step 4064: Training loss: 0.9502997994422913\n",
      "Step 4065: Training loss: 1.0026291608810425\n",
      "Step 4066: Training loss: 1.0921504497528076\n",
      "Step 4067: Training loss: 0.9868839383125305\n",
      "Step 4068: Training loss: 1.1016857624053955\n",
      "Step 4069: Training loss: 0.9301549792289734\n",
      "Step 4070: Training loss: 0.9547668099403381\n",
      "Step 4071: Training loss: 1.0095630884170532\n",
      "Step 4072: Training loss: 0.9119482040405273\n",
      "Step 4073: Training loss: 0.9570013284683228\n",
      "Step 4074: Training loss: 0.9548402428627014\n",
      "Step 4075: Training loss: 1.0584650039672852\n",
      "Step 4076: Training loss: 1.000309705734253\n",
      "Step 4077: Training loss: 1.0228331089019775\n",
      "Step 4078: Training loss: 0.9733491539955139\n",
      "Step 4079: Training loss: 0.9089930653572083\n",
      "Step 4080: Training loss: 1.0352027416229248\n",
      "Step 4081: Training loss: 0.911921501159668\n",
      "Step 4082: Training loss: 0.9280144572257996\n",
      "Step 4083: Training loss: 1.096380591392517\n",
      "Step 4084: Training loss: 1.0076000690460205\n",
      "Step 4085: Training loss: 1.0756813287734985\n",
      "Step 4086: Training loss: 1.0721856355667114\n",
      "Step 4087: Training loss: 0.9587191343307495\n",
      "Step 4088: Training loss: 1.0541300773620605\n",
      "Step 4089: Training loss: 0.8618519902229309\n",
      "Step 4090: Training loss: 0.9826453328132629\n",
      "Step 4091: Training loss: 0.9885348677635193\n",
      "Step 4092: Training loss: 0.9251417517662048\n",
      "Step 4093: Training loss: 1.0429288148880005\n",
      "Step 4094: Training loss: 1.1791163682937622\n",
      "Step 4095: Training loss: 0.9166243672370911\n",
      "Step 4096: Training loss: 0.9988200068473816\n",
      "Step 4097: Training loss: 1.1392189264297485\n",
      "Step 4098: Training loss: 0.9971756935119629\n",
      "Step 4099: Training loss: 0.9942337274551392\n",
      "Step 4100: Training loss: 0.9876132607460022\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4100\n",
      "Step 4101: Training loss: 0.9496051073074341\n",
      "Step 4102: Training loss: 0.9848567247390747\n",
      "Step 4103: Training loss: 0.8268787264823914\n",
      "Step 4104: Training loss: 0.8765988349914551\n",
      "Step 4105: Training loss: 1.1347317695617676\n",
      "Step 4106: Training loss: 0.963072657585144\n",
      "Step 4107: Training loss: 0.9568745493888855\n",
      "Step 4108: Training loss: 1.015317440032959\n",
      "Step 4109: Training loss: 0.9068930745124817\n",
      "Step 4110: Training loss: 0.9439578056335449\n",
      "Step 4111: Training loss: 0.9716807007789612\n",
      "Step 4112: Training loss: 0.9688482284545898\n",
      "Step 4113: Training loss: 1.0369560718536377\n",
      "Step 4114: Training loss: 0.8935551643371582\n",
      "Step 4115: Training loss: 1.0082827806472778\n",
      "Step 4116: Training loss: 0.9400646686553955\n",
      "Step 4117: Training loss: 0.9758428931236267\n",
      "Step 4118: Training loss: 1.1548421382904053\n",
      "Step 4119: Training loss: 0.9161307215690613\n",
      "Step 4120: Training loss: 1.020134687423706\n",
      "Step 4121: Training loss: 0.9899381399154663\n",
      "Step 4122: Training loss: 0.9821929931640625\n",
      "Step 4123: Training loss: 1.1152307987213135\n",
      "Step 4124: Training loss: 1.1316238641738892\n",
      "Step 4125: Training loss: 0.9247729182243347\n",
      "Step 4126: Training loss: 0.9400604367256165\n",
      "Step 4127: Training loss: 0.9302387833595276\n",
      "Step 4128: Training loss: 0.9576548337936401\n",
      "Step 4129: Training loss: 0.9838109612464905\n",
      "Step 4130: Training loss: 0.9811219573020935\n",
      "Step 4131: Training loss: 1.025625467300415\n",
      "Step 4132: Training loss: 1.1003986597061157\n",
      "Step 4133: Training loss: 0.9797109961509705\n",
      "Step 4134: Training loss: 1.0292888879776\n",
      "Step 4135: Training loss: 1.04511559009552\n",
      "Step 4136: Training loss: 0.9991132616996765\n",
      "Step 4137: Training loss: 0.9870254993438721\n",
      "Step 4138: Training loss: 0.8599128127098083\n",
      "Step 4139: Training loss: 0.9727836847305298\n",
      "Step 4140: Training loss: 0.8685674071311951\n",
      "Step 4141: Training loss: 0.9410878419876099\n",
      "Step 4142: Training loss: 0.8998796343803406\n",
      "Step 4143: Training loss: 0.976325273513794\n",
      "Step 4144: Training loss: 0.9230678677558899\n",
      "Step 4145: Training loss: 0.8747507333755493\n",
      "Step 4146: Training loss: 0.9547230005264282\n",
      "Step 4147: Training loss: 0.9149656891822815\n",
      "Step 4148: Training loss: 0.9639108777046204\n",
      "Step 4149: Training loss: 0.9742149710655212\n",
      "Step 4150: Training loss: 1.0449895858764648\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4150\n",
      "Step 4151: Training loss: 0.9187787771224976\n",
      "Step 4152: Training loss: 0.9516239762306213\n",
      "Step 4153: Training loss: 1.042029619216919\n",
      "Step 4154: Training loss: 0.9515411257743835\n",
      "Step 4155: Training loss: 0.9957418441772461\n",
      "Step 4156: Training loss: 0.9241148233413696\n",
      "Step 4157: Training loss: 0.9083953499794006\n",
      "Step 4158: Training loss: 0.8666044473648071\n",
      "Step 4159: Training loss: 1.0092148780822754\n",
      "Step 4160: Training loss: 0.935211718082428\n",
      "Step 4161: Training loss: 0.8592997193336487\n",
      "Step 4162: Training loss: 0.9826259613037109\n",
      "Step 4163: Training loss: 0.9686599373817444\n",
      "Step 4164: Training loss: 0.8678551912307739\n",
      "Step 4165: Training loss: 1.1626613140106201\n",
      "Step 4166: Training loss: 0.9518684148788452\n",
      "Step 4167: Training loss: 0.9141352772712708\n",
      "Step 4168: Training loss: 0.9349161386489868\n",
      "Step 4169: Training loss: 0.9103143215179443\n",
      "Step 4170: Training loss: 0.937579333782196\n",
      "Step 4171: Training loss: 0.9407727718353271\n",
      "Step 4172: Training loss: 0.8959868550300598\n",
      "Step 4173: Training loss: 0.8981153964996338\n",
      "Step 4174: Training loss: 1.0039740800857544\n",
      "Step 4175: Training loss: 0.9526538848876953\n",
      "Step 4176: Training loss: 0.9325054287910461\n",
      "Step 4177: Training loss: 1.0365811586380005\n",
      "Step 4178: Training loss: 0.881838858127594\n",
      "Step 4179: Training loss: 1.087990164756775\n",
      "Step 4180: Training loss: 0.8813747763633728\n",
      "Step 4181: Training loss: 0.8870901465415955\n",
      "Step 4182: Training loss: 0.870258092880249\n",
      "Step 4183: Training loss: 1.3126646280288696\n",
      "Step 4184: Training loss: 0.9266089797019958\n",
      "Step 4185: Training loss: 0.9282234907150269\n",
      "Step 4186: Training loss: 0.8845584988594055\n",
      "Step 4187: Training loss: 0.9442803859710693\n",
      "Step 4188: Training loss: 0.9152567982673645\n",
      "Step 4189: Training loss: 0.8765782713890076\n",
      "Step 4190: Training loss: 0.9365001320838928\n",
      "Step 4191: Training loss: 0.8132418394088745\n",
      "Step 4192: Training loss: 0.9530526995658875\n",
      "Step 4193: Training loss: 0.9221080541610718\n",
      "Step 4194: Training loss: 0.9011011123657227\n",
      "Step 4195: Training loss: 0.9505110383033752\n",
      "Step 4196: Training loss: 1.0658422708511353\n",
      "Step 4197: Training loss: 0.856368899345398\n",
      "Step 4198: Training loss: 0.9480857849121094\n",
      "Step 4199: Training loss: 0.9691041707992554\n",
      "Step 4200: Training loss: 0.8840848803520203\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4200\n",
      "Step 4201: Training loss: 0.9208734631538391\n",
      "Step 4202: Training loss: 0.7986015677452087\n",
      "Step 4203: Training loss: 0.918470025062561\n",
      "Step 4204: Training loss: 0.88584965467453\n",
      "Step 4205: Training loss: 0.9202793836593628\n",
      "Step 4206: Training loss: 0.9251733422279358\n",
      "Step 4207: Training loss: 0.9806334972381592\n",
      "Step 4208: Training loss: 0.8462827801704407\n",
      "Step 4209: Training loss: 0.8381465673446655\n",
      "Step 4210: Training loss: 0.8138318061828613\n",
      "Step 4211: Training loss: 0.8975921869277954\n",
      "Step 4212: Training loss: 0.8531245589256287\n",
      "Step 4213: Training loss: 0.9475409984588623\n",
      "Step 4214: Training loss: 0.9994692802429199\n",
      "Step 4215: Training loss: 0.956801176071167\n",
      "Step 4216: Training loss: 0.9645062685012817\n",
      "Step 4217: Training loss: 0.9313451051712036\n",
      "Step 4218: Training loss: 0.9862721562385559\n",
      "Step 4219: Training loss: 0.8709169626235962\n",
      "Step 4220: Training loss: 1.0770865678787231\n",
      "Step 4221: Training loss: 0.973404586315155\n",
      "Step 4222: Training loss: 1.0482057332992554\n",
      "Step 4223: Training loss: 0.9012702107429504\n",
      "Step 4224: Training loss: 0.961165726184845\n",
      "Step 4225: Training loss: 1.0522494316101074\n",
      "Step 4226: Training loss: 0.9279221296310425\n",
      "Step 4227: Training loss: 0.9488319754600525\n",
      "Step 4228: Training loss: 0.8492894768714905\n",
      "Step 4229: Training loss: 1.0404967069625854\n",
      "Step 4230: Training loss: 1.0248851776123047\n",
      "Step 4231: Training loss: 1.136649489402771\n",
      "Step 4232: Training loss: 0.8166325688362122\n",
      "Step 4233: Training loss: 0.8805435299873352\n",
      "Step 4234: Training loss: 1.034759759902954\n",
      "Step 4235: Training loss: 0.954747200012207\n",
      "Step 4236: Training loss: 1.1831169128417969\n",
      "Step 4237: Training loss: 0.9090980887413025\n",
      "Step 4238: Training loss: 0.8239393830299377\n",
      "Step 4239: Training loss: 0.9908515810966492\n",
      "Step 4240: Training loss: 0.8781062364578247\n",
      "Step 4241: Training loss: 0.9366563558578491\n",
      "Step 4242: Training loss: 0.919505774974823\n",
      "Step 4243: Training loss: 0.8265203237533569\n",
      "Step 4244: Training loss: 0.8309161067008972\n",
      "Step 4245: Training loss: 0.8445644378662109\n",
      "Step 4246: Training loss: 0.9924004673957825\n",
      "Step 4247: Training loss: 1.0151007175445557\n",
      "Step 4248: Training loss: 0.8619980812072754\n",
      "Step 4249: Training loss: 1.0244150161743164\n",
      "Step 4250: Training loss: 0.9063801765441895\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4250\n",
      "Step 4251: Training loss: 1.1886780261993408\n",
      "Step 4252: Training loss: 0.8835452795028687\n",
      "Step 4253: Training loss: 1.0094072818756104\n",
      "Step 4254: Training loss: 0.8522731065750122\n",
      "Step 4255: Training loss: 0.8186141848564148\n",
      "Step 4256: Training loss: 0.9039507508277893\n",
      "Step 4257: Training loss: 1.1010165214538574\n",
      "Step 4258: Training loss: 0.998950719833374\n",
      "Step 4259: Training loss: 0.9414079189300537\n",
      "Step 4260: Training loss: 0.7830266356468201\n",
      "Step 4261: Training loss: 0.9718842506408691\n",
      "Step 4262: Training loss: 0.8990322351455688\n",
      "Step 4263: Training loss: 0.9650955200195312\n",
      "Step 4264: Training loss: 1.4361227750778198\n",
      "Step 4265: Training loss: 0.8136076927185059\n",
      "Step 4266: Training loss: 0.8367076516151428\n",
      "Step 4267: Training loss: 0.9340759515762329\n",
      "Step 4268: Training loss: 0.9703691601753235\n",
      "Step 4269: Training loss: 0.8914455771446228\n",
      "Step 4270: Training loss: 1.1125568151474\n",
      "Step 4271: Training loss: 0.9426480531692505\n",
      "Step 4272: Training loss: 0.8362303972244263\n",
      "Step 4273: Training loss: 0.7800506949424744\n",
      "Step 4274: Training loss: 0.8441178798675537\n",
      "Step 4275: Training loss: 1.1934890747070312\n",
      "Step 4276: Training loss: 0.925654411315918\n",
      "Step 4277: Training loss: 0.834212601184845\n",
      "Step 4278: Training loss: 0.8363779187202454\n",
      "Step 4279: Training loss: 0.8625450730323792\n",
      "Step 4280: Training loss: 0.8744565844535828\n",
      "Step 4281: Training loss: 0.8844699263572693\n",
      "Step 4282: Training loss: 0.8927130103111267\n",
      "Step 4283: Training loss: 0.9867438673973083\n",
      "Step 4284: Training loss: 1.0141246318817139\n",
      "Step 4285: Training loss: 0.9375518560409546\n",
      "Step 4286: Training loss: 0.8813884258270264\n",
      "Step 4287: Training loss: 0.8998543620109558\n",
      "Step 4288: Training loss: 0.9173575639724731\n",
      "Step 4289: Training loss: 0.9312477111816406\n",
      "Step 4290: Training loss: 0.9685237407684326\n",
      "Step 4291: Training loss: 0.9186282753944397\n",
      "Step 4292: Training loss: 0.9382428526878357\n",
      "Step 4293: Training loss: 0.8206024765968323\n",
      "Step 4294: Training loss: 0.7541103363037109\n",
      "Step 4295: Training loss: 0.8155685663223267\n",
      "Step 4296: Training loss: 1.0732976198196411\n",
      "Step 4297: Training loss: 0.8614813685417175\n",
      "Step 4298: Training loss: 0.9041129946708679\n",
      "Step 4299: Training loss: 1.0015294551849365\n",
      "Step 4300: Training loss: 0.8923594951629639\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4300\n",
      "Step 4301: Training loss: 0.8473436832427979\n",
      "Step 4302: Training loss: 0.8022321462631226\n",
      "Step 4303: Training loss: 1.0250020027160645\n",
      "Step 4304: Training loss: 0.7943630814552307\n",
      "Step 4305: Training loss: 0.8643366694450378\n",
      "Step 4306: Training loss: 0.8382833003997803\n",
      "Step 4307: Training loss: 0.796352744102478\n",
      "Step 4308: Training loss: 0.9317609667778015\n",
      "Step 4309: Training loss: 0.9247969388961792\n",
      "Step 4310: Training loss: 0.905286967754364\n",
      "Step 4311: Training loss: 0.8548632860183716\n",
      "Step 4312: Training loss: 0.9616594314575195\n",
      "Step 4313: Training loss: 0.9768127202987671\n",
      "Step 4314: Training loss: 0.8938296437263489\n",
      "Step 4315: Training loss: 0.8684691786766052\n",
      "Step 4316: Training loss: 0.9076347947120667\n",
      "Step 4317: Training loss: 0.8980197906494141\n",
      "Step 4318: Training loss: 0.7492918968200684\n",
      "Step 4319: Training loss: 1.0888859033584595\n",
      "Step 4320: Training loss: 1.1710768938064575\n",
      "Step 4321: Training loss: 0.902493417263031\n",
      "Step 4322: Training loss: 0.9259412288665771\n",
      "Step 4323: Training loss: 0.8565599918365479\n",
      "Step 4324: Training loss: 1.031597375869751\n",
      "Step 4325: Training loss: 0.8617509007453918\n",
      "Step 4326: Training loss: 0.8199227452278137\n",
      "Step 4327: Training loss: 0.9215151071548462\n",
      "Step 4328: Training loss: 0.9877578020095825\n",
      "Step 4329: Training loss: 0.9974011778831482\n",
      "Step 4330: Training loss: 1.051256775856018\n",
      "Step 4331: Training loss: 0.8448309898376465\n",
      "Step 4332: Training loss: 0.8230841159820557\n",
      "Step 4333: Training loss: 0.7998414635658264\n",
      "Step 4334: Training loss: 0.9157956838607788\n",
      "Step 4335: Training loss: 0.9281489849090576\n",
      "Step 4336: Training loss: 0.8457268476486206\n",
      "Step 4337: Training loss: 0.8926399350166321\n",
      "Step 4338: Training loss: 0.8623570799827576\n",
      "Step 4339: Training loss: 0.8192927241325378\n",
      "Step 4340: Training loss: 0.8775423765182495\n",
      "Step 4341: Training loss: 0.8629714250564575\n",
      "Step 4342: Training loss: 0.8149397969245911\n",
      "Step 4343: Training loss: 0.8409880995750427\n",
      "Step 4344: Training loss: 0.9037121534347534\n",
      "Step 4345: Training loss: 0.9812037944793701\n",
      "Step 4346: Training loss: 0.7875162363052368\n",
      "Step 4347: Training loss: 0.9047532081604004\n",
      "Step 4348: Training loss: 0.9350016117095947\n",
      "Step 4349: Training loss: 0.802541971206665\n",
      "Step 4350: Training loss: 0.8200135231018066\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4350\n",
      "Step 4351: Training loss: 0.9184017777442932\n",
      "Step 4352: Training loss: 0.9327131509780884\n",
      "Step 4353: Training loss: 0.8706807494163513\n",
      "Step 4354: Training loss: 0.917267918586731\n",
      "Step 4355: Training loss: 0.8153121471405029\n",
      "Step 4356: Training loss: 0.7551934123039246\n",
      "Step 4357: Training loss: 0.9546855092048645\n",
      "Step 4358: Training loss: 0.9255712628364563\n",
      "Step 4359: Training loss: 0.8461287021636963\n",
      "Step 4360: Training loss: 0.9089727997779846\n",
      "Step 4361: Training loss: 0.8501097559928894\n",
      "Step 4362: Training loss: 0.8265780806541443\n",
      "Step 4363: Training loss: 0.7516653537750244\n",
      "Step 4364: Training loss: 0.8941953182220459\n",
      "Step 4365: Training loss: 0.8395541906356812\n",
      "Step 4366: Training loss: 0.9046643972396851\n",
      "Step 4367: Training loss: 0.9198505878448486\n",
      "Step 4368: Training loss: 0.8323366641998291\n",
      "Step 4369: Training loss: 0.7330918908119202\n",
      "Step 4370: Training loss: 0.71010822057724\n",
      "Step 4371: Training loss: 0.844387412071228\n",
      "Step 4372: Training loss: 0.7651432752609253\n",
      "Step 4373: Training loss: 0.8696684241294861\n",
      "Step 4374: Training loss: 0.8380104303359985\n",
      "Step 4375: Training loss: 0.818963348865509\n",
      "Step 4376: Training loss: 0.8301153779029846\n",
      "Step 4377: Training loss: 0.8503177165985107\n",
      "Step 4378: Training loss: 0.729211688041687\n",
      "Step 4379: Training loss: 0.9138549566268921\n",
      "Step 4380: Training loss: 0.952601969242096\n",
      "Step 4381: Training loss: 0.8686485290527344\n",
      "Step 4382: Training loss: 0.8496838212013245\n",
      "Step 4383: Training loss: 0.8281728625297546\n",
      "Step 4384: Training loss: 0.873763382434845\n",
      "Step 4385: Training loss: 0.930225133895874\n",
      "Step 4386: Training loss: 0.8882326483726501\n",
      "Step 4387: Training loss: 0.9667242169380188\n",
      "Step 4388: Training loss: 1.0531541109085083\n",
      "Step 4389: Training loss: 1.0281039476394653\n",
      "Step 4390: Training loss: 0.846368134021759\n",
      "Step 4391: Training loss: 0.7658694982528687\n",
      "Step 4392: Training loss: 0.7378156781196594\n",
      "Step 4393: Training loss: 0.8317415714263916\n",
      "Step 4394: Training loss: 0.9179515838623047\n",
      "Step 4395: Training loss: 0.6986415386199951\n",
      "Step 4396: Training loss: 0.8847270011901855\n",
      "Step 4397: Training loss: 0.8665422797203064\n",
      "Step 4398: Training loss: 0.9676544666290283\n",
      "Step 4399: Training loss: 0.9370766878128052\n",
      "Step 4400: Training loss: 0.843679666519165\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4400\n",
      "Step 4401: Training loss: 0.9190793037414551\n",
      "Step 4402: Training loss: 1.0103148221969604\n",
      "Step 4403: Training loss: 0.8675833344459534\n",
      "Step 4404: Training loss: 0.7715993523597717\n",
      "Step 4405: Training loss: 0.8194750547409058\n",
      "Step 4406: Training loss: 1.0405699014663696\n",
      "Step 4407: Training loss: 0.9656599760055542\n",
      "Step 4408: Training loss: 0.7889395952224731\n",
      "Step 4409: Training loss: 1.005808711051941\n",
      "Step 4410: Training loss: 0.8080127239227295\n",
      "Step 4411: Training loss: 0.8317954540252686\n",
      "Step 4412: Training loss: 0.9014712572097778\n",
      "Step 4413: Training loss: 0.7722972631454468\n",
      "Step 4414: Training loss: 0.697124719619751\n",
      "Step 4415: Training loss: 0.874114990234375\n",
      "Step 4416: Training loss: 0.8297470211982727\n",
      "Step 4417: Training loss: 0.7411999106407166\n",
      "Step 4418: Training loss: 0.9061360359191895\n",
      "Step 4419: Training loss: 0.869572639465332\n",
      "Step 4420: Training loss: 0.7899523377418518\n",
      "Step 4421: Training loss: 0.7667861580848694\n",
      "Step 4422: Training loss: 0.8854241967201233\n",
      "Step 4423: Training loss: 0.922285258769989\n",
      "Step 4424: Training loss: 0.9705449938774109\n",
      "Step 4425: Training loss: 0.7962177991867065\n",
      "Step 4426: Training loss: 0.9309501647949219\n",
      "Step 4427: Training loss: 0.8820115923881531\n",
      "Step 4428: Training loss: 0.9891091585159302\n",
      "Step 4429: Training loss: 0.9072293043136597\n",
      "Step 4430: Training loss: 1.0150237083435059\n",
      "Step 4431: Training loss: 0.9246786832809448\n",
      "Step 4432: Training loss: 0.7785764336585999\n",
      "Step 4433: Training loss: 0.7846352458000183\n",
      "Step 4434: Training loss: 0.836557149887085\n",
      "Step 4435: Training loss: 0.9111981391906738\n",
      "Step 4436: Training loss: 0.7999973297119141\n",
      "Step 4437: Training loss: 0.9054796099662781\n",
      "Step 4438: Training loss: 0.7925613522529602\n",
      "Step 4439: Training loss: 0.6391768455505371\n",
      "Step 4440: Training loss: 0.7611124515533447\n",
      "Step 4441: Training loss: 0.8338980674743652\n",
      "Step 4442: Training loss: 0.7818688750267029\n",
      "Step 4443: Training loss: 0.7574006915092468\n",
      "Step 4444: Training loss: 0.7984458208084106\n",
      "Step 4445: Training loss: 0.786823034286499\n",
      "Step 4446: Training loss: 0.7405892610549927\n",
      "Step 4447: Training loss: 0.800230085849762\n",
      "Step 4448: Training loss: 0.9859916567802429\n",
      "Step 4449: Training loss: 0.8281534910202026\n",
      "Step 4450: Training loss: 0.7062940001487732\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4450\n",
      "Step 4451: Training loss: 0.8870216608047485\n",
      "Step 4452: Training loss: 0.6280815601348877\n",
      "Step 4453: Training loss: 0.6886893510818481\n",
      "Step 4454: Training loss: 0.7569867968559265\n",
      "Step 4455: Training loss: 0.7875567078590393\n",
      "Step 4456: Training loss: 0.9242920279502869\n",
      "Step 4457: Training loss: 0.8522093892097473\n",
      "Step 4458: Training loss: 0.712153971195221\n",
      "Step 4459: Training loss: 0.8299803137779236\n",
      "Step 4460: Training loss: 0.7575086355209351\n",
      "Step 4461: Training loss: 0.6663858890533447\n",
      "Step 4462: Training loss: 0.8566202521324158\n",
      "Step 4463: Training loss: 0.9296250939369202\n",
      "Step 4464: Training loss: 0.8655475974082947\n",
      "Step 4465: Training loss: 0.8472146987915039\n",
      "Step 4466: Training loss: 0.7493566870689392\n",
      "Step 4467: Training loss: 0.8141489028930664\n",
      "Step 4468: Training loss: 0.7549422979354858\n",
      "Step 4469: Training loss: 0.8194131255149841\n",
      "Step 4470: Training loss: 0.8534018993377686\n",
      "Step 4471: Training loss: 0.8261219263076782\n",
      "Step 4472: Training loss: 0.7982927560806274\n",
      "Step 4473: Training loss: 0.933741569519043\n",
      "Step 4474: Training loss: 0.8336910009384155\n",
      "Step 4475: Training loss: 0.9124297499656677\n",
      "Step 4476: Training loss: 0.8152929544448853\n",
      "Step 4477: Training loss: 0.9741259813308716\n",
      "Step 4478: Training loss: 0.7994628548622131\n",
      "Step 4479: Training loss: 0.8058019876480103\n",
      "Step 4480: Training loss: 0.7491862177848816\n",
      "Step 4481: Training loss: 0.8122696876525879\n",
      "Step 4482: Training loss: 0.8291115164756775\n",
      "Step 4483: Training loss: 0.6051905155181885\n",
      "Step 4484: Training loss: 0.8545393943786621\n",
      "Step 4485: Training loss: 0.7923101782798767\n",
      "Step 4486: Training loss: 0.9000388383865356\n",
      "Step 4487: Training loss: 0.8739159107208252\n",
      "Step 4488: Training loss: 0.9665907621383667\n",
      "Step 4489: Training loss: 0.7144460082054138\n",
      "Step 4490: Training loss: 0.9039176106452942\n",
      "Step 4491: Training loss: 0.7961412668228149\n",
      "Step 4492: Training loss: 0.8285537958145142\n",
      "Step 4493: Training loss: 0.8559654355049133\n",
      "Step 4494: Training loss: 0.8327245116233826\n",
      "Step 4495: Training loss: 0.7473706603050232\n",
      "Step 4496: Training loss: 0.687606692314148\n",
      "Step 4497: Training loss: 0.7911638617515564\n",
      "Step 4498: Training loss: 0.7860811352729797\n",
      "Step 4499: Training loss: 0.8737393617630005\n",
      "Step 4500: Training loss: 0.7783734798431396\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4500\n",
      "Step 4501: Training loss: 0.838873028755188\n",
      "Step 4502: Training loss: 0.7881035804748535\n",
      "Step 4503: Training loss: 0.8200510740280151\n",
      "Step 4504: Training loss: 1.065381646156311\n",
      "Step 4505: Training loss: 0.7732595801353455\n",
      "Step 4506: Training loss: 0.8213637471199036\n",
      "Step 4507: Training loss: 0.6391856670379639\n",
      "Step 4508: Training loss: 0.7675751447677612\n",
      "Step 4509: Training loss: 0.8928953409194946\n",
      "Step 4510: Training loss: 0.7990418672561646\n",
      "Step 4511: Training loss: 0.7433996796607971\n",
      "Step 4512: Training loss: 0.6891957521438599\n",
      "Step 4513: Training loss: 0.966139554977417\n",
      "Step 4514: Training loss: 0.7593305110931396\n",
      "Step 4515: Training loss: 0.9699697494506836\n",
      "Step 4516: Training loss: 0.8299117684364319\n",
      "Step 4517: Training loss: 0.8481824398040771\n",
      "Step 4518: Training loss: 0.919485330581665\n",
      "Step 4519: Training loss: 0.8990874886512756\n",
      "Step 4520: Training loss: 0.7343997955322266\n",
      "Step 4521: Training loss: 0.7117813229560852\n",
      "Step 4522: Training loss: 0.7836136817932129\n",
      "Step 4523: Training loss: 0.7530413866043091\n",
      "Step 4524: Training loss: 0.9027387499809265\n",
      "Step 4525: Training loss: 0.8858449459075928\n",
      "Step 4526: Training loss: 0.743241012096405\n",
      "Step 4527: Training loss: 0.8364105224609375\n",
      "Step 4528: Training loss: 0.876501739025116\n",
      "Step 4529: Training loss: 0.8236815929412842\n",
      "Step 4530: Training loss: 0.7245023250579834\n",
      "Step 4531: Training loss: 0.6780409216880798\n",
      "Step 4532: Training loss: 0.7354977130889893\n",
      "Step 4533: Training loss: 1.4105801582336426\n",
      "Step 4534: Training loss: 0.9046429991722107\n",
      "Step 4535: Training loss: 0.8540586829185486\n",
      "Step 4536: Training loss: 0.8085067272186279\n",
      "Step 4537: Training loss: 0.8679260015487671\n",
      "Step 4538: Training loss: 0.8384277820587158\n",
      "Step 4539: Training loss: 0.8581578135490417\n",
      "Step 4540: Training loss: 0.8922622203826904\n",
      "Step 4541: Training loss: 0.7586673498153687\n",
      "Step 4542: Training loss: 0.7685998678207397\n",
      "Step 4543: Training loss: 0.8327395915985107\n",
      "Step 4544: Training loss: 0.9270160794258118\n",
      "Step 4545: Training loss: 0.8684945106506348\n",
      "Step 4546: Training loss: 0.7288397550582886\n",
      "Step 4547: Training loss: 0.6859920620918274\n",
      "Step 4548: Training loss: 0.6460504531860352\n",
      "Step 4549: Training loss: 0.743630051612854\n",
      "Step 4550: Training loss: 0.7650542259216309\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4550\n",
      "Step 4551: Training loss: 0.8879336714744568\n",
      "Step 4552: Training loss: 0.6950498819351196\n",
      "Step 4553: Training loss: 0.6885606050491333\n",
      "Step 4554: Training loss: 0.7890164256095886\n",
      "Step 4555: Training loss: 0.8605788350105286\n",
      "Step 4556: Training loss: 0.875174343585968\n",
      "Step 4557: Training loss: 0.8710178732872009\n",
      "Step 4558: Training loss: 0.5897566080093384\n",
      "Step 4559: Training loss: 0.7886552214622498\n",
      "Step 4560: Training loss: 1.3336427211761475\n",
      "Step 4561: Training loss: 0.7274502515792847\n",
      "Step 4562: Training loss: 0.9219896197319031\n",
      "Step 4563: Training loss: 0.5260293483734131\n",
      "Step 4564: Training loss: 0.8265092968940735\n",
      "Step 4565: Training loss: 0.9555736184120178\n",
      "Step 4566: Training loss: 0.8395451903343201\n",
      "Step 4567: Training loss: 0.8743262887001038\n",
      "Step 4568: Training loss: 0.9170709848403931\n",
      "Step 4569: Training loss: 0.7753172516822815\n",
      "Step 4570: Training loss: 0.7293321490287781\n",
      "Step 4571: Training loss: 0.934490442276001\n",
      "Step 4572: Training loss: 0.8723465800285339\n",
      "Step 4573: Training loss: 0.8554999828338623\n",
      "Step 4574: Training loss: 0.7212675213813782\n",
      "Step 4575: Training loss: 0.9067840576171875\n",
      "Step 4576: Training loss: 0.7802445888519287\n",
      "Step 4577: Training loss: 0.7281509041786194\n",
      "Step 4578: Training loss: 0.714541494846344\n",
      "Step 4579: Training loss: 0.793308436870575\n",
      "Step 4580: Training loss: 0.8849728107452393\n",
      "Step 4581: Training loss: 0.8889271020889282\n",
      "Step 4582: Training loss: 0.6937106847763062\n",
      "Step 4583: Training loss: 0.7376545071601868\n",
      "Step 4584: Training loss: 0.8717003464698792\n",
      "Step 4585: Training loss: 0.838766872882843\n",
      "Step 4586: Training loss: 0.6260678172111511\n",
      "Step 4587: Training loss: 0.8517160415649414\n",
      "Step 4588: Training loss: 1.0014164447784424\n",
      "Step 4589: Training loss: 0.9452565908432007\n",
      "Step 4590: Training loss: 0.8209603428840637\n",
      "Step 4591: Training loss: 0.7891034483909607\n",
      "Step 4592: Training loss: 0.8442932963371277\n",
      "Step 4593: Training loss: 0.6645525693893433\n",
      "Step 4594: Training loss: 0.7228971123695374\n",
      "Step 4595: Training loss: 0.628007709980011\n",
      "Step 4596: Training loss: 0.975619912147522\n",
      "Step 4597: Training loss: 0.8946244716644287\n",
      "Step 4598: Training loss: 0.7062921524047852\n",
      "Step 4599: Training loss: 0.9274097681045532\n",
      "Step 4600: Training loss: 0.752615213394165\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4600\n",
      "Step 4601: Training loss: 0.8470984101295471\n",
      "Step 4602: Training loss: 0.8542940616607666\n",
      "Step 4603: Training loss: 0.7828559875488281\n",
      "Step 4604: Training loss: 0.7355035543441772\n",
      "Step 4605: Training loss: 0.8597084283828735\n",
      "Step 4606: Training loss: 0.8414454460144043\n",
      "Step 4607: Training loss: 0.8863462209701538\n",
      "Step 4608: Training loss: 0.7756739258766174\n",
      "Step 4609: Training loss: 0.8433743119239807\n",
      "Step 4610: Training loss: 0.7207241654396057\n",
      "Step 4611: Training loss: 0.764878511428833\n",
      "Step 4612: Training loss: 0.6849033236503601\n",
      "Step 4613: Training loss: 0.8559524416923523\n",
      "Step 4614: Training loss: 0.8014909029006958\n",
      "Step 4615: Training loss: 0.5760700106620789\n",
      "Step 4616: Training loss: 0.7514442801475525\n",
      "Step 4617: Training loss: 0.757843017578125\n",
      "Step 4618: Training loss: 0.660610556602478\n",
      "Step 4619: Training loss: 0.8452025055885315\n",
      "Step 4620: Training loss: 0.5926100611686707\n",
      "Step 4621: Training loss: 0.8010239601135254\n",
      "Step 4622: Training loss: 0.9514089226722717\n",
      "Step 4623: Training loss: 0.7514739632606506\n",
      "Step 4624: Training loss: 0.9074064493179321\n",
      "Step 4625: Training loss: 0.7882477045059204\n",
      "Step 4626: Training loss: 0.8785169720649719\n",
      "Step 4627: Training loss: 0.875245213508606\n",
      "Step 4628: Training loss: 1.082559585571289\n",
      "Step 4629: Training loss: 0.8892145752906799\n",
      "Step 4630: Training loss: 0.8291643857955933\n",
      "Step 4631: Training loss: 0.6799572110176086\n",
      "Step 4632: Training loss: 0.6731049418449402\n",
      "Step 4633: Training loss: 0.8025652170181274\n",
      "Step 4634: Training loss: 0.795630931854248\n",
      "Step 4635: Training loss: 0.7026605606079102\n",
      "Step 4636: Training loss: 0.6490221619606018\n",
      "Step 4637: Training loss: 0.6394418478012085\n",
      "Step 4638: Training loss: 0.7745145559310913\n",
      "Step 4639: Training loss: 0.8012641668319702\n",
      "Step 4640: Training loss: 0.9365618228912354\n",
      "Step 4641: Training loss: 0.7099043726921082\n",
      "Step 4642: Training loss: 0.6017767190933228\n",
      "Step 4643: Training loss: 0.848451554775238\n",
      "Step 4644: Training loss: 0.7539932727813721\n",
      "Step 4645: Training loss: 0.8413023352622986\n",
      "Step 4646: Training loss: 0.876419723033905\n",
      "Step 4647: Training loss: 0.6484808325767517\n",
      "Step 4648: Training loss: 0.8739036321640015\n",
      "Step 4649: Training loss: 1.0079452991485596\n",
      "Step 4650: Training loss: 0.8308519721031189\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-4650\n",
      "Step 4651: Training loss: 0.6948456764221191\n",
      "Step 4652: Training loss: 0.6096709966659546\n",
      "Step 4653: Training loss: 0.6171786785125732\n",
      "Step 4654: Training loss: 0.5947131514549255\n",
      "Step 4655: Training loss: 0.8225447535514832\n",
      "Step 4656: Training loss: 0.8035157322883606\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# log_dir = \"logs/\"\n",
    "# writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Define the hyperparameters\n",
    "per_gpu_train_batch_size = 32\n",
    "per_gpu_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 100\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 3\n",
    "max_steps = -1\n",
    "warmup_steps = 10000\n",
    "logging_steps = 1\n",
    "save_steps = 50\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "# Define the training loop\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_train_epochs}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Move the inputs and labels to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # Update the parameters\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log training loss\n",
    "            epoch_loss += loss.item()\n",
    "            if global_step % logging_steps == 0:\n",
    "                print(f'Step {global_step}: Training loss: {epoch_loss/logging_steps}')\n",
    "                writer.add_scalar('Training loss', epoch_loss/logging_steps, global_step)\n",
    "                epoch_loss = 0\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if global_step % save_steps == 0:\n",
    "                output_dir = f'./checkpoints/checkpoint-{global_step}'\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "                print(f'Saved model checkpoint to {output_dir}')\n",
    "                \n",
    "                    \n",
    "                model.train()\n",
    "                \n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "        \n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64dd50c1c4174e67b1de4c741e3f2e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Training loss: 10.909353368759156\n",
      "Step 2000: Training loss: 10.909596610069276\n",
      "Step 3000: Training loss: 10.908786114692688\n",
      "Step 4000: Training loss: 10.90892912387848\n",
      "Step 5000: Training loss: 10.909625336647034\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     45\u001b[0m \u001b[39m# Clip the gradients to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), max_norm\u001b[39m=\u001b[39;49mmax_grad_norm)\n\u001b[0;32m     48\u001b[0m \u001b[39m# Update the parameters\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[1;32m---> 76\u001b[0m         torch\u001b[39m.\u001b[39;49m_foreach_mul_(grads, clip_coef_clamped\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[0;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Define the hyperparameters\n",
    "per_gpu_train_batch_size = 4\n",
    "per_gpu_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 3\n",
    "max_steps = -1\n",
    "warmup_steps = 0\n",
    "logging_steps = 1000\n",
    "save_steps = 5000\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "# Define the training loop\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_train_epochs}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Move the inputs and labels to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # Update the parameters\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log training loss\n",
    "            epoch_loss += loss.item()\n",
    "            if global_step % logging_steps == 0:\n",
    "                print(f'Step {global_step}: Training loss: {epoch_loss/logging_steps}')\n",
    "                writer.add_scalar('Training loss', epoch_loss/logging_steps, global_step)\n",
    "                epoch_loss = 0\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if global_step % save_steps == 0:\n",
    "                output_dir = f'./checkpoints/checkpoint-{global_step}'\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "                print(f'Saved model checkpoint to {output_dir}')\n",
    "                \n",
    "                    \n",
    "                model.train()\n",
    "                \n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "        \n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "    \n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "log_file = \"my_log_file.log\"\n",
    "\n",
    "# Specify the log file name in the log_dir directory\n",
    "log_path = os.path.join(log_dir, log_file)\n",
    "\n",
    "# Create the SummaryWriter object with the specified log file\n",
    "writer = SummaryWriter(log_dir=log_dir, filename_suffix=log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "\n",
    "try:\n",
    "    # Create the logging directory if it doesn't exist\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Create the SummaryWriter object\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating SummaryWriter object: {e}\")\n",
    "    writer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6a2985617248599c3d87e42adb6c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: Training loss: 10.907888774871827\n",
      "Step 200: Training loss: 10.903826532363892\n",
      "Step 300: Training loss: 10.904981193542481\n",
      "Step 400: Training loss: 10.90920166015625\n",
      "Step 500: Training loss: 10.906534976959229\n",
      "Step 600: Training loss: 10.909966363906861\n",
      "Step 700: Training loss: 10.910112705230713\n",
      "Step 800: Training loss: 10.905688705444335\n",
      "Step 900: Training loss: 10.909560213088989\n",
      "Step 1000: Training loss: 10.904947700500488\n",
      "Step 1100: Training loss: 10.909452152252197\n",
      "Step 1200: Training loss: 10.911711168289184\n",
      "Step 1300: Training loss: 10.905303325653076\n",
      "Step 1400: Training loss: 10.907367601394654\n",
      "Step 1500: Training loss: 10.911247653961182\n",
      "Step 1600: Training loss: 10.903938703536987\n",
      "Step 1700: Training loss: 10.910807628631591\n",
      "Step 1800: Training loss: 10.907160120010376\n",
      "Step 1900: Training loss: 10.911795253753661\n",
      "Step 2000: Training loss: 10.907452135086059\n",
      "Step 2100: Training loss: 10.905585432052613\n",
      "Step 2200: Training loss: 10.908268795013427\n",
      "Step 2300: Training loss: 10.90606258392334\n",
      "Step 2400: Training loss: 10.906298742294311\n",
      "Step 2500: Training loss: 10.912224159240722\n",
      "Step 2600: Training loss: 10.907681436538697\n",
      "Step 2700: Training loss: 10.911226253509522\n",
      "Step 2800: Training loss: 10.908757104873658\n",
      "Step 2900: Training loss: 10.905739183425903\n",
      "Step 3000: Training loss: 10.91476119041443\n",
      "Step 3100: Training loss: 10.90821120262146\n",
      "Step 3200: Training loss: 10.910997352600099\n",
      "Step 3300: Training loss: 10.910010528564452\n",
      "Step 3400: Training loss: 10.909078102111817\n",
      "Step 3500: Training loss: 10.909466915130615\n",
      "Step 3600: Training loss: 10.907138204574585\n",
      "Step 3700: Training loss: 10.907478322982788\n",
      "Step 3800: Training loss: 10.909994411468507\n",
      "Step 3900: Training loss: 10.909499187469482\n",
      "Step 4000: Training loss: 10.905260486602783\n",
      "Step 4100: Training loss: 10.90910698890686\n",
      "Step 4200: Training loss: 10.9123180103302\n",
      "Step 4300: Training loss: 10.909841747283936\n",
      "Step 4400: Training loss: 10.909100246429443\n",
      "Step 4500: Training loss: 10.908501186370849\n",
      "Step 4600: Training loss: 10.907988138198853\n",
      "Step 4700: Training loss: 10.910586214065551\n",
      "Step 4800: Training loss: 10.90834204673767\n",
      "Step 4900: Training loss: 10.908768634796143\n",
      "Step 5000: Training loss: 10.9106734085083\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-5000\n",
      "Step 5100: Training loss: 10.912756175994874\n",
      "Step 5200: Training loss: 10.90981644630432\n",
      "Step 5300: Training loss: 10.907562599182128\n",
      "Step 5400: Training loss: 10.906374912261963\n",
      "Step 5500: Training loss: 10.909554443359376\n",
      "Step 5600: Training loss: 10.907810144424438\n",
      "Step 5700: Training loss: 10.908493928909301\n",
      "Step 5800: Training loss: 10.91269944190979\n",
      "Step 5900: Training loss: 10.913422603607177\n",
      "Step 6000: Training loss: 10.912504091262818\n",
      "Step 6100: Training loss: 10.909076118469239\n",
      "Step 6200: Training loss: 10.911436614990235\n",
      "Step 6300: Training loss: 10.908531923294067\n",
      "Step 6400: Training loss: 10.90725567817688\n",
      "Step 6500: Training loss: 10.910330772399902\n",
      "Step 6600: Training loss: 10.90366473197937\n",
      "Step 6700: Training loss: 10.908173866271973\n",
      "Step 6800: Training loss: 10.907296333312988\n",
      "Step 6900: Training loss: 10.908168144226075\n",
      "Step 7000: Training loss: 10.904479837417602\n",
      "Step 7100: Training loss: 10.906893291473388\n",
      "Step 7200: Training loss: 10.904704580307007\n",
      "Step 7300: Training loss: 10.907170639038085\n",
      "Step 7400: Training loss: 10.907530860900879\n",
      "Step 7500: Training loss: 10.911079683303832\n",
      "Step 7600: Training loss: 10.90549578666687\n",
      "Step 7700: Training loss: 10.90757700920105\n",
      "Step 7800: Training loss: 10.910136108398438\n",
      "Step 7900: Training loss: 10.910514430999756\n",
      "Step 8000: Training loss: 10.90770601272583\n",
      "Step 8100: Training loss: 10.908856525421143\n",
      "Step 8200: Training loss: 10.909293079376221\n",
      "Step 8300: Training loss: 10.905212078094483\n",
      "Step 8400: Training loss: 10.909220504760743\n",
      "Step 8500: Training loss: 10.912774639129639\n",
      "Step 8600: Training loss: 10.909907875061036\n",
      "Step 8700: Training loss: 10.908844451904297\n",
      "Step 8800: Training loss: 10.907559747695922\n",
      "Step 8900: Training loss: 10.906093673706055\n",
      "Step 9000: Training loss: 10.90819356918335\n",
      "Step 9100: Training loss: 10.910916957855225\n",
      "Step 9200: Training loss: 10.906448011398316\n",
      "Step 9300: Training loss: 10.912018928527832\n",
      "Step 9400: Training loss: 10.9096870803833\n",
      "Step 9500: Training loss: 10.910072746276855\n",
      "Step 9600: Training loss: 10.906279468536377\n",
      "Step 9700: Training loss: 10.908413743972778\n",
      "Step 9800: Training loss: 10.906165027618409\n",
      "Step 9900: Training loss: 10.906588649749756\n",
      "Step 10000: Training loss: 10.908536825180054\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-10000\n",
      "Step 10100: Training loss: 10.912470445632934\n",
      "Step 10200: Training loss: 10.902904634475709\n",
      "Step 10300: Training loss: 10.905464935302735\n",
      "Step 10400: Training loss: 10.907890186309814\n",
      "Step 10500: Training loss: 10.914143705368042\n",
      "Step 10600: Training loss: 10.908958196640015\n",
      "Step 10700: Training loss: 10.909773044586181\n",
      "Step 10800: Training loss: 10.908548974990845\n",
      "Step 10900: Training loss: 10.90702582359314\n",
      "Step 11000: Training loss: 10.905433654785156\n",
      "Step 11100: Training loss: 10.911984949111938\n",
      "Step 11200: Training loss: 10.907472648620605\n",
      "Step 11300: Training loss: 10.908017387390137\n",
      "Step 11400: Training loss: 10.905296134948731\n",
      "Step 11500: Training loss: 10.910839672088622\n",
      "Step 11600: Training loss: 10.908628311157226\n",
      "Step 11700: Training loss: 10.905390577316284\n",
      "Step 11800: Training loss: 10.905648765563965\n",
      "Step 11900: Training loss: 10.910293321609497\n",
      "Step 12000: Training loss: 10.90879337310791\n",
      "Step 12100: Training loss: 10.908199682235718\n",
      "Step 12200: Training loss: 10.906363496780395\n",
      "Step 12300: Training loss: 10.908221492767334\n",
      "Step 12400: Training loss: 10.909299983978272\n",
      "Step 12500: Training loss: 10.90902678489685\n",
      "Step 12600: Training loss: 10.912210693359375\n",
      "Step 12700: Training loss: 10.904934959411621\n",
      "Step 12800: Training loss: 10.907869577407837\n",
      "Step 12900: Training loss: 10.905537195205689\n",
      "Step 13000: Training loss: 10.907449731826782\n",
      "Step 13100: Training loss: 10.903535356521607\n",
      "Step 13200: Training loss: 10.90786644935608\n",
      "Step 13300: Training loss: 10.909458703994751\n",
      "Step 13400: Training loss: 10.906814651489258\n",
      "Step 13500: Training loss: 10.909203052520752\n",
      "Step 13600: Training loss: 10.907269859313965\n",
      "Step 13700: Training loss: 10.909129695892334\n",
      "Step 13800: Training loss: 10.906281356811524\n",
      "Step 13900: Training loss: 10.906506366729737\n",
      "Step 14000: Training loss: 10.905646905899047\n",
      "Step 14100: Training loss: 10.910981616973878\n",
      "Step 14200: Training loss: 10.910559339523315\n",
      "Step 14300: Training loss: 10.908928318023682\n",
      "Step 14400: Training loss: 10.909757709503173\n",
      "Step 14500: Training loss: 10.909845247268677\n",
      "Step 14600: Training loss: 10.915010986328125\n",
      "Step 14700: Training loss: 10.908179512023926\n",
      "Step 14800: Training loss: 10.9108598613739\n",
      "Step 14900: Training loss: 10.911432733535767\n",
      "Step 15000: Training loss: 10.907440586090088\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-15000\n",
      "Step 15100: Training loss: 10.908630933761597\n",
      "Step 15200: Training loss: 10.908464784622192\n",
      "Step 15300: Training loss: 10.905065603256226\n",
      "Step 15400: Training loss: 10.906054363250732\n",
      "Step 15500: Training loss: 10.909977235794067\n",
      "Step 15600: Training loss: 10.90393624305725\n",
      "Step 15700: Training loss: 10.906247730255126\n",
      "Step 15800: Training loss: 10.911113500595093\n",
      "Step 15900: Training loss: 10.913937940597535\n",
      "Step 16000: Training loss: 10.907475690841675\n",
      "Step 16100: Training loss: 10.912565088272094\n",
      "Step 16200: Training loss: 10.909793891906737\n",
      "Step 16300: Training loss: 10.911005058288573\n",
      "Step 16400: Training loss: 10.906131696701049\n",
      "Step 16500: Training loss: 10.90892466545105\n",
      "Step 16600: Training loss: 10.91090163230896\n",
      "Step 16700: Training loss: 10.912584047317505\n",
      "Step 16800: Training loss: 10.912170753479003\n",
      "Step 16900: Training loss: 10.907155780792236\n",
      "Step 17000: Training loss: 10.910539350509644\n",
      "Step 17100: Training loss: 10.908730974197388\n",
      "Step 17200: Training loss: 10.907738485336303\n",
      "Step 17300: Training loss: 10.90757658958435\n",
      "Step 17400: Training loss: 10.905794343948365\n",
      "Step 17500: Training loss: 10.910334177017212\n",
      "Step 17600: Training loss: 10.909587993621827\n",
      "Step 17700: Training loss: 10.904715938568115\n",
      "Step 17800: Training loss: 10.908091735839843\n",
      "Step 17900: Training loss: 10.906173610687256\n",
      "Step 18000: Training loss: 10.910747594833374\n",
      "Step 18100: Training loss: 10.909940824508666\n",
      "Step 18200: Training loss: 10.905656709671021\n",
      "Step 18300: Training loss: 10.906926012039184\n",
      "Step 18400: Training loss: 10.907705574035644\n",
      "Step 18500: Training loss: 10.909963474273681\n",
      "Step 18600: Training loss: 10.913384885787965\n",
      "Step 18700: Training loss: 10.911603422164918\n",
      "Step 18800: Training loss: 10.907960052490234\n",
      "Step 18900: Training loss: 10.907964706420898\n",
      "Step 19000: Training loss: 10.903989057540894\n",
      "Step 19100: Training loss: 10.908684606552123\n",
      "Step 19200: Training loss: 10.912330694198609\n",
      "Step 19300: Training loss: 10.907294044494629\n",
      "Step 19400: Training loss: 10.904357805252076\n",
      "Step 19500: Training loss: 10.90809217453003\n",
      "Step 19600: Training loss: 10.910197486877442\n",
      "Step 19700: Training loss: 10.910695486068725\n",
      "Step 19800: Training loss: 10.91052324295044\n",
      "Step 19900: Training loss: 10.908920593261719\n",
      "Step 20000: Training loss: 10.908064680099487\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-20000\n",
      "Step 20100: Training loss: 10.914211492538453\n",
      "Step 20200: Training loss: 10.9072714138031\n",
      "Step 20300: Training loss: 10.910627298355102\n",
      "Step 20400: Training loss: 10.909455690383911\n",
      "Step 20500: Training loss: 10.912876482009887\n",
      "Step 20600: Training loss: 10.907900886535645\n",
      "Step 20700: Training loss: 10.910557832717895\n",
      "Step 20800: Training loss: 10.90876501083374\n",
      "Step 20900: Training loss: 10.911743879318237\n",
      "Step 21000: Training loss: 10.909325733184815\n",
      "Step 21100: Training loss: 10.905941772460938\n",
      "Step 21200: Training loss: 10.910099077224732\n",
      "Step 21300: Training loss: 10.905660524368287\n",
      "Step 21400: Training loss: 10.90885139465332\n",
      "Step 21500: Training loss: 10.907474956512452\n",
      "Step 21600: Training loss: 10.910350818634033\n",
      "Step 21700: Training loss: 10.905211696624756\n",
      "Step 21800: Training loss: 10.90977954864502\n",
      "Step 21900: Training loss: 10.907625293731689\n",
      "Step 22000: Training loss: 10.910481061935425\n",
      "Step 22100: Training loss: 10.910080451965332\n",
      "Step 22200: Training loss: 10.908158884048461\n",
      "Step 22300: Training loss: 10.90588836669922\n",
      "Step 22400: Training loss: 10.909921760559081\n",
      "Step 22500: Training loss: 10.903315696716309\n",
      "Step 22600: Training loss: 10.910698156356812\n",
      "Step 22700: Training loss: 10.908275690078735\n",
      "Step 22800: Training loss: 10.906470584869385\n",
      "Step 22900: Training loss: 10.911066389083862\n",
      "Step 23000: Training loss: 10.908124704360961\n",
      "Step 23100: Training loss: 10.911805324554443\n",
      "Step 23200: Training loss: 10.908905811309815\n",
      "Step 23300: Training loss: 10.911955280303955\n",
      "Step 23400: Training loss: 10.90903624534607\n",
      "Step 23500: Training loss: 10.906910696029662\n",
      "Step 23600: Training loss: 10.91215495109558\n",
      "Step 23700: Training loss: 10.910078802108764\n",
      "Step 23800: Training loss: 10.907721920013428\n",
      "Step 23900: Training loss: 10.90956615447998\n",
      "Step 24000: Training loss: 10.90915584564209\n",
      "Step 24100: Training loss: 10.911340923309327\n",
      "Step 24200: Training loss: 10.910100021362304\n",
      "Step 24300: Training loss: 10.908749341964722\n",
      "Step 24400: Training loss: 10.907739181518554\n",
      "Step 24500: Training loss: 10.911900854110717\n",
      "Step 24600: Training loss: 10.90715859413147\n",
      "Step 24700: Training loss: 10.907113008499145\n",
      "Step 24800: Training loss: 10.911717500686645\n",
      "Step 24900: Training loss: 10.912895469665527\n",
      "Step 25000: Training loss: 10.910404233932494\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-25000\n",
      "Step 25100: Training loss: 10.910925216674805\n",
      "Step 25200: Training loss: 10.910089702606202\n",
      "Step 25300: Training loss: 10.910865125656128\n",
      "Step 25400: Training loss: 10.909406356811523\n",
      "Step 25500: Training loss: 10.910148611068726\n",
      "Step 25600: Training loss: 10.91001480102539\n",
      "Step 25700: Training loss: 10.90899133682251\n",
      "Step 25800: Training loss: 10.909592990875245\n",
      "Step 25900: Training loss: 10.909241819381714\n",
      "Step 26000: Training loss: 10.909015617370606\n",
      "Step 26100: Training loss: 10.9118479347229\n",
      "Step 26200: Training loss: 10.904605655670165\n",
      "Step 26300: Training loss: 10.910593509674072\n",
      "Step 26400: Training loss: 10.909369258880615\n",
      "Step 26500: Training loss: 10.908081865310669\n",
      "Step 26600: Training loss: 10.910479373931885\n",
      "Step 26700: Training loss: 10.90549090385437\n",
      "Step 26800: Training loss: 10.908339881896973\n",
      "Step 26900: Training loss: 10.908186664581299\n",
      "Step 27000: Training loss: 10.906795864105225\n",
      "Step 27100: Training loss: 10.90710605621338\n",
      "Step 27200: Training loss: 10.913514404296874\n",
      "Step 27300: Training loss: 10.909312553405762\n",
      "Step 27400: Training loss: 10.909934864044189\n",
      "Step 27500: Training loss: 10.908159561157227\n",
      "Step 27600: Training loss: 10.910844898223877\n",
      "Step 27700: Training loss: 10.908434839248658\n",
      "Step 27800: Training loss: 10.909211740493774\n",
      "Step 27900: Training loss: 10.907341384887696\n",
      "Step 28000: Training loss: 10.912272996902466\n",
      "Step 28100: Training loss: 10.908649473190307\n",
      "Step 28200: Training loss: 10.905956707000733\n",
      "Step 28300: Training loss: 10.911083307266235\n",
      "Step 28400: Training loss: 10.90778805732727\n",
      "Step 28500: Training loss: 10.908251819610596\n",
      "Step 28600: Training loss: 10.908325147628783\n",
      "Step 28700: Training loss: 10.907869939804078\n",
      "Step 28800: Training loss: 10.913244953155518\n",
      "Step 28900: Training loss: 10.911208333969116\n",
      "Step 29000: Training loss: 10.913443193435668\n",
      "Step 29100: Training loss: 10.910862646102906\n",
      "Step 29200: Training loss: 10.907706022262573\n",
      "Step 29300: Training loss: 10.909305925369262\n",
      "Step 29400: Training loss: 10.9059765625\n",
      "Step 29500: Training loss: 10.907758359909058\n",
      "Step 29600: Training loss: 10.906937770843506\n",
      "Step 29700: Training loss: 10.903679599761963\n",
      "Step 29800: Training loss: 10.909015474319459\n",
      "Step 29900: Training loss: 10.908338689804078\n",
      "Step 30000: Training loss: 10.904496574401856\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-30000\n",
      "Step 30100: Training loss: 10.906094312667847\n",
      "Step 30200: Training loss: 10.905062294006347\n",
      "Step 30300: Training loss: 10.912609224319459\n",
      "Step 30400: Training loss: 10.907217502593994\n",
      "Step 30500: Training loss: 10.907365140914917\n",
      "Step 30600: Training loss: 10.91131658554077\n",
      "Step 30700: Training loss: 10.911702852249146\n",
      "Step 30800: Training loss: 10.90674334526062\n",
      "Step 30900: Training loss: 10.911506881713867\n",
      "Step 31000: Training loss: 10.910802993774414\n",
      "Step 31100: Training loss: 10.909596614837646\n",
      "Step 31200: Training loss: 10.90378303527832\n",
      "Step 31300: Training loss: 10.910765237808228\n",
      "Step 31400: Training loss: 10.908819141387939\n",
      "Step 31500: Training loss: 10.912465896606445\n",
      "Step 31600: Training loss: 10.91080719947815\n",
      "Step 31700: Training loss: 10.907402114868164\n",
      "Step 31800: Training loss: 10.902319660186768\n",
      "Step 31900: Training loss: 10.908315000534058\n",
      "Step 32000: Training loss: 10.904841032028198\n",
      "Step 32100: Training loss: 10.908408145904541\n",
      "Step 32200: Training loss: 10.911274175643921\n",
      "Step 32300: Training loss: 10.9109494972229\n",
      "Step 32400: Training loss: 10.908377666473388\n",
      "Step 32500: Training loss: 10.907353944778443\n",
      "Step 32600: Training loss: 10.910162830352784\n",
      "Step 32700: Training loss: 10.907471590042114\n",
      "Step 32800: Training loss: 10.906913595199585\n",
      "Step 32900: Training loss: 10.902677488327026\n",
      "Step 33000: Training loss: 10.911768798828126\n",
      "Step 33100: Training loss: 10.908245592117309\n",
      "Step 33200: Training loss: 10.9090225315094\n",
      "Step 33300: Training loss: 10.908022079467774\n",
      "Step 33400: Training loss: 10.909541702270507\n",
      "Step 33500: Training loss: 10.906769247055054\n",
      "Step 33600: Training loss: 10.913589830398559\n",
      "Step 33700: Training loss: 10.910495576858521\n",
      "Step 33800: Training loss: 10.90786690711975\n",
      "Step 33900: Training loss: 10.904617319107055\n",
      "Step 34000: Training loss: 10.907082710266113\n",
      "Step 34100: Training loss: 10.909527950286865\n",
      "Step 34200: Training loss: 10.906665897369384\n",
      "Step 34300: Training loss: 10.90897066116333\n",
      "Step 34400: Training loss: 10.910129203796387\n",
      "Step 34500: Training loss: 10.907024164199829\n",
      "Step 34600: Training loss: 10.909713706970216\n",
      "Step 34700: Training loss: 10.911747541427612\n",
      "Step 34800: Training loss: 10.907051582336425\n",
      "Step 34900: Training loss: 10.91280348777771\n",
      "Step 35000: Training loss: 10.904826622009278\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-35000\n",
      "Step 35100: Training loss: 10.90810510635376\n",
      "Step 35200: Training loss: 10.907287864685058\n",
      "Step 35300: Training loss: 10.906686754226685\n",
      "Step 35400: Training loss: 10.906220254898072\n",
      "Step 35500: Training loss: 10.908453598022462\n",
      "Step 35600: Training loss: 10.909619188308715\n",
      "Step 35700: Training loss: 10.907596740722656\n",
      "Step 35800: Training loss: 10.904340782165526\n",
      "Step 35900: Training loss: 10.910383548736572\n",
      "Step 36000: Training loss: 10.906395330429078\n",
      "Step 36100: Training loss: 10.909989891052247\n",
      "Step 36200: Training loss: 10.90699860572815\n",
      "Step 36300: Training loss: 10.913861074447631\n",
      "Step 36400: Training loss: 10.907087678909301\n",
      "Step 36500: Training loss: 10.912086744308471\n",
      "Step 36600: Training loss: 10.905389242172241\n",
      "Step 36700: Training loss: 10.91130539894104\n",
      "Step 36800: Training loss: 10.906509084701538\n",
      "Step 36900: Training loss: 10.906444215774536\n",
      "Step 37000: Training loss: 10.909879589080811\n",
      "Step 37100: Training loss: 10.907463760375977\n",
      "Step 37200: Training loss: 10.910589714050293\n",
      "Step 37300: Training loss: 10.907859592437744\n",
      "Step 37400: Training loss: 10.90943109512329\n",
      "Step 37500: Training loss: 10.905605049133301\n",
      "Step 37600: Training loss: 10.909042119979858\n",
      "Step 37700: Training loss: 10.909312496185303\n",
      "Step 37800: Training loss: 10.906450824737549\n",
      "Step 37900: Training loss: 10.909828233718873\n",
      "Step 38000: Training loss: 10.91665117263794\n",
      "Step 38100: Training loss: 10.907677125930785\n",
      "Step 38200: Training loss: 10.910316543579102\n",
      "Step 38300: Training loss: 10.908732719421387\n",
      "Step 38400: Training loss: 10.908244857788086\n",
      "Step 38500: Training loss: 10.91084267616272\n",
      "Step 38600: Training loss: 10.912000913619995\n",
      "Step 38700: Training loss: 10.900795230865478\n",
      "Step 38800: Training loss: 10.910219354629517\n",
      "Step 38900: Training loss: 10.909488906860352\n",
      "Step 39000: Training loss: 10.910413379669189\n",
      "Step 39100: Training loss: 10.908227767944336\n",
      "Step 39200: Training loss: 10.910131416320802\n",
      "Step 39300: Training loss: 10.908469514846802\n",
      "Step 39400: Training loss: 10.906735095977783\n",
      "Step 39500: Training loss: 10.908205995559692\n",
      "Step 39600: Training loss: 10.913515634536743\n",
      "Step 39700: Training loss: 10.908623962402343\n",
      "Step 39800: Training loss: 10.905197706222534\n",
      "Step 39900: Training loss: 10.906761703491211\n",
      "Step 40000: Training loss: 10.914445638656616\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-40000\n",
      "Step 40100: Training loss: 10.905967273712157\n",
      "Step 40200: Training loss: 10.906551313400268\n",
      "Step 40300: Training loss: 10.913738470077515\n",
      "Step 40400: Training loss: 10.909240245819092\n",
      "Step 40500: Training loss: 10.908933019638061\n",
      "Step 40600: Training loss: 10.911707696914673\n",
      "Step 40700: Training loss: 10.907286529541016\n",
      "Step 40800: Training loss: 10.911612005233765\n",
      "Step 40900: Training loss: 10.907376718521117\n",
      "Step 41000: Training loss: 10.908150768280029\n",
      "Step 41100: Training loss: 10.91259817123413\n",
      "Step 41200: Training loss: 10.906552848815918\n",
      "Step 41300: Training loss: 10.906954326629638\n",
      "Step 41400: Training loss: 10.906677122116088\n",
      "Step 41500: Training loss: 10.909311246871948\n",
      "Step 41600: Training loss: 10.910154323577881\n",
      "Step 41700: Training loss: 10.907430353164672\n",
      "Step 41800: Training loss: 10.91068585395813\n",
      "Step 41900: Training loss: 10.908054599761963\n",
      "Step 42000: Training loss: 10.908671789169311\n",
      "Step 42100: Training loss: 10.906420583724975\n",
      "Step 42200: Training loss: 10.905626735687257\n",
      "Step 42300: Training loss: 10.909531679153442\n",
      "Step 42400: Training loss: 10.90708604812622\n",
      "Step 42500: Training loss: 10.911473598480224\n",
      "Step 42600: Training loss: 10.910941286087036\n",
      "Step 42700: Training loss: 10.908142957687378\n",
      "Step 42800: Training loss: 10.906964073181152\n",
      "Step 42900: Training loss: 10.90854588508606\n",
      "Step 43000: Training loss: 10.906444730758666\n",
      "Step 43100: Training loss: 10.907857666015625\n",
      "Step 43200: Training loss: 10.912139940261842\n",
      "Step 43300: Training loss: 10.911363363265991\n",
      "Step 43400: Training loss: 10.905750007629395\n",
      "Step 43500: Training loss: 10.907658338546753\n",
      "Step 43600: Training loss: 10.91208436012268\n",
      "Step 43700: Training loss: 10.909986543655396\n",
      "Step 43800: Training loss: 10.90897647857666\n",
      "Step 43900: Training loss: 10.907451248168945\n",
      "Step 44000: Training loss: 10.9100492477417\n",
      "Step 44100: Training loss: 10.908767642974853\n",
      "Step 44200: Training loss: 10.912293090820313\n",
      "Step 44300: Training loss: 10.907589521408081\n",
      "Step 44400: Training loss: 10.903237161636353\n",
      "Step 44500: Training loss: 10.908429374694824\n",
      "Step 44600: Training loss: 10.913815107345581\n",
      "Step 44700: Training loss: 10.913338861465455\n",
      "Step 44800: Training loss: 10.910609216690064\n",
      "Step 44900: Training loss: 10.908163862228394\n",
      "Step 45000: Training loss: 10.912926969528199\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-45000\n",
      "Step 45100: Training loss: 10.910704107284547\n",
      "Step 45200: Training loss: 10.90998327255249\n",
      "Step 45300: Training loss: 10.908743133544922\n",
      "Step 45400: Training loss: 10.910254497528076\n",
      "Step 45500: Training loss: 10.906025924682616\n",
      "Step 45600: Training loss: 10.910967922210693\n",
      "Step 45700: Training loss: 10.906218223571777\n",
      "Step 45800: Training loss: 10.907715616226197\n",
      "Step 45900: Training loss: 10.907668962478638\n",
      "Step 46000: Training loss: 10.907337818145752\n",
      "Step 46100: Training loss: 10.911158628463745\n",
      "Step 46200: Training loss: 10.907106466293335\n",
      "Step 46300: Training loss: 10.912926120758057\n",
      "Step 46400: Training loss: 10.910769710540771\n",
      "Step 46500: Training loss: 10.906353902816772\n",
      "Step 46600: Training loss: 10.910629653930664\n",
      "Step 46700: Training loss: 10.904186086654663\n",
      "Step 46800: Training loss: 10.906205253601074\n",
      "Step 46900: Training loss: 10.90922287940979\n",
      "Step 47000: Training loss: 10.912631416320801\n",
      "Step 47100: Training loss: 10.91234351158142\n",
      "Step 47200: Training loss: 10.908621673583985\n",
      "Step 47300: Training loss: 10.910322580337525\n",
      "Step 47400: Training loss: 10.908849010467529\n",
      "Step 47500: Training loss: 10.90460886001587\n",
      "Step 47600: Training loss: 10.909747486114503\n",
      "Step 47700: Training loss: 10.907007179260255\n",
      "Step 47800: Training loss: 10.909783020019532\n",
      "Step 47900: Training loss: 10.913829708099366\n",
      "Step 48000: Training loss: 10.910358781814574\n",
      "Step 48100: Training loss: 10.905879077911377\n",
      "Step 48200: Training loss: 10.909781064987182\n",
      "Step 48300: Training loss: 10.90671353340149\n",
      "Step 48400: Training loss: 10.905497980117797\n",
      "Step 48500: Training loss: 10.907174730300904\n",
      "Step 48600: Training loss: 10.912284259796143\n",
      "Step 48700: Training loss: 10.906558437347412\n",
      "Step 48800: Training loss: 10.904844312667846\n",
      "Step 48900: Training loss: 10.909658842086792\n",
      "Step 49000: Training loss: 10.908168954849243\n",
      "Step 49100: Training loss: 10.908904523849488\n",
      "Step 49200: Training loss: 10.903012228012084\n",
      "Step 49300: Training loss: 10.910365238189698\n",
      "Step 49400: Training loss: 10.908475704193116\n",
      "Step 49500: Training loss: 10.907740278244018\n",
      "Step 49600: Training loss: 10.902868947982789\n",
      "Step 49700: Training loss: 10.911634311676025\n",
      "Step 49800: Training loss: 10.906091766357422\n",
      "Step 49900: Training loss: 10.907148694992065\n",
      "Step 50000: Training loss: 10.907569017410278\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-50000\n",
      "Step 50100: Training loss: 10.912350101470947\n",
      "Step 50200: Training loss: 10.90763129234314\n",
      "Step 50300: Training loss: 10.911373453140259\n",
      "Step 50400: Training loss: 10.914456377029419\n",
      "Step 50500: Training loss: 10.907176656723022\n",
      "Step 50600: Training loss: 10.910584754943848\n",
      "Step 50700: Training loss: 10.907749223709107\n",
      "Step 50800: Training loss: 10.908080005645752\n",
      "Step 50900: Training loss: 10.906769008636475\n",
      "Step 51000: Training loss: 10.911273593902587\n",
      "Step 51100: Training loss: 10.909079256057739\n",
      "Step 51200: Training loss: 10.907540044784547\n",
      "Step 51300: Training loss: 10.908010845184327\n",
      "Step 51400: Training loss: 10.908080596923828\n",
      "Step 51500: Training loss: 10.907296943664551\n",
      "Step 51600: Training loss: 10.915896530151366\n",
      "Step 51700: Training loss: 10.913846788406373\n",
      "Step 51800: Training loss: 10.9108522605896\n",
      "Step 51900: Training loss: 10.909437818527222\n",
      "Step 52000: Training loss: 10.909344682693481\n",
      "Step 52100: Training loss: 10.908240642547607\n",
      "Step 52200: Training loss: 10.907307605743409\n",
      "Step 52300: Training loss: 10.911305170059205\n",
      "Step 52400: Training loss: 10.908374099731445\n",
      "Step 52500: Training loss: 10.902327318191528\n",
      "Step 52600: Training loss: 10.906637926101684\n",
      "Step 52700: Training loss: 10.911651782989502\n",
      "Step 52800: Training loss: 10.907308883666992\n",
      "Step 52900: Training loss: 10.90655764579773\n",
      "Step 53000: Training loss: 10.90900818824768\n",
      "Step 53100: Training loss: 10.908192920684815\n",
      "Step 53200: Training loss: 10.90827672958374\n",
      "Step 53300: Training loss: 10.908830080032349\n",
      "Step 53400: Training loss: 10.911064605712891\n",
      "Step 53500: Training loss: 10.914405689239501\n",
      "Step 53600: Training loss: 10.903783092498779\n",
      "Step 53700: Training loss: 10.908045654296876\n",
      "Step 53800: Training loss: 10.908653469085694\n",
      "Step 53900: Training loss: 10.90664698600769\n",
      "Step 54000: Training loss: 10.910617952346803\n",
      "Step 54100: Training loss: 10.907508897781373\n",
      "Step 54200: Training loss: 10.908199863433838\n",
      "Step 54300: Training loss: 10.911992540359497\n",
      "Step 54400: Training loss: 10.907220849990845\n",
      "Step 54500: Training loss: 10.909570055007935\n",
      "Step 54600: Training loss: 10.906525001525878\n",
      "Step 54700: Training loss: 10.90497546195984\n",
      "Step 54800: Training loss: 10.912216300964355\n",
      "Step 54900: Training loss: 10.907561931610108\n",
      "Step 55000: Training loss: 10.908604440689087\n",
      "Saved model checkpoint to ./checkpoints/checkpoint-55000\n",
      "Step 55100: Training loss: 10.908978977203368\n",
      "Step 55200: Training loss: 10.904778823852538\n",
      "Step 55300: Training loss: 10.907912721633911\n",
      "Step 55400: Training loss: 10.911792764663696\n",
      "Step 55500: Training loss: 10.909837741851806\n",
      "Step 55600: Training loss: 10.906401081085205\n",
      "Step 55700: Training loss: 10.90842230796814\n",
      "Step 55800: Training loss: 10.909643268585205\n",
      "Step 55900: Training loss: 10.90748372077942\n",
      "Step 56000: Training loss: 10.908044395446778\n",
      "Step 56100: Training loss: 10.910672721862793\n",
      "Step 56200: Training loss: 10.912084674835205\n",
      "Step 56300: Training loss: 10.904943923950196\n",
      "Step 56400: Training loss: 10.913197154998779\n",
      "Step 56500: Training loss: 10.907281656265258\n",
      "Step 56600: Training loss: 10.914887237548829\n",
      "Step 56700: Training loss: 10.907648487091064\n",
      "Step 56800: Training loss: 10.9098810005188\n",
      "Step 56900: Training loss: 10.906669721603393\n",
      "Step 57000: Training loss: 10.90686532020569\n",
      "Step 57100: Training loss: 10.907735252380371\n",
      "Step 57200: Training loss: 10.905303268432617\n",
      "Step 57300: Training loss: 10.907994747161865\n",
      "Step 57400: Training loss: 10.914461355209351\n",
      "Step 57500: Training loss: 10.907420473098755\n",
      "Step 57600: Training loss: 10.907964401245117\n",
      "Step 57700: Training loss: 10.904884471893311\n",
      "Step 57800: Training loss: 10.912177333831787\n",
      "Step 57900: Training loss: 10.907201795578002\n",
      "Step 58000: Training loss: 10.907498445510864\n",
      "Step 58100: Training loss: 10.907215194702149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     41\u001b[0m \u001b[39m# Clip the gradients to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), max_norm\u001b[39m=\u001b[39;49mmax_grad_norm)\n\u001b[0;32m     44\u001b[0m \u001b[39m# Update the parameters\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39mif\u001b[39;00m (step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m gradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[1;32m---> 76\u001b[0m         torch\u001b[39m.\u001b[39;49m_foreach_mul_(grads, clip_coef_clamped\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[0;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_dir = \"logs/\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# Define the hyperparameters\n",
    "per_gpu_train_batch_size = 4\n",
    "per_gpu_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 3\n",
    "max_steps = -1\n",
    "warmup_steps = 0\n",
    "logging_steps = 100\n",
    "save_steps = 5000\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "# Define the training loop\n",
    "global_step = 0\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_train_epochs}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Move the inputs and labels to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # Update the parameters\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log training loss\n",
    "            epoch_loss += loss.item()\n",
    "            if global_step % logging_steps == 0:\n",
    "                print(f'Step {global_step}: Training loss: {epoch_loss/logging_steps}')\n",
    "                epoch_loss = 0\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if global_step % save_steps == 0:\n",
    "                output_dir = f'./checkpoints/checkpoint-{global_step}'\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "                print(f'Saved model checkpoint to {output_dir}')\n",
    "                \n",
    "                    \n",
    "                model.train()\n",
    "                \n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "        \n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import RobertaForMaskedLM, RobertaTokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name= 'C:/Bangla_dude/checkpoints/checkpoint-5000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('bangla_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./checkpoints/checkpoint-5000 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./checkpoints/checkpoint-5000 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/278621 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     40\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     42\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1216\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1211\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1216\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1217\u001b[0m     input_ids,\n\u001b[0;32m   1218\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1219\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1220\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1221\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1222\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1223\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1224\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1225\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1226\u001b[0m )\n\u001b[0;32m   1227\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1228\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:845\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    840\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    848\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    849\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[0;32m    852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:123\u001b[0m, in \u001b[0;36mRobertaEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    120\u001b[0m         token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mword_embeddings(input_ids)\n\u001b[0;32m    124\u001b[0m token_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    126\u001b[0m embeddings \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Define the hyperparameters\n",
    "per_gpu_train_batch_size = 4\n",
    "per_gpu_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0.0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1.0\n",
    "num_train_epochs = 3\n",
    "max_steps = -1\n",
    "warmup_steps = 0\n",
    "logging_steps = 100\n",
    "save_steps = 5000\n",
    "\n",
    "# Load the saved checkpoint\n",
    "checkpoint_dir = './checkpoints/checkpoint-5000'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon, weight_decay=weight_decay)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=max_steps)\n",
    "\n",
    "\n",
    "# Define the training loop\n",
    "global_step = 5000\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_train_epochs}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Move the inputs and labels to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # Update the parameters\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            \n",
    "            # Log training loss\n",
    "            epoch_loss += loss.item()\n",
    "            if global_step % logging_steps == 0:\n",
    "                print(f'Step {global_step}: Training loss: {epoch_loss/logging_steps}')\n",
    "                epoch_loss = 0\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if global_step % save_steps == 0:\n",
    "                output_dir = f'./checkpoints/checkpoint-{global_step}'\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                model.save_pretrained(output_dir)\n",
    "      \n",
    "            if max_steps > 0 and global_step > max_steps:\n",
    "                break\n",
    "        \n",
    "    if max_steps > 0 and global_step > max_steps:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 785/278621 [04:01<23:47:11,  3.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m \u001b[39m# Clip the gradients to prevent exploding gradients\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(model\u001b[39m.\u001b[39;49mparameters(), max_norm\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m \u001b[39m# Update the parameters\u001b[39;00m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:76\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m ((device, _), [grads]) \u001b[39min\u001b[39;00m grouped_grads\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m (foreach \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m foreach) \u001b[39mand\u001b[39;00m _has_foreach_support(grads, device\u001b[39m=\u001b[39mdevice):\n\u001b[1;32m---> 76\u001b[0m         torch\u001b[39m.\u001b[39;49m_foreach_mul_(grads, clip_coef_clamped\u001b[39m.\u001b[39;49mto(device))  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[39melif\u001b[39;00m foreach:\n\u001b[0;32m     78\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mforeach=True was passed, but can\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mt use the foreach API on \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Move the inputs and labels to the device (GPU or CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    print(f'Training loss: {epoch_loss/len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch =3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caf11a854da4b82942cb91693a297f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/278621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (1114481) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m attention_mask \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m labels \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mmask, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m     10\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1100\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[39m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1100\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1101\u001b[0m     input_ids,\n\u001b[0;32m   1102\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1103\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1104\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1105\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1106\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1107\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1108\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1109\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1110\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1111\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1112\u001b[0m )\n\u001b[0;32m   1113\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1114\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    853\u001b[0m     embedding_output,\n\u001b[0;32m    854\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    855\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    856\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    857\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    858\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    859\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    860\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    861\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    862\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    863\u001b[0m )\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    528\u001b[0m         hidden_states,\n\u001b[0;32m    529\u001b[0m         attention_mask,\n\u001b[0;32m    530\u001b[0m         layer_head_mask,\n\u001b[0;32m    531\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    532\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    533\u001b[0m         past_key_value,\n\u001b[0;32m    534\u001b[0m         output_attentions,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    400\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    401\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    409\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    412\u001b[0m         hidden_states,\n\u001b[0;32m    413\u001b[0m         attention_mask,\n\u001b[0;32m    414\u001b[0m         head_mask,\n\u001b[0;32m    415\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    416\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    417\u001b[0m     )\n\u001b[0;32m    418\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    420\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:338\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    337\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 338\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    339\u001b[0m         hidden_states,\n\u001b[0;32m    340\u001b[0m         attention_mask,\n\u001b[0;32m    341\u001b[0m         head_mask,\n\u001b[0;32m    342\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    343\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    344\u001b[0m         past_key_value,\n\u001b[0;32m    345\u001b[0m         output_attentions,\n\u001b[0;32m    346\u001b[0m     )\n\u001b[0;32m    347\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    348\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:261\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    258\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[0;32m    259\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39;49m attention_mask\n\u001b[0;32m    263\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m    264\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (1114481) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "loop = tqdm(dataloader, leave=True)\n",
    "for batch in loop:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    loop.set_description(f'Loss: {loss.item()}')\n",
    "    loop.set_postfix(loss=loss.item())\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
