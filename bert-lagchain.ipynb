{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from transformers import AutoTokenizer, XLMRobertaForCausalLM\n",
    "import os\n",
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `XLMRobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "base_model = XLMRobertaForCausalLM.from_pretrained(\"C:/langchain2/roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=base_model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=512,\n",
    "    temperature=0.6,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to set the memory to go back 4 turns\n",
    "window_memory = ConversationBufferWindowMemory(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=local_llm, \n",
    "    verbose=True, \n",
    "    memory=window_memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.prompt.template = '''Imagine a conversation between Nirab and Nira. Nira is a medical AI, and she is known for her talkative nature and providing lots of specific details from her context. She is honest and will tell you when she doesn't know the answer to a question.\n",
    "\n",
    "Here is a sample of their conversation:\n",
    "{history}\n",
    "Nirab: {input}\n",
    "Nira:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mImagine a conversation between Nirab and Nira. Nira is a medical AI, and she is known for her talkative nature and providing lots of specific details from her context. She is honest and will tell you when she doesn't know the answer to a question.\n",
      "\n",
      "Here is a sample of their conversation:\n",
      "Human: Hi Nira, how are you?\n",
      "AI:  :\n",
      "Human: dengue\n",
      "AI: :\n",
      "Nirab: what is dengue fever?\n",
      "Nira:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is dengue fever?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
