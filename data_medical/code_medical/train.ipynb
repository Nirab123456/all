{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/langchain2/data_medical/generic_drug_combo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output_text</th>\n",
       "      <th>intput_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gastroesophageal reflux disease</td>\n",
       "      <td>what is the uses of Magaldrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fever</td>\n",
       "      <td>what is the uses of Doxylamine-PE-DM-Acetamino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>calcium carb-mag hydrox-simeth, alum-mag hydro...</td>\n",
       "      <td>which medication is used for flatulence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bacterial Urinary Tract Infection</td>\n",
       "      <td>what is the uses of Cefepime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>diclofenac sodium</td>\n",
       "      <td>which medication is used for gout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>acetaminophen-pamabrom, ibuprofen</td>\n",
       "      <td>which medication is used for gout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>ampicillin-sulbactam</td>\n",
       "      <td>which medication is used for diverticulitis of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>edema</td>\n",
       "      <td>what is the uses of Acetazolamide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>gastroesophageal reflux disease</td>\n",
       "      <td>what is the uses of Cimetidine Hcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>fever</td>\n",
       "      <td>what is the uses of Aspirin-Acetaminophen-Caff...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1370 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            output_text   \n",
       "0                       gastroesophageal reflux disease  \\\n",
       "1                                                 fever   \n",
       "2     calcium carb-mag hydrox-simeth, alum-mag hydro...   \n",
       "3                     Bacterial Urinary Tract Infection   \n",
       "4                                     diclofenac sodium   \n",
       "...                                                 ...   \n",
       "1365                  acetaminophen-pamabrom, ibuprofen   \n",
       "1366                               ampicillin-sulbactam   \n",
       "1367                                              edema   \n",
       "1368                    gastroesophageal reflux disease   \n",
       "1369                                              fever   \n",
       "\n",
       "                                            intput_text  \n",
       "0                        what is the uses of Magaldrate  \n",
       "1     what is the uses of Doxylamine-PE-DM-Acetamino...  \n",
       "2               which medication is used for flatulence  \n",
       "3                          what is the uses of Cefepime  \n",
       "4                     which medication is used for gout  \n",
       "...                                                 ...  \n",
       "1365                  which medication is used for gout  \n",
       "1366  which medication is used for diverticulitis of...  \n",
       "1367                  what is the uses of Acetazolamide  \n",
       "1368                 what is the uses of Cimetidine Hcl  \n",
       "1369  what is the uses of Aspirin-Acetaminophen-Caff...  \n",
       "\n",
       "[1370 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the maximum length of input_text column\n",
    "# max_len = df['input_text'].apply(len).max()\n",
    "\n",
    "# print('Maximum length of input_text column:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the number of rows in 'text' column containing more than 512 tokens\n",
    "# count = N_df['input_text'].apply(lambda x: len(x.split()) > 600).sum()\n",
    "\n",
    "# # print the count\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training Data:  1233\n",
      "Length of Val Data:  137\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trn_df, val_df = train_test_split(df, test_size=0.1)\n",
    "print('Length of Training Data: ', len(trn_df))\n",
    "print('Length of Val Data: ', len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import re\n",
    "import shutil\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"C:/langchain2/data_medical/code_medical/model2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') \n",
    "# # check if CUDA is available\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "#     print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "# else:\n",
    "#     # if CUDA is not available, exit gracefully\n",
    "#     print(\"CUDA is not available. Exiting...\")\n",
    "#     exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"C:/langchain2/data_medical/code_medical/model2/checkpoint-71976\")\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"C:/langchain2/data_medical/code_medical/model2/checkpoint-71976\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from transformers import PreTrainedTokenizer, MODEL_WITH_LM_HEAD_MAPPING, WEIGHTS_NAME\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def construct_convo(row, tokenizer):\n",
    "  # print(row)\n",
    "  # print(tokenizer.encode(row[0]))\n",
    "  # print(tokenizer.encode(row[-1]))\n",
    "  conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row])) \n",
    "  print(conv) \n",
    "  flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "  conv = flatten(conv)\n",
    "  # print(conv)\n",
    "  return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "  def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n",
    "    block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "    directory = args.cache_dir\n",
    "    cached_features_file = os.path.join(\n",
    "        directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "    )\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "      logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "      with open(cached_features_file, \"rb\") as handle:\n",
    "          self.examples = pickle.load(handle)\n",
    "    else:\n",
    "      logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "      self.examples = []\n",
    "      for _, row in df.iterrows():\n",
    "          conv = construct_convo(row, tokenizer)\n",
    "          self.examples.append(conv)\n",
    "      \n",
    "      logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "      with open(cached_features_file, \"wb\") as handle:\n",
    "        pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, row in trn_df.iterrows():\n",
    "#   conv = construct_convo(row, tokenizer)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def construct_convo(row, tokenizer):\n",
    "  conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row])) \n",
    "  flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "  conv = flatten(conv)\n",
    "  return conv\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "  def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=256):\n",
    "    block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n",
    "\n",
    "    directory = args.cache_dir\n",
    "    cached_features_file = os.path.join(\n",
    "        directory, args.model_type + \"_cached_lm_\" + str(block_size)\n",
    "    )\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "      logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "      with open(cached_features_file, \"rb\") as handle:\n",
    "          self.examples = pickle.load(handle)\n",
    "    else:\n",
    "      logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "      self.examples = []\n",
    "      for _, row in df.iterrows():\n",
    "          conv = construct_convo(row, tokenizer)\n",
    "          self.examples.append(conv)\n",
    "      \n",
    "      logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "      with open(cached_features_file, \"wb\") as handle:\n",
    "        pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return torch.tensor(self.examples[item], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching and storing of data/checkpoints\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n",
    "   return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n",
    "\n",
    "def set_seed(args):\n",
    "   random.seed(args.seed)\n",
    "   np.random.seed(args.seed)\n",
    "   torch.manual_seed(args.seed)\n",
    "   if args.n_gpu > 0:\n",
    "       torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
    "   ordering_and_checkpoint_path = []\n",
    "\n",
    "\n",
    "   glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "\n",
    "   for path in glob_checkpoints:\n",
    "       if use_mtime:\n",
    "           ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "       else:\n",
    "           regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "           if regex_match and regex_match.groups():\n",
    "               ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "\n",
    "   checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "   checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "   return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
    "   if not args.save_total_limit:\n",
    "       return\n",
    "   if args.save_total_limit <= 0:\n",
    "       return\n",
    "\n",
    "\n",
    "   # Check if we should delete older checkpoint(s)\n",
    "   checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "   if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "       return\n",
    "\n",
    "   number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "   checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "   for checkpoint in checkpoints_to_be_deleted:\n",
    "       logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "       shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "    print(\"Batch size: \", args.train_batch_size)\n",
    "    print(\"Data Loader length: \", len(train_dataloader))\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # add_special_tokens_(model, tokenizer)\n",
    "\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": args.weight_decay,\n",
    "        },\n",
    "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    # Check if saved optimizer or scheduler states exist\n",
    "    if (\n",
    "        args.model_name_or_path\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
    "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
    "    ):\n",
    "        # Load in optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\n",
    "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "        args.train_batch_size\n",
    "        * args.gradient_accumulation_steps\n",
    "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
    "    )\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    # Check if continuing training from a checkpoint\n",
    "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
    "        try:\n",
    "            # set global_step to gobal_step of last saved checkpoint from model path\n",
    "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
    "            global_step = int(checkpoint_suffix)\n",
    "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "\n",
    "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
    "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
    "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
    "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
    "        except ValueError:\n",
    "            logger.info(\"  Starting fine-tuning.\")\n",
    "\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # Added here for reproducibility\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            # Skip past any already trained steps if resuming training\n",
    "            if steps_trained_in_current_epoch > 0:\n",
    "                steps_trained_in_current_epoch -= 1\n",
    "                continue\n",
    "\n",
    "            inputs, labels = (batch, batch)\n",
    "            if inputs.shape[1] > 1024: continue\n",
    "            inputs = inputs.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            model.train()\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if (\n",
    "                        args.local_rank == -1 and args.evaluate_during_training\n",
    "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "                    os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                print(\"Batch shape: \", batch.shape)\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "# Evaluation of some model\n",
    "\n",
    "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "    # Note that DistributedSampler samples randomly\n",
    "\n",
    "    def collate(examples: List[torch.Tensor]):\n",
    "        if tokenizer._pad_token is None:\n",
    "            return pad_sequence(examples, batch_first=True)\n",
    "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, labels = (batch, batch)\n",
    "        inputs = inputs.to(args.device)\n",
    "        labels = labels.to(args.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main runner\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def main(df_trn, df_val):\n",
    "    args = Args()\n",
    "    \n",
    "    if args.should_continue:\n",
    "        sorted_checkpoints = _sorted_checkpoints(args)\n",
    "        if len(sorted_checkpoints) == 0:\n",
    "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "        else:\n",
    "            args.model_name_or_path = sorted_checkpoints[-1]\n",
    "\n",
    "    if (\n",
    "        os.path.exists(args.output_dir)\n",
    "        and os.listdir(args.output_dir)\n",
    "        and args.do_train\n",
    "        and not args.overwrite_output_dir\n",
    "        and not args.should_continue\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    device = torch.device(\"cuda\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "    args.device = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
    "    )\n",
    "    logger.warning(\n",
    "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "        args.local_rank,\n",
    "        device,\n",
    "        args.n_gpu,\n",
    "        bool(args.local_rank != -1),\n",
    "        args.fp16,\n",
    "    )\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=False,\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir,\n",
    "    )\n",
    "    model.to(args.device)\n",
    "    \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n",
    "        print(train_dataset)\n",
    "        print(\"Train Dataset Length: \", len(train_dataset))\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = (\n",
    "            model.module if hasattr(model, \"module\") else model\n",
    "        )  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args.output_dir)\n",
    "        tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args.do_eval and args.local_rank in [-1, 0]:\n",
    "        checkpoints = [args.output_dir]\n",
    "        if args.eval_all_checkpoints:\n",
    "            checkpoints = list(\n",
    "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
    "            )\n",
    "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
    "\n",
    "            model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "            model.to(args.device)\n",
    "            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n",
    "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.output_dir = 'C:/langchain2/data_medical/code_medical/model2'\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'C:/langchain2/data_medical/code_medical/model2'\n",
    "        self.config_name = 'C:/langchain2/data_medical/code_medical/model2'\n",
    "        self.tokenizer_name = 'C:/langchain2/data_medical/code_medical/model2'\n",
    "        self.cache_dir = 'cached'\n",
    "        self.block_size = 1024\n",
    "        self.do_train = True\n",
    "        self.do_eval = True\n",
    "        self.evaluate_during_training = False\n",
    "        self.per_gpu_train_batch_size = 1\n",
    "        self.per_gpu_eval_batch_size = 1\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.learning_rate = 5e-6\n",
    "        self.weight_decay = 0.0\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.num_train_epochs = 10\n",
    "        self.max_steps = -1\n",
    "        self.warmup_steps = 0\n",
    "        self.logging_steps = 5\n",
    "        self.save_steps = 3000\n",
    "        self.save_total_limit = None\n",
    "        self.eval_all_checkpoints = False\n",
    "        self.no_cuda = False\n",
    "        self.overwrite_output_dir = True\n",
    "        self.overwrite_cache = True\n",
    "        self.should_continue = False\n",
    "        self.seed = 42\n",
    "        self.local_rank = -1\n",
    "        self.fp16 = False\n",
    "        self.fp16_opt_level = 'O1'\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 16:49:50 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/06/2023 16:49:51 - INFO - __main__ -   Training/evaluation parameters <__main__.Args object at 0x000002F33B222E90>\n",
      "05/06/2023 16:49:51 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/06/2023 16:49:52 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_256\n",
      "c:\\Users\\rifat\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.ConversationDataset object at 0x000002F33B220760>\n",
      "Train Dataset Length:  1233\n",
      "Batch size:  1\n",
      "Data Loader length:  1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 16:49:53 - INFO - __main__ -   ***** Running training *****\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Num examples = 1233\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Num Epochs = 10\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Total optimization steps = 12330\n",
      "05/06/2023 16:49:53 - INFO - __main__ -     Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205b15e6d70542909e11c63e89d9ca42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e02039582f44dbaabd4297baf3048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cbfa2227214cf0be4fdb916797ab75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17aeb74ea7b4fd08f0fe7233ced35ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 16:54:01 - INFO - __main__ -   Saving model checkpoint to C:/langchain2/data_medical/code_medical/model2\\checkpoint-3000\n",
      "05/06/2023 16:54:05 - INFO - __main__ -   Saving optimizer and scheduler states to C:/langchain2/data_medical/code_medical/model2\\checkpoint-3000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0377de78c5349caaca86fbacca68baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb34c733782471d96bfc24a8f9793a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 16:58:26 - INFO - __main__ -   Saving model checkpoint to C:/langchain2/data_medical/code_medical/model2\\checkpoint-6000\n",
      "05/06/2023 16:58:31 - INFO - __main__ -   Saving optimizer and scheduler states to C:/langchain2/data_medical/code_medical/model2\\checkpoint-6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfec74b1bec0421b8a16841d88fd9d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcb97eb963445819dc5282f1af17fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bdbc1e23904acba7e223c0d7addafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 17:02:47 - INFO - __main__ -   Saving model checkpoint to C:/langchain2/data_medical/code_medical/model2\\checkpoint-9000\n",
      "05/06/2023 17:02:52 - INFO - __main__ -   Saving optimizer and scheduler states to C:/langchain2/data_medical/code_medical/model2\\checkpoint-9000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53d4c493835467bada1bda25f0ef573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76a69f0057f46deb5c7eaf1ea7227e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 17:07:13 - INFO - __main__ -   Saving model checkpoint to C:/langchain2/data_medical/code_medical/model2\\checkpoint-12000\n",
      "05/06/2023 17:07:19 - INFO - __main__ -   Saving optimizer and scheduler states to C:/langchain2/data_medical/code_medical/model2\\checkpoint-12000\n",
      "05/06/2023 17:07:48 - INFO - __main__ -    global_step = 12330, average loss = 5.253072975615805\n",
      "05/06/2023 17:07:48 - INFO - __main__ -   Saving model checkpoint to C:/langchain2/data_medical/code_medical/model2\n",
      "05/06/2023 17:07:51 - INFO - __main__ -   Evaluate the following checkpoints: ['C:/langchain2/data_medical/code_medical/model2']\n",
      "05/06/2023 17:07:53 - INFO - __main__ -   Creating features from dataset file at cached\n",
      "05/06/2023 17:07:53 - INFO - __main__ -   Saving features into cached file cached\\gpt2_cached_lm_256\n",
      "05/06/2023 17:07:53 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "05/06/2023 17:07:53 - INFO - __main__ -     Num examples = 137\n",
      "05/06/2023 17:07:53 - INFO - __main__ -     Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1ca9d635ce406992fc3e30f16b6c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/06/2023 17:07:55 - INFO - __main__ -   ***** Eval results  *****\n",
      "05/06/2023 17:07:55 - INFO - __main__ -     perplexity = tensor(146.4523)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'perplexity_': tensor(146.4523)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(trn_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "\n",
    "# Concatenate the text columns and replace '\\n' with a space\n",
    "df['input_text'] = df['BACKGROUND'].fillna('') + ' ' + df['OBJECTIVE'].fillna('') + ' ' + df['RESULTS'].fillna('') + ' ' + df['CONCLUSIONS'].fillna('')\n",
    "df['input_text'] = df['input_text'].str.replace('\\n', ' ')\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Set the maximum length of each chunk\n",
    "max_len = 512\n",
    "\n",
    "# Process each row in the dataset\n",
    "for index, row in df.iterrows():\n",
    "    # Split the input text into chunks of fixed length\n",
    "    input_text = row['input_text']\n",
    "    chunks = [input_text[i:i+max_len] for i in range(0, len(input_text), max_len)]\n",
    "\n",
    "    # Process each chunk separately and concatenate the output\n",
    "    output = ''\n",
    "    for chunk in chunks:\n",
    "        input_ids = tokenizer.encode(chunk, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = model(input_ids)[0]\n",
    "        output += tokenizer.decode(model_output[0, -max_len:], skip_special_tokens=True)\n",
    "\n",
    "    # Store the output back into the dataframe\n",
    "    df.at[index, 'output_text'] = output\n",
    "\n",
    "# Save the updated dataframe\n",
    "df.to_csv('your_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the DataFrame\n",
    "df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Tokenize the text and convert it to a TextDataset\n",
    "tokenized_text = df.apply(lambda x: tokenizer.encode(x['BACKGROUND'] + x['OBJECTIVE'] + x['METHODS'] + x['RESULTS'] + x['CONCLUSIONS']), axis=1)\n",
    "dataset = TextDataset(tokenized_text.tolist(), tokenizer=tokenizer)\n",
    "\n",
    "# Set up the data collator and training arguments\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=500,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
